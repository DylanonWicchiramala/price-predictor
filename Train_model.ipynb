{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas_ta as ta\n",
    "import getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_param = {\n",
    "    'win_size':22,\n",
    "    'stride':1,\n",
    "    'split':True,\n",
    "    'number_y':1,\n",
    "    'random_state':420,\n",
    "    'test_size':0.2,\n",
    "}\n",
    "\n",
    "v_preprocess_param = {\n",
    "    'win_size':22,\n",
    "    'stride':1,\n",
    "    'split':False,\n",
    "    'number_y':1,\n",
    "    'random_state':420,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = 'BTC-USD'\n",
    "\n",
    "prices_df = getData.loader(tickers=tickers, interval=\"1d\", period='max', end=\"2023-01-01\").dataframe\n",
    "prices_df_val = getData.loader(tickers=tickers, interval=\"1d\", start='2023-01-01').dataframe\n",
    "\n",
    "datasets = getData.preprocessor(prices_df, preprocess_param=preprocess_param).dataset\n",
    "val_sets = getData.preprocessor(prices_df_val, preprocess_param=v_preprocess_param).dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PriceHistoryDataset(Dataset):\n",
    "    def __init__(self, dataset, to_predict=['Open', 'High', 'Low', 'Close']):\n",
    "        y = dataset['y'][:,:,self.__map_to_indices(to_predict)]\n",
    "        x = dataset['x']\n",
    "        self.columns = dataset['columns']\n",
    "        self.initial_price = dataset['initial price']\n",
    "        self.current_date = dataset['current date']\n",
    "        \n",
    "        self.X = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __map_to_indices(self, args):\n",
    "        mapping = {'Open': 0, 'High': 1, 'Low': 2, 'Close': 3}\n",
    "        return [mapping[arg] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = datasets['columns']\n",
    "\n",
    "to_predict = ['Close']\n",
    "\n",
    "train_set = PriceHistoryDataset(datasets['train'], to_predict)\n",
    "test_set = PriceHistoryDataset(datasets['test'], to_predict)\n",
    "val_set = PriceHistoryDataset(val_sets, to_predict)\n",
    "\n",
    "train_loader= DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LSTMModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hidden_size, lstm_layers, head_layers, input_size=8, output_size=3, dropout=0.05):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=lstm_layers)\n",
    "        \n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(head_layers-1)\n",
    "        ])\n",
    "        \n",
    "        self.out_linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # keep track of losses function.\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.loss_func = nn.L1Loss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.gru(x)\n",
    "        o = lstm_out[:,-1:,:]\n",
    "        \n",
    "        for linear in self.linears:\n",
    "            o = linear(o)\n",
    "        \n",
    "        output = self.out_linear(o)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.train_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.test_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.test_losses).mean()\n",
    "        print(f'Test Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.train_losses).mean()\n",
    "        print(f'Train Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = LSTMModel(output_size=len(to_predict), hidden_size=128, lstm_layers=5, head_layers=2, dropout=0.0)\n",
    "# model = LSTMModel.load_from_checkpoint(\"/model/lightning_logs/vsrsion_.../checkpoints/....ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 615681\n",
      "Number of layers: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Number of layers:\", len(list(model.children())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | gru        | LSTM       | 599 K \n",
      "1 | linears    | ModuleList | 16.5 K\n",
      "2 | out_linear | Linear     | 129   \n",
      "3 | loss_func  | L1Loss     | 0     \n",
      "------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.463     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707872acb11d46cda1a8a6af8d27ff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2592051029205322\n",
      "Train Loss: 0.22667977213859558\n",
      "Train Loss: 0.21543721854686737\n",
      "Train Loss: 0.20782046020030975\n",
      "Train Loss: 0.4430702328681946\n",
      "Train Loss: 0.41358327865600586\n",
      "Train Loss: 0.39541900157928467\n",
      "Train Loss: 0.3890368938446045\n",
      "Train Loss: 0.3690817952156067\n",
      "Train Loss: 0.3517449200153351\n",
      "Train Loss: 0.33669567108154297\n",
      "Train Loss: 0.32104912400245667\n",
      "Train Loss: 0.30618926882743835\n",
      "Train Loss: 0.2905607521533966\n",
      "Train Loss: 0.2756863534450531\n",
      "Train Loss: 0.2618788182735443\n",
      "Train Loss: 0.24914328753948212\n",
      "Train Loss: 0.23775362968444824\n",
      "Train Loss: 0.22763007879257202\n",
      "Train Loss: 0.21848131716251373\n",
      "Train Loss: 0.21010039746761322\n",
      "Train Loss: 0.2022065371274948\n",
      "Train Loss: 0.1951187700033188\n",
      "Train Loss: 0.1889791488647461\n",
      "Train Loss: 0.1838812679052353\n",
      "Train Loss: 0.1786920577287674\n",
      "Train Loss: 0.17354103922843933\n",
      "Train Loss: 0.16871875524520874\n",
      "Train Loss: 0.16410665214061737\n",
      "Train Loss: 0.15977592766284943\n",
      "Train Loss: 0.15570507943630219\n",
      "Train Loss: 0.15185129642486572\n",
      "Train Loss: 0.1482337862253189\n",
      "Train Loss: 0.1449202001094818\n",
      "Train Loss: 0.14178211987018585\n",
      "Train Loss: 0.1387271136045456\n",
      "Train Loss: 0.1358681619167328\n",
      "Train Loss: 0.13317275047302246\n",
      "Train Loss: 0.13055871427059174\n",
      "Train Loss: 0.12806212902069092\n",
      "Train Loss: 0.12569992244243622\n",
      "Train Loss: 0.12352335453033447\n",
      "Train Loss: 0.12163699418306351\n",
      "Train Loss: 0.11975126713514328\n",
      "Train Loss: 0.11783093959093094\n",
      "Train Loss: 0.11597029864788055\n",
      "Train Loss: 0.1141953244805336\n",
      "Train Loss: 0.11245817691087723\n",
      "Train Loss: 0.11078403145074844\n",
      "Train Loss: 0.109221450984478\n",
      "Train Loss: 0.10768986493349075\n",
      "Train Loss: 0.1061846911907196\n",
      "Train Loss: 0.10475361347198486\n",
      "Train Loss: 0.10338964313268661\n",
      "Train Loss: 0.10207272320985794\n",
      "Train Loss: 0.10082501173019409\n",
      "Train Loss: 0.09964674711227417\n",
      "Train Loss: 0.09846841543912888\n",
      "Train Loss: 0.09739284217357635\n",
      "Train Loss: 0.09630827605724335\n",
      "Train Loss: 0.09526584297418594\n",
      "Train Loss: 0.09428523480892181\n",
      "Train Loss: 0.0933208167552948\n",
      "Train Loss: 0.09236563742160797\n",
      "Train Loss: 0.09141116589307785\n",
      "Train Loss: 0.09049450606107712\n",
      "Train Loss: 0.08961706608533859\n",
      "Train Loss: 0.08875523507595062\n",
      "Train Loss: 0.08788572996854782\n",
      "Train Loss: 0.08707045763731003\n",
      "Train Loss: 0.0862814262509346\n",
      "Train Loss: 0.08549022674560547\n",
      "Train Loss: 0.084746815264225\n",
      "Train Loss: 0.0840490311384201\n",
      "Train Loss: 0.08331936597824097\n",
      "Train Loss: 0.0826609879732132\n",
      "Train Loss: 0.08205965161323547\n",
      "Train Loss: 0.0814313292503357\n",
      "Train Loss: 0.08078929036855698\n",
      "Train Loss: 0.08014578372240067\n",
      "Train Loss: 0.07951243221759796\n",
      "Train Loss: 0.07890584319829941\n",
      "Train Loss: 0.07830606400966644\n",
      "Train Loss: 0.07771299034357071\n",
      "Train Loss: 0.07717456668615341\n",
      "Train Loss: 0.07670164853334427\n",
      "Train Loss: 0.0761699303984642\n",
      "Train Loss: 0.07563283294439316\n",
      "Train Loss: 0.07511545717716217\n",
      "Train Loss: 0.07460532337427139\n",
      "Train Loss: 0.07411584258079529\n",
      "Train Loss: 0.07365642488002777\n",
      "Train Loss: 0.07318414747714996\n",
      "Train Loss: 0.07274602353572845\n",
      "Train Loss: 0.07229140400886536\n",
      "Train Loss: 0.07186096906661987\n",
      "Train Loss: 0.07146274298429489\n",
      "Train Loss: 0.07106107473373413\n",
      "Train Loss: 0.0706593319773674\n",
      "Train Loss: 0.07025376707315445\n",
      "Train Loss: 0.06987809389829636\n",
      "Train Loss: 0.06950151920318604\n",
      "Train Loss: 0.06911006569862366\n",
      "Train Loss: 0.06872407346963882\n",
      "Train Loss: 0.06834887713193893\n",
      "Train Loss: 0.06796831637620926\n",
      "Train Loss: 0.06760019063949585\n",
      "Train Loss: 0.06724362820386887\n",
      "Train Loss: 0.06693170219659805\n",
      "Train Loss: 0.06663301587104797\n",
      "Train Loss: 0.06632886826992035\n",
      "Train Loss: 0.06601051241159439\n",
      "Train Loss: 0.06569068133831024\n",
      "Train Loss: 0.06540236622095108\n",
      "Train Loss: 0.06510978192090988\n",
      "Train Loss: 0.06479198485612869\n",
      "Train Loss: 0.06448724120855331\n",
      "Train Loss: 0.06418143957853317\n",
      "Train Loss: 0.06388489902019501\n",
      "Train Loss: 0.06359098851680756\n",
      "Train Loss: 0.06329990923404694\n",
      "Train Loss: 0.06302326917648315\n",
      "Train Loss: 0.06275814771652222\n",
      "Train Loss: 0.06252512335777283\n",
      "Train Loss: 0.06227784603834152\n",
      "Train Loss: 0.06202031672000885\n",
      "Train Loss: 0.06176035851240158\n",
      "Train Loss: 0.061498191207647324\n",
      "Train Loss: 0.06124091520905495\n",
      "Train Loss: 0.061003655195236206\n",
      "Train Loss: 0.06077585369348526\n",
      "Train Loss: 0.060553379356861115\n",
      "Train Loss: 0.060358837246894836\n",
      "Train Loss: 0.06016293913125992\n",
      "Train Loss: 0.05994770675897598\n",
      "Train Loss: 0.05972673371434212\n",
      "Train Loss: 0.05950506776571274\n",
      "Train Loss: 0.05928855389356613\n",
      "Train Loss: 0.05906950309872627\n",
      "Train Loss: 0.05886119604110718\n",
      "Train Loss: 0.0586516335606575\n",
      "Train Loss: 0.05844968557357788\n",
      "Train Loss: 0.05825310945510864\n",
      "Train Loss: 0.058047953993082047\n",
      "Train Loss: 0.05785055458545685\n",
      "Train Loss: 0.05766591429710388\n",
      "Train Loss: 0.05746757611632347\n",
      "Train Loss: 0.057263001799583435\n",
      "Train Loss: 0.057068221271038055\n",
      "Train Loss: 0.05688752979040146\n",
      "Train Loss: 0.0566936619579792\n",
      "Train Loss: 0.056515391916036606\n",
      "Train Loss: 0.056343019008636475\n",
      "Train Loss: 0.05617954209446907\n",
      "Train Loss: 0.05600828677415848\n",
      "Train Loss: 0.055840667337179184\n",
      "Train Loss: 0.05567971244454384\n",
      "Train Loss: 0.055530671030282974\n",
      "Train Loss: 0.05537547171115875\n",
      "Train Loss: 0.055218834429979324\n",
      "Train Loss: 0.055054087191820145\n",
      "Train Loss: 0.05489817261695862\n",
      "Train Loss: 0.05473481863737106\n",
      "Train Loss: 0.05457700043916702\n",
      "Train Loss: 0.05441620945930481\n",
      "Train Loss: 0.05425668880343437\n",
      "Train Loss: 0.05409655719995499\n",
      "Train Loss: 0.05394722893834114\n",
      "Train Loss: 0.05379656329751015\n",
      "Train Loss: 0.05364259332418442\n",
      "Train Loss: 0.053495291620492935\n",
      "Train Loss: 0.05336206778883934\n",
      "Train Loss: 0.05321677774190903\n",
      "Train Loss: 0.05307525023818016\n",
      "Train Loss: 0.05292896926403046\n",
      "Train Loss: 0.052794475108385086\n",
      "Train Loss: 0.05266287922859192\n",
      "Train Loss: 0.052528493106365204\n",
      "Train Loss: 0.05238936468958855\n",
      "Train Loss: 0.05225866287946701\n",
      "Train Loss: 0.05211937800049782\n",
      "Train Loss: 0.05198344588279724\n",
      "Train Loss: 0.051851071417331696\n",
      "Train Loss: 0.0517309233546257\n",
      "Train Loss: 0.0516340397298336\n",
      "Train Loss: 0.051547497510910034\n",
      "Train Loss: 0.051449064165353775\n",
      "Train Loss: 0.05136435851454735\n",
      "Train Loss: 0.0512850321829319\n",
      "Train Loss: 0.05118199810385704\n",
      "Train Loss: 0.05107800289988518\n",
      "Train Loss: 0.05098211392760277\n",
      "Train Loss: 0.05087973549962044\n",
      "Train Loss: 0.05078377202153206\n",
      "Train Loss: 0.05068882182240486\n",
      "Train Loss: 0.050590649247169495\n",
      "Train Loss: 0.050483476370573044\n",
      "Train Loss: 0.05037552863359451\n",
      "Train Loss: 0.05026489496231079\n",
      "Train Loss: 0.05015646293759346\n",
      "Train Loss: 0.05005815625190735\n",
      "Train Loss: 0.049963612109422684\n",
      "Train Loss: 0.04986665025353432\n",
      "Train Loss: 0.04976791888475418\n",
      "Train Loss: 0.04967454448342323\n",
      "Train Loss: 0.04958777874708176\n",
      "Train Loss: 0.04948895424604416\n",
      "Train Loss: 0.04938850924372673\n",
      "Train Loss: 0.049282532185316086\n",
      "Train Loss: 0.04917563870549202\n",
      "Train Loss: 0.04907463118433952\n",
      "Train Loss: 0.0489744208753109\n",
      "Train Loss: 0.04886901006102562\n",
      "Train Loss: 0.04877601936459541\n",
      "Train Loss: 0.048679012805223465\n",
      "Train Loss: 0.04857686534523964\n",
      "Train Loss: 0.04847581312060356\n",
      "Train Loss: 0.04837406054139137\n",
      "Train Loss: 0.04826994240283966\n",
      "Train Loss: 0.04817547649145126\n",
      "Train Loss: 0.04808582365512848\n",
      "Train Loss: 0.04801230877637863\n",
      "Train Loss: 0.047948576509952545\n",
      "Train Loss: 0.04786853492259979\n",
      "Train Loss: 0.047786518931388855\n",
      "Train Loss: 0.047697633504867554\n",
      "Train Loss: 0.04760723561048508\n",
      "Train Loss: 0.04751576855778694\n",
      "Train Loss: 0.04742332547903061\n",
      "Train Loss: 0.047326598316431046\n",
      "Train Loss: 0.04723319783806801\n",
      "Train Loss: 0.04714338108897209\n",
      "Train Loss: 0.04705853387713432\n",
      "Train Loss: 0.04698312282562256\n",
      "Train Loss: 0.04692378267645836\n",
      "Train Loss: 0.04686307534575462\n",
      "Train Loss: 0.04679853469133377\n",
      "Train Loss: 0.04672703146934509\n",
      "Train Loss: 0.04664963111281395\n",
      "Train Loss: 0.04656912386417389\n",
      "Train Loss: 0.046491220593452454\n",
      "Train Loss: 0.04641694203019142\n",
      "Train Loss: 0.0463433638215065\n",
      "Train Loss: 0.04626455903053284\n",
      "Train Loss: 0.046190377324819565\n",
      "Train Loss: 0.04612114652991295\n",
      "Train Loss: 0.0460505411028862\n",
      "Train Loss: 0.045971475541591644\n",
      "Train Loss: 0.04590440168976784\n",
      "Train Loss: 0.04582957550883293\n",
      "Train Loss: 0.045754022896289825\n",
      "Train Loss: 0.04568110778927803\n",
      "Train Loss: 0.04561227932572365\n",
      "Train Loss: 0.04554455354809761\n",
      "Train Loss: 0.04547887295484543\n",
      "Train Loss: 0.045407235622406006\n",
      "Train Loss: 0.04533259943127632\n",
      "Train Loss: 0.04526044428348541\n",
      "Train Loss: 0.045189063996076584\n",
      "Train Loss: 0.04512057825922966\n",
      "Train Loss: 0.04505157098174095\n",
      "Train Loss: 0.04497729241847992\n",
      "Train Loss: 0.04490414634346962\n",
      "Train Loss: 0.04483222961425781\n",
      "Train Loss: 0.0447729229927063\n",
      "Train Loss: 0.04471785947680473\n",
      "Train Loss: 0.04465970769524574\n",
      "Train Loss: 0.044591981917619705\n",
      "Train Loss: 0.044522080570459366\n",
      "Train Loss: 0.04444960132241249\n",
      "Train Loss: 0.044375404715538025\n",
      "Train Loss: 0.0443020761013031\n",
      "Train Loss: 0.044235408306121826\n",
      "Train Loss: 0.04417148977518082\n",
      "Train Loss: 0.0441196970641613\n",
      "Train Loss: 0.04406939074397087\n",
      "Train Loss: 0.04400618001818657\n",
      "Train Loss: 0.04394432157278061\n",
      "Train Loss: 0.04387512430548668\n",
      "Train Loss: 0.04381344094872475\n",
      "Train Loss: 0.04374691843986511\n",
      "Train Loss: 0.04368384927511215\n",
      "Train Loss: 0.043625254184007645\n",
      "Train Loss: 0.04356473311781883\n",
      "Train Loss: 0.04349806532263756\n",
      "Train Loss: 0.04343459755182266\n",
      "Train Loss: 0.043373752385377884\n",
      "Train Loss: 0.043312691152095795\n",
      "Train Loss: 0.04325849190354347\n",
      "Train Loss: 0.0432155504822731\n",
      "Train Loss: 0.04317779839038849\n",
      "Train Loss: 0.04311935976147652\n",
      "Train Loss: 0.04305717721581459\n",
      "Train Loss: 0.04298872500658035\n",
      "Train Loss: 0.04292277246713638\n",
      "Train Loss: 0.04286010563373566\n",
      "Train Loss: 0.042799271643161774\n",
      "Train Loss: 0.042752791196107864\n",
      "Train Loss: 0.04270414263010025\n",
      "Train Loss: 0.042662981897592545\n",
      "Train Loss: 0.04262446612119675\n",
      "Train Loss: 0.042570386081933975\n",
      "Train Loss: 0.04250701889395714\n",
      "Train Loss: 0.042443860322237015\n",
      "Train Loss: 0.042389556765556335\n",
      "Train Loss: 0.042347248643636703\n",
      "Train Loss: 0.042308155447244644\n",
      "Train Loss: 0.04225581884384155\n",
      "Train Loss: 0.042197294533252716\n",
      "Train Loss: 0.04214078187942505\n",
      "Train Loss: 0.04209790751338005\n",
      "Train Loss: 0.042052723467350006\n",
      "Train Loss: 0.041998572647571564\n",
      "Train Loss: 0.04194749519228935\n",
      "Train Loss: 0.041894812136888504\n",
      "Train Loss: 0.041851285845041275\n",
      "Train Loss: 0.04181760549545288\n",
      "Train Loss: 0.04179246351122856\n",
      "Train Loss: 0.04177000746130943\n",
      "Train Loss: 0.0417521633207798\n",
      "Train Loss: 0.041718609631061554\n",
      "Train Loss: 0.041675541549921036\n",
      "Train Loss: 0.04162988439202309\n",
      "Train Loss: 0.04157869890332222\n",
      "Train Loss: 0.04152040556073189\n",
      "Train Loss: 0.04146017134189606\n",
      "Train Loss: 0.041400518268346786\n",
      "Train Loss: 0.04134782403707504\n",
      "Train Loss: 0.04129797965288162\n",
      "Train Loss: 0.041249074041843414\n",
      "Train Loss: 0.04120133817195892\n",
      "Train Loss: 0.04115479066967964\n",
      "Train Loss: 0.041111934930086136\n",
      "Train Loss: 0.041077401489019394\n",
      "Train Loss: 0.041038788855075836\n",
      "Train Loss: 0.04099732264876366\n",
      "Train Loss: 0.0409565269947052\n",
      "Train Loss: 0.040913600474596024\n",
      "Train Loss: 0.04087279736995697\n",
      "Train Loss: 0.04082111269235611\n",
      "Train Loss: 0.04076509177684784\n",
      "Train Loss: 0.04071277379989624\n",
      "Train Loss: 0.040670547634363174\n",
      "Train Loss: 0.040637895464897156\n",
      "Train Loss: 0.040595412254333496\n",
      "Train Loss: 0.04054053872823715\n",
      "Train Loss: 0.040483612567186356\n",
      "Train Loss: 0.040435560047626495\n",
      "Train Loss: 0.040391478687524796\n",
      "Train Loss: 0.040346093475818634\n",
      "Train Loss: 0.040302131325006485\n",
      "Train Loss: 0.04026312008500099\n",
      "Train Loss: 0.040211115032434464\n",
      "Train Loss: 0.04015452787280083\n",
      "Train Loss: 0.040102165192365646\n",
      "Train Loss: 0.04005572199821472\n",
      "Train Loss: 0.04002081975340843\n",
      "Train Loss: 0.03997137024998665\n",
      "Train Loss: 0.03991609811782837\n",
      "Train Loss: 0.03986054286360741\n",
      "Train Loss: 0.03981097415089607\n",
      "Train Loss: 0.0397842600941658\n",
      "Train Loss: 0.03974839299917221\n",
      "Train Loss: 0.039700958877801895\n",
      "Train Loss: 0.03965472802519798\n",
      "Train Loss: 0.03961062803864479\n",
      "Train Loss: 0.03956238552927971\n",
      "Train Loss: 0.03951668366789818\n",
      "Train Loss: 0.03947561979293823\n",
      "Train Loss: 0.03944063559174538\n",
      "Train Loss: 0.03940482810139656\n",
      "Train Loss: 0.03936952352523804\n",
      "Train Loss: 0.03934457525610924\n",
      "Train Loss: 0.0393143966794014\n",
      "Train Loss: 0.03928150609135628\n",
      "Train Loss: 0.03924209624528885\n",
      "Train Loss: 0.039200544357299805\n",
      "Train Loss: 0.03916006162762642\n",
      "Train Loss: 0.03911472484469414\n",
      "Train Loss: 0.03906433656811714\n",
      "Train Loss: 0.0390193797647953\n",
      "Train Loss: 0.03899354115128517\n",
      "Train Loss: 0.038960427045822144\n",
      "Train Loss: 0.03892269730567932\n",
      "Train Loss: 0.038881752640008926\n",
      "Train Loss: 0.03884011134505272\n",
      "Train Loss: 0.03879540413618088\n",
      "Train Loss: 0.03875476494431496\n",
      "Train Loss: 0.03871630132198334\n",
      "Train Loss: 0.03867518529295921\n",
      "Train Loss: 0.038637612015008926\n",
      "Train Loss: 0.03858976066112518\n",
      "Train Loss: 0.03854256495833397\n",
      "Train Loss: 0.03850054368376732\n",
      "Train Loss: 0.0384596586227417\n",
      "Train Loss: 0.0384126752614975\n",
      "Train Loss: 0.038367826491594315\n",
      "Train Loss: 0.038326919078826904\n",
      "Train Loss: 0.038283031433820724\n",
      "Train Loss: 0.038237977772951126\n",
      "Train Loss: 0.03819318488240242\n",
      "Train Loss: 0.03815688192844391\n",
      "Train Loss: 0.03812747448682785\n",
      "Train Loss: 0.03809921070933342\n",
      "Train Loss: 0.03806116804480553\n",
      "Train Loss: 0.0380171574652195\n",
      "Train Loss: 0.03798118978738785\n",
      "Train Loss: 0.037952721118927\n",
      "Train Loss: 0.037914078682661057\n",
      "Train Loss: 0.037879206240177155\n",
      "Train Loss: 0.037841737270355225\n",
      "Train Loss: 0.03780420497059822\n",
      "Train Loss: 0.03777674213051796\n",
      "Train Loss: 0.03777410089969635\n",
      "Train Loss: 0.037813954055309296\n",
      "Train Loss: 0.03786982595920563\n",
      "Train Loss: 0.037867892533540726\n",
      "Train Loss: 0.03784267604351044\n",
      "Train Loss: 0.03781057521700859\n",
      "Train Loss: 0.03777149319648743\n",
      "Train Loss: 0.03773324936628342\n",
      "Train Loss: 0.037704579532146454\n",
      "Train Loss: 0.03770659491419792\n",
      "Train Loss: 0.03776157274842262\n",
      "Train Loss: 0.037768349051475525\n",
      "Train Loss: 0.0377485565841198\n",
      "Train Loss: 0.037712156772613525\n",
      "Train Loss: 0.037672996520996094\n",
      "Train Loss: 0.03762844577431679\n",
      "Train Loss: 0.0375945158302784\n",
      "Train Loss: 0.03759907931089401\n",
      "Train Loss: 0.03764495998620987\n",
      "Train Loss: 0.03763720765709877\n",
      "Train Loss: 0.037603046745061874\n",
      "Train Loss: 0.03757110610604286\n",
      "Train Loss: 0.03755101561546326\n",
      "Train Loss: 0.03756481036543846\n",
      "Train Loss: 0.03756289929151535\n",
      "Train Loss: 0.037544745951890945\n",
      "Train Loss: 0.037512555718421936\n",
      "Train Loss: 0.03746654838323593\n",
      "Train Loss: 0.03741568699479103\n",
      "Train Loss: 0.03736642003059387\n",
      "Train Loss: 0.03732050955295563\n",
      "Train Loss: 0.037277981638908386\n",
      "Train Loss: 0.03724035620689392\n",
      "Train Loss: 0.03721607103943825\n",
      "Train Loss: 0.03719758614897728\n",
      "Train Loss: 0.037168651819229126\n",
      "Train Loss: 0.03713618591427803\n",
      "Train Loss: 0.03711723908782005\n",
      "Train Loss: 0.03711877763271332\n",
      "Train Loss: 0.03713212162256241\n",
      "Train Loss: 0.037128742784261703\n",
      "Train Loss: 0.03710078075528145\n",
      "Train Loss: 0.03705637902021408\n",
      "Train Loss: 0.03700389713048935\n",
      "Train Loss: 0.03694971650838852\n",
      "Train Loss: 0.036898087710142136\n",
      "Train Loss: 0.03684912621974945\n",
      "Train Loss: 0.03680841997265816\n",
      "Train Loss: 0.03677472472190857\n",
      "Train Loss: 0.036741096526384354\n",
      "Train Loss: 0.03671259433031082\n",
      "Train Loss: 0.036698371171951294\n",
      "Train Loss: 0.03668753802776337\n",
      "Train Loss: 0.036684729158878326\n",
      "Train Loss: 0.036686453968286514\n",
      "Train Loss: 0.0366717167198658\n",
      "Train Loss: 0.03664542734622955\n",
      "Train Loss: 0.03660834953188896\n",
      "Train Loss: 0.03656606376171112\n",
      "Train Loss: 0.036524172872304916\n",
      "Train Loss: 0.0364878848195076\n",
      "Train Loss: 0.03648363798856735\n",
      "Train Loss: 0.03646903857588768\n",
      "Train Loss: 0.03645169362425804\n",
      "Train Loss: 0.036431532353162766\n",
      "Train Loss: 0.03641446679830551\n",
      "Train Loss: 0.036393776535987854\n",
      "Train Loss: 0.036375463008880615\n",
      "Train Loss: 0.03635649383068085\n",
      "Train Loss: 0.03635628893971443\n",
      "Train Loss: 0.03634602576494217\n",
      "Train Loss: 0.03632146120071411\n",
      "Train Loss: 0.03628679737448692\n",
      "Train Loss: 0.03625182434916496\n",
      "Train Loss: 0.03622330725193024\n",
      "Train Loss: 0.03620193898677826\n",
      "Train Loss: 0.03618578985333443\n",
      "Train Loss: 0.036181624978780746\n",
      "Train Loss: 0.036160510033369064\n",
      "Train Loss: 0.03612682595849037\n",
      "Train Loss: 0.03608445078134537\n",
      "Train Loss: 0.036038581281900406\n",
      "Train Loss: 0.0359952487051487\n",
      "Train Loss: 0.03596227243542671\n",
      "Train Loss: 0.03594088926911354\n",
      "Train Loss: 0.03592709079384804\n",
      "Train Loss: 0.03592289984226227\n",
      "Train Loss: 0.03590310364961624\n",
      "Train Loss: 0.035862356424331665\n",
      "Train Loss: 0.03581802174448967\n",
      "Train Loss: 0.03577464818954468\n",
      "Train Loss: 0.03573580086231232\n",
      "Train Loss: 0.03569644317030907\n",
      "Train Loss: 0.03566938638687134\n",
      "Train Loss: 0.03564668819308281\n",
      "Train Loss: 0.035635024309158325\n",
      "Train Loss: 0.03561602160334587\n",
      "Train Loss: 0.035586465150117874\n",
      "Train Loss: 0.03555087000131607\n",
      "Train Loss: 0.0355120413005352\n",
      "Train Loss: 0.035482365638017654\n",
      "Train Loss: 0.035461679100990295\n",
      "Train Loss: 0.035448234528303146\n",
      "Train Loss: 0.03543664887547493\n",
      "Train Loss: 0.03544292971491814\n",
      "Train Loss: 0.0354764424264431\n",
      "Train Loss: 0.03552064672112465\n",
      "Train Loss: 0.035538893193006516\n",
      "Train Loss: 0.03553934395313263\n",
      "Train Loss: 0.03553291782736778\n",
      "Train Loss: 0.035523124039173126\n",
      "Train Loss: 0.035510867834091187\n",
      "Train Loss: 0.03550126776099205\n",
      "Train Loss: 0.03550735488533974\n",
      "Train Loss: 0.03552120923995972\n",
      "Train Loss: 0.035512689501047134\n",
      "Train Loss: 0.035497378557920456\n",
      "Train Loss: 0.035473961383104324\n",
      "Train Loss: 0.03545861691236496\n",
      "Train Loss: 0.0354565866291523\n",
      "Train Loss: 0.03546927496790886\n",
      "Train Loss: 0.03547723963856697\n",
      "Train Loss: 0.035467639565467834\n",
      "Train Loss: 0.03544618934392929\n",
      "Train Loss: 0.03541480377316475\n",
      "Train Loss: 0.035375840961933136\n",
      "Train Loss: 0.035332582890987396\n",
      "Train Loss: 0.03528766706585884\n",
      "Train Loss: 0.03524377942085266\n",
      "Train Loss: 0.03520668298006058\n",
      "Train Loss: 0.035179853439331055\n",
      "Train Loss: 0.03516053780913353\n",
      "Train Loss: 0.03515666723251343\n",
      "Train Loss: 0.03518080338835716\n",
      "Train Loss: 0.03519560396671295\n",
      "Train Loss: 0.03518444299697876\n",
      "Train Loss: 0.03515706583857536\n",
      "Train Loss: 0.035123519599437714\n",
      "Train Loss: 0.03508469834923744\n",
      "Train Loss: 0.03504453971982002\n",
      "Train Loss: 0.03500646352767944\n",
      "Train Loss: 0.034970398992300034\n",
      "Train Loss: 0.03494353964924812\n",
      "Train Loss: 0.0349288173019886\n",
      "Train Loss: 0.03494086116552353\n",
      "Train Loss: 0.03495141863822937\n",
      "Train Loss: 0.03494077920913696\n",
      "Train Loss: 0.034916676580905914\n",
      "Train Loss: 0.034884579479694366\n",
      "Train Loss: 0.03484706953167915\n",
      "Train Loss: 0.03480566293001175\n",
      "Train Loss: 0.034765295684337616\n",
      "Train Loss: 0.034727793186903\n",
      "Train Loss: 0.03469989076256752\n",
      "Train Loss: 0.03468666225671768\n",
      "Train Loss: 0.03468233719468117\n",
      "Train Loss: 0.03467612713575363\n",
      "Train Loss: 0.034657418727874756\n",
      "Train Loss: 0.03462465479969978\n",
      "Train Loss: 0.03458255156874657\n",
      "Train Loss: 0.034540191292762756\n",
      "Train Loss: 0.034500498324632645\n",
      "Train Loss: 0.034461457282304764\n",
      "Train Loss: 0.034425199031829834\n",
      "Train Loss: 0.034393925219774246\n",
      "Train Loss: 0.03438725695014\n",
      "Train Loss: 0.03438442572951317\n",
      "Train Loss: 0.034361276775598526\n",
      "Train Loss: 0.03432858735322952\n",
      "Train Loss: 0.03429647162556648\n",
      "Train Loss: 0.03425734490156174\n",
      "Train Loss: 0.034214213490486145\n",
      "Train Loss: 0.03417007625102997\n",
      "Train Loss: 0.03412848338484764\n",
      "Train Loss: 0.03409331291913986\n",
      "Train Loss: 0.034067604690790176\n",
      "Train Loss: 0.034044887870550156\n",
      "Train Loss: 0.0340256467461586\n",
      "Train Loss: 0.03401003032922745\n",
      "Train Loss: 0.03398747742176056\n",
      "Train Loss: 0.033963222056627274\n",
      "Train Loss: 0.033939823508262634\n",
      "Train Loss: 0.03392134606838226\n",
      "Train Loss: 0.03390568122267723\n",
      "Train Loss: 0.03389626741409302\n",
      "Train Loss: 0.033875882625579834\n",
      "Train Loss: 0.03384745866060257\n",
      "Train Loss: 0.03381635621190071\n",
      "Train Loss: 0.033797044306993484\n",
      "Train Loss: 0.03378632292151451\n",
      "Train Loss: 0.03378745913505554\n",
      "Train Loss: 0.033782102167606354\n",
      "Train Loss: 0.0337802991271019\n",
      "Train Loss: 0.03377485275268555\n",
      "Train Loss: 0.03376726433634758\n",
      "Train Loss: 0.03375151380896568\n",
      "Train Loss: 0.03373561426997185\n",
      "Train Loss: 0.03373437002301216\n",
      "Train Loss: 0.033731162548065186\n",
      "Train Loss: 0.03372589126229286\n",
      "Train Loss: 0.03372668847441673\n",
      "Train Loss: 0.03372731804847717\n",
      "Train Loss: 0.03372231870889664\n",
      "Train Loss: 0.033734146505594254\n",
      "Train Loss: 0.033750616014003754\n",
      "Train Loss: 0.033750541508197784\n",
      "Train Loss: 0.03373809903860092\n",
      "Train Loss: 0.033713340759277344\n",
      "Train Loss: 0.03368106484413147\n",
      "Train Loss: 0.033647775650024414\n",
      "Train Loss: 0.03362149000167847\n",
      "Train Loss: 0.03360605239868164\n",
      "Train Loss: 0.033590056002140045\n",
      "Train Loss: 0.033575043082237244\n",
      "Train Loss: 0.03354975953698158\n",
      "Train Loss: 0.033518023788928986\n",
      "Train Loss: 0.03348597511649132\n",
      "Train Loss: 0.033466923981904984\n",
      "Train Loss: 0.03345050290226936\n",
      "Train Loss: 0.03343513235449791\n",
      "Train Loss: 0.033418066799640656\n",
      "Train Loss: 0.03340378776192665\n",
      "Train Loss: 0.03339921683073044\n",
      "Train Loss: 0.0333930104970932\n",
      "Train Loss: 0.033378373831510544\n",
      "Train Loss: 0.03336328640580177\n",
      "Train Loss: 0.0333406925201416\n",
      "Train Loss: 0.033313777297735214\n",
      "Train Loss: 0.0332878902554512\n",
      "Train Loss: 0.03327023237943649\n",
      "Train Loss: 0.033258698880672455\n",
      "Train Loss: 0.03324640914797783\n",
      "Train Loss: 0.03323180228471756\n",
      "Train Loss: 0.03321831673383713\n",
      "Train Loss: 0.033204808831214905\n",
      "Train Loss: 0.033192701637744904\n",
      "Train Loss: 0.03317743167281151\n",
      "Train Loss: 0.03316742181777954\n",
      "Train Loss: 0.0331558994948864\n",
      "Train Loss: 0.03314138576388359\n",
      "Train Loss: 0.03312668576836586\n",
      "Train Loss: 0.03310932219028473\n",
      "Train Loss: 0.03309476375579834\n",
      "Train Loss: 0.03308573737740517\n",
      "Train Loss: 0.03307328000664711\n",
      "Train Loss: 0.033064648509025574\n",
      "Train Loss: 0.03306039422750473\n",
      "Train Loss: 0.033055856823921204\n",
      "Train Loss: 0.03304486349225044\n",
      "Train Loss: 0.0330297015607357\n",
      "Train Loss: 0.03301338106393814\n",
      "Train Loss: 0.032997794449329376\n",
      "Train Loss: 0.03298488259315491\n",
      "Train Loss: 0.032973356544971466\n",
      "Train Loss: 0.03296734765172005\n",
      "Train Loss: 0.03295578807592392\n",
      "Train Loss: 0.03294271230697632\n",
      "Train Loss: 0.032929956912994385\n",
      "Train Loss: 0.03292189538478851\n",
      "Train Loss: 0.032916877418756485\n",
      "Train Loss: 0.03290598839521408\n",
      "Train Loss: 0.032899159938097\n",
      "Train Loss: 0.032883238047361374\n",
      "Train Loss: 0.03286079317331314\n",
      "Train Loss: 0.03284032270312309\n",
      "Train Loss: 0.032823093235492706\n",
      "Train Loss: 0.032804034650325775\n",
      "Train Loss: 0.032782986760139465\n",
      "Train Loss: 0.03276398032903671\n",
      "Train Loss: 0.03275088593363762\n",
      "Train Loss: 0.03273649886250496\n",
      "Train Loss: 0.03271953761577606\n",
      "Train Loss: 0.03270541504025459\n",
      "Train Loss: 0.032690148800611496\n",
      "Train Loss: 0.03267829492688179\n",
      "Train Loss: 0.03266535699367523\n",
      "Train Loss: 0.03265058249235153\n",
      "Train Loss: 0.03263626620173454\n",
      "Train Loss: 0.032631807029247284\n",
      "Train Loss: 0.03263125568628311\n",
      "Train Loss: 0.03262901306152344\n",
      "Train Loss: 0.03262506425380707\n",
      "Train Loss: 0.03261256590485573\n",
      "Train Loss: 0.03259897604584694\n",
      "Train Loss: 0.03258902207016945\n",
      "Train Loss: 0.03257835656404495\n",
      "Train Loss: 0.03256208077073097\n",
      "Train Loss: 0.032548148185014725\n",
      "Train Loss: 0.03253534808754921\n",
      "Train Loss: 0.03252304717898369\n",
      "Train Loss: 0.03251071274280548\n",
      "Train Loss: 0.032499562948942184\n",
      "Train Loss: 0.03248399868607521\n",
      "Train Loss: 0.03246389329433441\n",
      "Train Loss: 0.032448288053274155\n",
      "Train Loss: 0.03243641182780266\n",
      "Train Loss: 0.032424140721559525\n",
      "Train Loss: 0.032409556210041046\n",
      "Train Loss: 0.03239339962601662\n",
      "Train Loss: 0.0323784276843071\n",
      "Train Loss: 0.0323634035885334\n",
      "Train Loss: 0.0323474183678627\n",
      "Train Loss: 0.03232969716191292\n",
      "Train Loss: 0.03231240063905716\n",
      "Train Loss: 0.03229478374123573\n",
      "Train Loss: 0.03227929398417473\n",
      "Train Loss: 0.032265424728393555\n",
      "Train Loss: 0.03225120157003403\n",
      "Train Loss: 0.032234348356723785\n",
      "Train Loss: 0.032218869775533676\n",
      "Train Loss: 0.03220469132065773\n",
      "Train Loss: 0.03219326213002205\n",
      "Train Loss: 0.03218064084649086\n",
      "Train Loss: 0.032169900834560394\n",
      "Train Loss: 0.03215968236327171\n",
      "Train Loss: 0.032151345163583755\n",
      "Train Loss: 0.032148271799087524\n",
      "Train Loss: 0.03214754909276962\n",
      "Train Loss: 0.03215158358216286\n",
      "Train Loss: 0.03215595707297325\n",
      "Train Loss: 0.03215831145644188\n",
      "Train Loss: 0.032155077904462814\n",
      "Train Loss: 0.03215130791068077\n",
      "Train Loss: 0.03214104473590851\n",
      "Train Loss: 0.03212457150220871\n",
      "Train Loss: 0.032107263803482056\n",
      "Train Loss: 0.03209158033132553\n",
      "Train Loss: 0.0320814847946167\n",
      "Train Loss: 0.032066408544778824\n",
      "Train Loss: 0.03205101564526558\n",
      "Train Loss: 0.032036297023296356\n",
      "Train Loss: 0.03201909735798836\n",
      "Train Loss: 0.03200056776404381\n",
      "Train Loss: 0.03198504447937012\n",
      "Train Loss: 0.03196820616722107\n",
      "Train Loss: 0.0319465734064579\n",
      "Train Loss: 0.031923867762088776\n",
      "Train Loss: 0.031905509531497955\n",
      "Train Loss: 0.031887516379356384\n",
      "Train Loss: 0.03186718001961708\n",
      "Train Loss: 0.03184790536761284\n",
      "Train Loss: 0.03182889148592949\n",
      "Train Loss: 0.03181302919983864\n",
      "Train Loss: 0.03179900348186493\n",
      "Train Loss: 0.03178073465824127\n",
      "Train Loss: 0.031758811324834824\n",
      "Train Loss: 0.031733423471450806\n",
      "Train Loss: 0.03171131759881973\n",
      "Train Loss: 0.03169511258602142\n",
      "Train Loss: 0.03167818486690521\n",
      "Train Loss: 0.03165481239557266\n",
      "Train Loss: 0.03162815421819687\n",
      "Train Loss: 0.03160018473863602\n",
      "Train Loss: 0.03157268837094307\n",
      "Train Loss: 0.03154744952917099\n",
      "Train Loss: 0.03152751550078392\n",
      "Train Loss: 0.03151105344295502\n",
      "Train Loss: 0.03149332478642464\n",
      "Train Loss: 0.031474120914936066\n",
      "Train Loss: 0.03144882246851921\n",
      "Train Loss: 0.03142521157860756\n",
      "Train Loss: 0.031405892223119736\n",
      "Train Loss: 0.03138484060764313\n",
      "Train Loss: 0.031360283493995667\n",
      "Train Loss: 0.03133832663297653\n",
      "Train Loss: 0.03132107853889465\n",
      "Train Loss: 0.03130609542131424\n",
      "Train Loss: 0.03128938749432564\n",
      "Train Loss: 0.03126749396324158\n",
      "Train Loss: 0.03124450147151947\n",
      "Train Loss: 0.031230805441737175\n",
      "Train Loss: 0.03122217208147049\n",
      "Train Loss: 0.031208442524075508\n",
      "Train Loss: 0.03119652345776558\n",
      "Train Loss: 0.031184105202555656\n",
      "Train Loss: 0.03116784431040287\n",
      "Train Loss: 0.03116169385612011\n",
      "Train Loss: 0.03115237131714821\n",
      "Train Loss: 0.031138284131884575\n",
      "Train Loss: 0.031123386695981026\n",
      "Train Loss: 0.03110612742602825\n",
      "Train Loss: 0.031094444915652275\n",
      "Train Loss: 0.0310849379748106\n",
      "Train Loss: 0.0310770645737648\n",
      "Train Loss: 0.031070474535226822\n",
      "Train Loss: 0.03106815181672573\n",
      "Train Loss: 0.031067246571183205\n",
      "Train Loss: 0.03106745332479477\n",
      "Train Loss: 0.031069112941622734\n",
      "Train Loss: 0.031074179336428642\n",
      "Train Loss: 0.031070541590452194\n",
      "Train Loss: 0.031060773879289627\n",
      "Train Loss: 0.03105292282998562\n",
      "Train Loss: 0.031044449657201767\n",
      "Train Loss: 0.031033970415592194\n",
      "Train Loss: 0.031022071838378906\n",
      "Train Loss: 0.031009074300527573\n",
      "Train Loss: 0.03099800832569599\n",
      "Train Loss: 0.030985886231064796\n",
      "Train Loss: 0.030972031876444817\n",
      "Train Loss: 0.03096148744225502\n",
      "Train Loss: 0.030949115753173828\n",
      "Train Loss: 0.03093448095023632\n",
      "Train Loss: 0.03092213347554207\n",
      "Train Loss: 0.030908644199371338\n",
      "Train Loss: 0.030891943722963333\n",
      "Train Loss: 0.030875997617840767\n",
      "Train Loss: 0.03085736557841301\n",
      "Train Loss: 0.030838457867503166\n",
      "Train Loss: 0.03082067333161831\n",
      "Train Loss: 0.03080477938055992\n",
      "Train Loss: 0.030791737139225006\n",
      "Train Loss: 0.030773818492889404\n",
      "Train Loss: 0.030757151544094086\n",
      "Train Loss: 0.03074003756046295\n",
      "Train Loss: 0.030722593888640404\n",
      "Train Loss: 0.030706454068422318\n",
      "Train Loss: 0.03069249354302883\n",
      "Train Loss: 0.03068213164806366\n",
      "Train Loss: 0.030667269602417946\n",
      "Train Loss: 0.03065178170800209\n",
      "Train Loss: 0.03063812479376793\n",
      "Train Loss: 0.030622437596321106\n",
      "Train Loss: 0.030608324334025383\n",
      "Train Loss: 0.030590170994400978\n",
      "Train Loss: 0.030568484216928482\n",
      "Train Loss: 0.03055051900446415\n",
      "Train Loss: 0.030536752194166183\n",
      "Train Loss: 0.03052341751754284\n",
      "Train Loss: 0.030506940558552742\n",
      "Train Loss: 0.03049391135573387\n",
      "Train Loss: 0.030484097078442574\n",
      "Train Loss: 0.030472159385681152\n",
      "Train Loss: 0.030460072681307793\n",
      "Train Loss: 0.03044796548783779\n",
      "Train Loss: 0.030436139553785324\n",
      "Train Loss: 0.03042961098253727\n",
      "Train Loss: 0.030416935682296753\n",
      "Train Loss: 0.030401691794395447\n",
      "Train Loss: 0.030387477949261665\n",
      "Train Loss: 0.030367029830813408\n",
      "Train Loss: 0.030345577746629715\n",
      "Train Loss: 0.030324768275022507\n",
      "Train Loss: 0.030305396765470505\n",
      "Train Loss: 0.030290834605693817\n",
      "Train Loss: 0.030272245407104492\n",
      "Train Loss: 0.030253442004323006\n",
      "Train Loss: 0.030238227918744087\n",
      "Train Loss: 0.030223675072193146\n",
      "Train Loss: 0.030207354575395584\n",
      "Train Loss: 0.03019489347934723\n",
      "Train Loss: 0.030185043811798096\n",
      "Train Loss: 0.030173709616065025\n",
      "Train Loss: 0.03016275353729725\n",
      "Train Loss: 0.03015010990202427\n",
      "Train Loss: 0.030143877491354942\n",
      "Train Loss: 0.030136289075016975\n",
      "Train Loss: 0.030128851532936096\n",
      "Train Loss: 0.03011440485715866\n",
      "Train Loss: 0.03009897656738758\n",
      "Train Loss: 0.03008529171347618\n",
      "Train Loss: 0.030068743973970413\n",
      "Train Loss: 0.030049145221710205\n",
      "Train Loss: 0.030030280351638794\n",
      "Train Loss: 0.030014798045158386\n",
      "Train Loss: 0.029999740421772003\n",
      "Train Loss: 0.029983242973685265\n",
      "Train Loss: 0.02996750921010971\n",
      "Train Loss: 0.02994922548532486\n",
      "Train Loss: 0.0299332607537508\n",
      "Train Loss: 0.02991839498281479\n",
      "Train Loss: 0.029901310801506042\n",
      "Train Loss: 0.029883988201618195\n",
      "Train Loss: 0.029866201803088188\n",
      "Train Loss: 0.029847072437405586\n",
      "Train Loss: 0.029831858351826668\n",
      "Train Loss: 0.029819823801517487\n",
      "Train Loss: 0.029808277264237404\n",
      "Train Loss: 0.02980337291955948\n",
      "Train Loss: 0.029805218800902367\n",
      "Train Loss: 0.02980932593345642\n",
      "Train Loss: 0.0298137366771698\n",
      "Train Loss: 0.029818851500749588\n",
      "Train Loss: 0.02982470393180847\n",
      "Train Loss: 0.02983427420258522\n",
      "Train Loss: 0.02984161302447319\n",
      "Train Loss: 0.029839424416422844\n",
      "Train Loss: 0.02983218804001808\n",
      "Train Loss: 0.029818596318364143\n",
      "Train Loss: 0.029804032295942307\n",
      "Train Loss: 0.029788337647914886\n",
      "Train Loss: 0.029772425070405006\n",
      "Train Loss: 0.029756952077150345\n",
      "Train Loss: 0.029740819707512856\n",
      "Train Loss: 0.029723262414336205\n",
      "Train Loss: 0.02970759943127632\n",
      "Train Loss: 0.029692644253373146\n",
      "Train Loss: 0.02968011610209942\n",
      "Train Loss: 0.029666097834706306\n",
      "Train Loss: 0.029652761295437813\n",
      "Train Loss: 0.029640793800354004\n",
      "Train Loss: 0.029626518487930298\n",
      "Train Loss: 0.029611986130475998\n",
      "Train Loss: 0.029600482434034348\n",
      "Train Loss: 0.029593508690595627\n",
      "Train Loss: 0.029587870463728905\n",
      "Train Loss: 0.029583867639303207\n",
      "Train Loss: 0.029579851776361465\n",
      "Train Loss: 0.029574133455753326\n",
      "Train Loss: 0.029570143669843674\n",
      "Train Loss: 0.029559791088104248\n",
      "Train Loss: 0.0295462217181921\n",
      "Train Loss: 0.02953588217496872\n",
      "Train Loss: 0.02952764369547367\n",
      "Train Loss: 0.02951871044933796\n",
      "Train Loss: 0.029505819082260132\n",
      "Train Loss: 0.029498230665922165\n",
      "Train Loss: 0.0294931810349226\n",
      "Train Loss: 0.029485778883099556\n",
      "Train Loss: 0.029473649337887764\n",
      "Train Loss: 0.02946198359131813\n",
      "Train Loss: 0.029450980946421623\n",
      "Train Loss: 0.029436888173222542\n",
      "Train Loss: 0.029421785846352577\n",
      "Train Loss: 0.02940615452826023\n",
      "Train Loss: 0.029389604926109314\n",
      "Train Loss: 0.029371313750743866\n",
      "Train Loss: 0.029355578124523163\n",
      "Train Loss: 0.029341833665966988\n",
      "Train Loss: 0.029327983036637306\n",
      "Train Loss: 0.029312465339899063\n",
      "Train Loss: 0.029297497123479843\n",
      "Train Loss: 0.029288025572896004\n",
      "Train Loss: 0.029274648055434227\n",
      "Train Loss: 0.02925676293671131\n",
      "Train Loss: 0.02923821657896042\n",
      "Train Loss: 0.02921925112605095\n",
      "Train Loss: 0.029202314093708992\n",
      "Train Loss: 0.02918541245162487\n",
      "Train Loss: 0.02917090244591236\n",
      "Train Loss: 0.02915683388710022\n",
      "Train Loss: 0.029143011197447777\n",
      "Train Loss: 0.02912834845483303\n",
      "Train Loss: 0.029115330427885056\n",
      "Train Loss: 0.029102351516485214\n",
      "Train Loss: 0.02908824384212494\n",
      "Train Loss: 0.029073772951960564\n",
      "Train Loss: 0.029062652960419655\n",
      "Train Loss: 0.02905546873807907\n",
      "Train Loss: 0.02904987521469593\n",
      "Train Loss: 0.029043979942798615\n",
      "Train Loss: 0.029038194566965103\n",
      "Train Loss: 0.02903628535568714\n",
      "Train Loss: 0.02903018705546856\n",
      "Train Loss: 0.029025018215179443\n",
      "Train Loss: 0.029019543901085854\n",
      "Train Loss: 0.02900928445160389\n",
      "Train Loss: 0.028994716703891754\n",
      "Train Loss: 0.028978539630770683\n",
      "Train Loss: 0.028961870819330215\n",
      "Train Loss: 0.028944537043571472\n",
      "Train Loss: 0.02892630733549595\n",
      "Train Loss: 0.028908954933285713\n",
      "Train Loss: 0.028893059119582176\n",
      "Train Loss: 0.028878042474389076\n",
      "Train Loss: 0.02886250801384449\n",
      "Train Loss: 0.028848590329289436\n",
      "Train Loss: 0.02883571945130825\n",
      "Train Loss: 0.0288220401853323\n",
      "Train Loss: 0.028805652633309364\n",
      "Train Loss: 0.028793854638934135\n",
      "Train Loss: 0.028781479224562645\n",
      "Train Loss: 0.028765957802534103\n",
      "Train Loss: 0.028749819844961166\n",
      "Train Loss: 0.028734682127833366\n",
      "Train Loss: 0.028722062706947327\n",
      "Train Loss: 0.028708837926387787\n",
      "Train Loss: 0.028692016378045082\n",
      "Train Loss: 0.028678443282842636\n",
      "Train Loss: 0.02866518124938011\n",
      "Train Loss: 0.028651176020503044\n",
      "Train Loss: 0.0286368690431118\n",
      "Train Loss: 0.028623411431908607\n",
      "Train Loss: 0.02861287072300911\n",
      "Train Loss: 0.02860664576292038\n",
      "Train Loss: 0.0286011453717947\n",
      "Train Loss: 0.028597097843885422\n",
      "Train Loss: 0.02858779951930046\n",
      "Train Loss: 0.02857872098684311\n",
      "Train Loss: 0.028564738109707832\n",
      "Train Loss: 0.028548909351229668\n",
      "Train Loss: 0.0285345409065485\n",
      "Train Loss: 0.02851932682096958\n",
      "Train Loss: 0.028503384441137314\n",
      "Train Loss: 0.028486765921115875\n",
      "Train Loss: 0.028470901772379875\n",
      "Train Loss: 0.02845519408583641\n",
      "Train Loss: 0.028440095484256744\n",
      "Train Loss: 0.028422433882951736\n",
      "Train Loss: 0.028403762727975845\n",
      "Train Loss: 0.02838604338467121\n",
      "Train Loss: 0.028369968757033348\n",
      "Train Loss: 0.0283539816737175\n",
      "Train Loss: 0.028338508680462837\n",
      "Train Loss: 0.028324998915195465\n",
      "Train Loss: 0.028312738984823227\n",
      "Train Loss: 0.02829982526600361\n",
      "Train Loss: 0.028285520151257515\n",
      "Train Loss: 0.028271116316318512\n",
      "Train Loss: 0.028260156512260437\n",
      "Train Loss: 0.02825072407722473\n",
      "Train Loss: 0.02824425883591175\n",
      "Train Loss: 0.028238985687494278\n",
      "Train Loss: 0.028239890933036804\n",
      "Train Loss: 0.0282420814037323\n",
      "Train Loss: 0.028246412053704262\n",
      "Train Loss: 0.028246182948350906\n",
      "Train Loss: 0.028245313093066216\n",
      "Train Loss: 0.028246188536286354\n",
      "Train Loss: 0.02824489399790764\n",
      "Train Loss: 0.028241267427802086\n",
      "Train Loss: 0.028235025703907013\n",
      "Train Loss: 0.028227366507053375\n",
      "Train Loss: 0.028219424188137054\n",
      "Train Loss: 0.028212733566761017\n",
      "Train Loss: 0.028207411989569664\n",
      "Train Loss: 0.028203394263982773\n",
      "Train Loss: 0.02820047177374363\n",
      "Train Loss: 0.02820047363638878\n",
      "Train Loss: 0.028201887384057045\n",
      "Train Loss: 0.028202202171087265\n",
      "Train Loss: 0.028201548382639885\n",
      "Train Loss: 0.028203794732689857\n",
      "Train Loss: 0.028199423104524612\n",
      "Train Loss: 0.028189804404973984\n",
      "Train Loss: 0.02818158268928528\n",
      "Train Loss: 0.028171749785542488\n",
      "Train Loss: 0.028162341564893723\n",
      "Train Loss: 0.02815311774611473\n",
      "Train Loss: 0.028150295838713646\n",
      "Train Loss: 0.028144948184490204\n",
      "Train Loss: 0.028136953711509705\n",
      "Train Loss: 0.02812938764691353\n",
      "Train Loss: 0.028124460950493813\n",
      "Train Loss: 0.02811451628804207\n",
      "Train Loss: 0.028106538578867912\n",
      "Train Loss: 0.028099995106458664\n",
      "Train Loss: 0.028088310733437538\n",
      "Train Loss: 0.028078269213438034\n",
      "Train Loss: 0.02806449495255947\n",
      "Train Loss: 0.028048809617757797\n",
      "Train Loss: 0.028033006936311722\n",
      "Train Loss: 0.028017690405249596\n",
      "Train Loss: 0.028002694249153137\n",
      "Train Loss: 0.02799009159207344\n",
      "Train Loss: 0.027978697791695595\n",
      "Train Loss: 0.027967168018221855\n",
      "Train Loss: 0.027955595403909683\n",
      "Train Loss: 0.02793966233730316\n",
      "Train Loss: 0.027923215180635452\n",
      "Train Loss: 0.02791101671755314\n",
      "Train Loss: 0.027897968888282776\n",
      "Train Loss: 0.02788308821618557\n",
      "Train Loss: 0.02786839008331299\n",
      "Train Loss: 0.027854492887854576\n",
      "Train Loss: 0.027841441333293915\n",
      "Train Loss: 0.02782747894525528\n",
      "Train Loss: 0.027813047170639038\n",
      "Train Loss: 0.02779863215982914\n",
      "Train Loss: 0.02778487280011177\n",
      "Train Loss: 0.02777085267007351\n",
      "Train Loss: 0.02775542065501213\n",
      "Train Loss: 0.02774037979543209\n",
      "Train Loss: 0.027726322412490845\n",
      "Train Loss: 0.02771195024251938\n",
      "Train Loss: 0.027695918455719948\n",
      "Train Loss: 0.02767980471253395\n",
      "Train Loss: 0.027665769681334496\n",
      "Train Loss: 0.027650820091366768\n",
      "Train Loss: 0.02763492800295353\n",
      "Train Loss: 0.027618957683444023\n",
      "Train Loss: 0.027603454887866974\n",
      "Train Loss: 0.027588412165641785\n",
      "Train Loss: 0.027575217187404633\n",
      "Train Loss: 0.027562135830521584\n",
      "Train Loss: 0.027547379955649376\n",
      "Train Loss: 0.02753281779587269\n",
      "Train Loss: 0.0275191031396389\n",
      "Train Loss: 0.02750878781080246\n",
      "Train Loss: 0.02749958448112011\n",
      "Train Loss: 0.027493229135870934\n",
      "Train Loss: 0.027485981583595276\n",
      "Train Loss: 0.027485312893986702\n",
      "Train Loss: 0.02748396061360836\n",
      "Train Loss: 0.02748245559632778\n",
      "Train Loss: 0.02747930958867073\n",
      "Train Loss: 0.027478815987706184\n",
      "Train Loss: 0.027478400617837906\n",
      "Train Loss: 0.02747501991689205\n",
      "Train Loss: 0.027468184009194374\n",
      "Train Loss: 0.02746136672794819\n",
      "Train Loss: 0.027454541996121407\n",
      "Train Loss: 0.027445532381534576\n",
      "Train Loss: 0.02743556722998619\n",
      "Train Loss: 0.027424557134509087\n",
      "Train Loss: 0.027412094175815582\n",
      "Train Loss: 0.027398796752095222\n",
      "Train Loss: 0.027387214824557304\n",
      "Train Loss: 0.027376942336559296\n",
      "Train Loss: 0.027367033064365387\n",
      "Train Loss: 0.027358314022421837\n",
      "Train Loss: 0.027352891862392426\n",
      "Train Loss: 0.02735240012407303\n",
      "Train Loss: 0.02735002152621746\n",
      "Train Loss: 0.027350621297955513\n",
      "Train Loss: 0.02734743058681488\n",
      "Train Loss: 0.027342835441231728\n",
      "Train Loss: 0.027331002056598663\n",
      "Train Loss: 0.027321862056851387\n",
      "Train Loss: 0.027310624718666077\n",
      "Train Loss: 0.027295172214508057\n",
      "Train Loss: 0.027279386296868324\n",
      "Train Loss: 0.027263548225164413\n",
      "Train Loss: 0.027247563004493713\n",
      "Train Loss: 0.027234701439738274\n",
      "Train Loss: 0.027220824733376503\n",
      "Train Loss: 0.027205197140574455\n",
      "Train Loss: 0.027188127860426903\n",
      "Train Loss: 0.027171049267053604\n",
      "Train Loss: 0.027155796065926552\n",
      "Train Loss: 0.027140477672219276\n",
      "Train Loss: 0.027127137407660484\n",
      "Train Loss: 0.027113329619169235\n",
      "Train Loss: 0.02709854207932949\n",
      "Train Loss: 0.02708405815064907\n",
      "Train Loss: 0.027069641277194023\n",
      "Train Loss: 0.027056481689214706\n",
      "Train Loss: 0.027044035494327545\n",
      "Train Loss: 0.02703133411705494\n",
      "Train Loss: 0.027017811313271523\n",
      "Train Loss: 0.027005048468708992\n",
      "Train Loss: 0.02699127048254013\n",
      "Train Loss: 0.0269803274422884\n",
      "Train Loss: 0.02697289176285267\n",
      "Train Loss: 0.02696816623210907\n",
      "Train Loss: 0.026968322694301605\n",
      "Train Loss: 0.026971710845828056\n",
      "Train Loss: 0.02697541005909443\n",
      "Train Loss: 0.02698618173599243\n",
      "Train Loss: 0.02699742652475834\n",
      "Train Loss: 0.027010351419448853\n",
      "Train Loss: 0.027016622945666313\n",
      "Train Loss: 0.02701726369559765\n",
      "Train Loss: 0.027009587734937668\n",
      "Train Loss: 0.02699885331094265\n",
      "Train Loss: 0.026984620839357376\n",
      "Train Loss: 0.0269689429551363\n",
      "Train Loss: 0.026954738423228264\n",
      "Train Loss: 0.0269422996789217\n",
      "Train Loss: 0.026937102898955345\n",
      "Train Loss: 0.026926595717668533\n",
      "Train Loss: 0.026915695518255234\n",
      "Train Loss: 0.026903517544269562\n",
      "Train Loss: 0.02688906341791153\n",
      "Train Loss: 0.026875561103224754\n",
      "Train Loss: 0.026863737031817436\n",
      "Train Loss: 0.02685404382646084\n",
      "Train Loss: 0.026844747364521027\n",
      "Train Loss: 0.02683517523109913\n",
      "Train Loss: 0.026824746280908585\n",
      "Train Loss: 0.026816200464963913\n",
      "Train Loss: 0.026808882132172585\n",
      "Train Loss: 0.026801399886608124\n",
      "Train Loss: 0.026792285963892937\n",
      "Train Loss: 0.0267843808978796\n",
      "Train Loss: 0.026775943115353584\n",
      "Train Loss: 0.026767341420054436\n",
      "Train Loss: 0.026759272441267967\n",
      "Train Loss: 0.026750482618808746\n",
      "Train Loss: 0.026743540540337563\n",
      "Train Loss: 0.026734597980976105\n",
      "Train Loss: 0.026725078001618385\n",
      "Train Loss: 0.02671605721116066\n",
      "Train Loss: 0.026709256693720818\n",
      "Train Loss: 0.026703791692852974\n",
      "Train Loss: 0.026697199791669846\n",
      "Train Loss: 0.026689650490880013\n",
      "Train Loss: 0.0266854390501976\n",
      "Train Loss: 0.026684556156396866\n",
      "Train Loss: 0.026684291660785675\n",
      "Train Loss: 0.026682700961828232\n",
      "Train Loss: 0.026677144691348076\n",
      "Train Loss: 0.026670312508940697\n",
      "Train Loss: 0.02666570618748665\n",
      "Train Loss: 0.026658855378627777\n",
      "Train Loss: 0.026649603620171547\n",
      "Train Loss: 0.02664383314549923\n",
      "Train Loss: 0.0266402680426836\n",
      "Train Loss: 0.026632508262991905\n",
      "Train Loss: 0.02662423625588417\n",
      "Train Loss: 0.026616008952260017\n",
      "Train Loss: 0.02660844847559929\n",
      "Train Loss: 0.02659773640334606\n",
      "Train Loss: 0.026585884392261505\n",
      "Train Loss: 0.026577016338706017\n",
      "Train Loss: 0.02656743861734867\n",
      "Train Loss: 0.026556579396128654\n",
      "Train Loss: 0.026546135544776917\n",
      "Train Loss: 0.026533205062150955\n",
      "Train Loss: 0.02651997096836567\n",
      "Train Loss: 0.026509838178753853\n",
      "Train Loss: 0.026498273015022278\n",
      "Train Loss: 0.026484744623303413\n",
      "Train Loss: 0.026470983400940895\n",
      "Train Loss: 0.026458127424120903\n",
      "Train Loss: 0.02644594945013523\n",
      "Train Loss: 0.02643563225865364\n",
      "Train Loss: 0.026424497365951538\n",
      "Train Loss: 0.02641214430332184\n",
      "Train Loss: 0.02639888972043991\n",
      "Train Loss: 0.026387883350253105\n",
      "Train Loss: 0.026377400383353233\n",
      "Train Loss: 0.026364849880337715\n",
      "Train Loss: 0.02635122835636139\n",
      "Train Loss: 0.026340926066040993\n",
      "Train Loss: 0.026330914348363876\n",
      "Train Loss: 0.026318462565541267\n",
      "Train Loss: 0.02630607970058918\n",
      "Train Loss: 0.02629559300839901\n",
      "Train Loss: 0.02628484182059765\n",
      "Train Loss: 0.026271648705005646\n",
      "Train Loss: 0.02625887654721737\n",
      "Train Loss: 0.026248004287481308\n",
      "Train Loss: 0.026235997676849365\n",
      "Train Loss: 0.026222702115774155\n",
      "Train Loss: 0.02621123008430004\n",
      "Train Loss: 0.026200592517852783\n",
      "Train Loss: 0.02619008533656597\n",
      "Train Loss: 0.026181302964687347\n",
      "Train Loss: 0.02617253176867962\n",
      "Train Loss: 0.026165464892983437\n",
      "Train Loss: 0.026159165427088737\n",
      "Train Loss: 0.02615303359925747\n",
      "Train Loss: 0.026147041469812393\n",
      "Train Loss: 0.026142017915844917\n",
      "Train Loss: 0.02613859251141548\n",
      "Train Loss: 0.026134507730603218\n",
      "Train Loss: 0.026127628982067108\n",
      "Train Loss: 0.026126116514205933\n",
      "Train Loss: 0.02612726204097271\n",
      "Train Loss: 0.026123089715838432\n",
      "Train Loss: 0.02611311338841915\n",
      "Train Loss: 0.026100602000951767\n",
      "Train Loss: 0.026088057085871696\n",
      "Train Loss: 0.026075774803757668\n",
      "Train Loss: 0.026064759120345116\n",
      "Train Loss: 0.02605331689119339\n",
      "Train Loss: 0.026040170341730118\n",
      "Train Loss: 0.02602515183389187\n",
      "Train Loss: 0.02601071633398533\n",
      "Train Loss: 0.025997254997491837\n",
      "Train Loss: 0.025984689593315125\n",
      "Train Loss: 0.025972826406359673\n",
      "Train Loss: 0.025962017476558685\n",
      "Train Loss: 0.025952821597456932\n",
      "Train Loss: 0.02594180777668953\n",
      "Train Loss: 0.025929562747478485\n",
      "Train Loss: 0.02591865137219429\n",
      "Train Loss: 0.02590877190232277\n",
      "Train Loss: 0.025898225605487823\n",
      "Train Loss: 0.02588682249188423\n",
      "Train Loss: 0.025876952335238457\n",
      "Train Loss: 0.025867776945233345\n",
      "Train Loss: 0.025859888643026352\n",
      "Train Loss: 0.025851204991340637\n",
      "Train Loss: 0.02584677003324032\n",
      "Train Loss: 0.025844184681773186\n",
      "Train Loss: 0.02584296278655529\n",
      "Train Loss: 0.02584526129066944\n",
      "Train Loss: 0.02584185265004635\n",
      "Train Loss: 0.02583278901875019\n",
      "Train Loss: 0.025823786854743958\n",
      "Train Loss: 0.025815816596150398\n",
      "Train Loss: 0.025811227038502693\n",
      "Train Loss: 0.02580440230667591\n",
      "Train Loss: 0.025798305869102478\n",
      "Train Loss: 0.02579079195857048\n",
      "Train Loss: 0.025787679478526115\n",
      "Train Loss: 0.025781281292438507\n",
      "Train Loss: 0.025771386921405792\n",
      "Train Loss: 0.025760771706700325\n",
      "Train Loss: 0.025751763954758644\n",
      "Train Loss: 0.025742894038558006\n",
      "Train Loss: 0.025735270231962204\n",
      "Train Loss: 0.025726797059178352\n",
      "Train Loss: 0.025720510631799698\n",
      "Train Loss: 0.02571314387023449\n",
      "Train Loss: 0.025705620646476746\n",
      "Train Loss: 0.0256999209523201\n",
      "Train Loss: 0.025693640112876892\n",
      "Train Loss: 0.025684455409646034\n",
      "Train Loss: 0.025676168501377106\n",
      "Train Loss: 0.02566932700574398\n",
      "Train Loss: 0.02566102147102356\n",
      "Train Loss: 0.02565479278564453\n",
      "Train Loss: 0.025656050071120262\n",
      "Train Loss: 0.025659695267677307\n",
      "Train Loss: 0.02566506899893284\n",
      "Train Loss: 0.0256680678576231\n",
      "Train Loss: 0.025668853893876076\n",
      "Train Loss: 0.0256664901971817\n",
      "Train Loss: 0.025661634281277657\n",
      "Train Loss: 0.025654355064034462\n",
      "Train Loss: 0.025645554065704346\n",
      "Train Loss: 0.025638310238718987\n",
      "Train Loss: 0.025631071999669075\n",
      "Train Loss: 0.025621458888053894\n",
      "Train Loss: 0.02561086229979992\n",
      "Train Loss: 0.025601010769605637\n",
      "Train Loss: 0.025590570643544197\n",
      "Train Loss: 0.02558119222521782\n",
      "Train Loss: 0.025569772347807884\n",
      "Train Loss: 0.025557516142725945\n",
      "Train Loss: 0.025546062737703323\n",
      "Train Loss: 0.025536907836794853\n",
      "Train Loss: 0.025526631623506546\n",
      "Train Loss: 0.02551492303609848\n",
      "Train Loss: 0.025503145530819893\n",
      "Train Loss: 0.025492027401924133\n",
      "Train Loss: 0.02548200450837612\n",
      "Train Loss: 0.025472236797213554\n",
      "Train Loss: 0.025462355464696884\n",
      "Train Loss: 0.02545344829559326\n",
      "Train Loss: 0.025443725287914276\n",
      "Train Loss: 0.025434894487261772\n",
      "Train Loss: 0.02542746253311634\n",
      "Train Loss: 0.02542169950902462\n",
      "Train Loss: 0.025414178147912025\n",
      "Train Loss: 0.025406964123249054\n",
      "Train Loss: 0.025400636717677116\n",
      "Train Loss: 0.025393731892108917\n",
      "Train Loss: 0.02539023756980896\n",
      "Train Loss: 0.025386570021510124\n",
      "Train Loss: 0.025382105261087418\n",
      "Train Loss: 0.025377735495567322\n",
      "Train Loss: 0.025374745950102806\n",
      "Train Loss: 0.025374215096235275\n",
      "Train Loss: 0.025370987132191658\n",
      "Train Loss: 0.025365598499774933\n",
      "Train Loss: 0.025359826162457466\n",
      "Train Loss: 0.02535436302423477\n",
      "Train Loss: 0.02535090409219265\n",
      "Train Loss: 0.025346092879772186\n",
      "Train Loss: 0.025341566652059555\n",
      "Train Loss: 0.025337006896734238\n",
      "Train Loss: 0.025334171950817108\n",
      "Train Loss: 0.025326743721961975\n",
      "Train Loss: 0.025318102911114693\n",
      "Train Loss: 0.025308648124337196\n",
      "Train Loss: 0.025300733745098114\n",
      "Train Loss: 0.02529318444430828\n",
      "Train Loss: 0.025286590680480003\n",
      "Train Loss: 0.02527857944369316\n",
      "Train Loss: 0.025276903063058853\n",
      "Train Loss: 0.025274503976106644\n",
      "Train Loss: 0.0252702459692955\n",
      "Train Loss: 0.02526518516242504\n",
      "Train Loss: 0.02526167407631874\n",
      "Train Loss: 0.02525375969707966\n",
      "Train Loss: 0.025243548676371574\n",
      "Train Loss: 0.02523733116686344\n",
      "Train Loss: 0.025228407233953476\n",
      "Train Loss: 0.025217942893505096\n",
      "Train Loss: 0.02520703710615635\n",
      "Train Loss: 0.025196468457579613\n",
      "Train Loss: 0.02518754079937935\n",
      "Train Loss: 0.025180377066135406\n",
      "Train Loss: 0.025169910863041878\n",
      "Train Loss: 0.025159697979688644\n",
      "Train Loss: 0.025156183168292046\n",
      "Train Loss: 0.02514907903969288\n",
      "Train Loss: 0.025139441713690758\n",
      "Train Loss: 0.02513202466070652\n",
      "Train Loss: 0.025121795013546944\n",
      "Train Loss: 0.02511104941368103\n",
      "Train Loss: 0.025102881714701653\n",
      "Train Loss: 0.025093944743275642\n",
      "Train Loss: 0.02508634328842163\n",
      "Train Loss: 0.02508118748664856\n",
      "Train Loss: 0.02507854998111725\n",
      "Train Loss: 0.025072552263736725\n",
      "Train Loss: 0.025063997134566307\n",
      "Train Loss: 0.025056639686226845\n",
      "Train Loss: 0.025049425661563873\n",
      "Train Loss: 0.025043165311217308\n",
      "Train Loss: 0.02503388002514839\n",
      "Train Loss: 0.02502600848674774\n",
      "Train Loss: 0.025019891560077667\n",
      "Train Loss: 0.02501114085316658\n",
      "Train Loss: 0.0250027384608984\n",
      "Train Loss: 0.02499665506184101\n",
      "Train Loss: 0.024988964200019836\n",
      "Train Loss: 0.024979673326015472\n",
      "Train Loss: 0.02497127652168274\n",
      "Train Loss: 0.02496451698243618\n",
      "Train Loss: 0.02495642937719822\n",
      "Train Loss: 0.0249502994120121\n",
      "Train Loss: 0.02494380809366703\n",
      "Train Loss: 0.024939678609371185\n",
      "Train Loss: 0.024931488558650017\n",
      "Train Loss: 0.024923406541347504\n",
      "Train Loss: 0.024915246292948723\n",
      "Train Loss: 0.02490846998989582\n",
      "Train Loss: 0.024904197081923485\n",
      "Train Loss: 0.024894218891859055\n",
      "Train Loss: 0.024884570389986038\n",
      "Train Loss: 0.024877384305000305\n",
      "Train Loss: 0.02487160451710224\n",
      "Train Loss: 0.024865703657269478\n",
      "Train Loss: 0.024860979989171028\n",
      "Train Loss: 0.02485537715256214\n",
      "Train Loss: 0.024852342903614044\n",
      "Train Loss: 0.024855080991983414\n",
      "Train Loss: 0.024855395779013634\n",
      "Train Loss: 0.024850724264979362\n",
      "Train Loss: 0.024843592196702957\n",
      "Train Loss: 0.024836476892232895\n",
      "Train Loss: 0.02483057789504528\n",
      "Train Loss: 0.02482273243367672\n",
      "Train Loss: 0.02481474168598652\n",
      "Train Loss: 0.02480897307395935\n",
      "Train Loss: 0.024802248924970627\n",
      "Train Loss: 0.024793170392513275\n",
      "Train Loss: 0.02478666789829731\n",
      "Train Loss: 0.024780647829174995\n",
      "Train Loss: 0.024771269410848618\n",
      "Train Loss: 0.024762583896517754\n",
      "Train Loss: 0.024754954501986504\n",
      "Train Loss: 0.02474558912217617\n",
      "Train Loss: 0.024737007915973663\n",
      "Train Loss: 0.024731367826461792\n",
      "Train Loss: 0.02472577802836895\n",
      "Train Loss: 0.024719255045056343\n",
      "Train Loss: 0.024716654792428017\n",
      "Train Loss: 0.024711323902010918\n",
      "Train Loss: 0.024708906188607216\n",
      "Train Loss: 0.02470383048057556\n",
      "Train Loss: 0.024700626730918884\n",
      "Train Loss: 0.024697627872228622\n",
      "Train Loss: 0.02470015361905098\n",
      "Train Loss: 0.024701369926333427\n",
      "Train Loss: 0.02470405399799347\n",
      "Train Loss: 0.02470029518008232\n",
      "Train Loss: 0.02469974383711815\n",
      "Train Loss: 0.024696093052625656\n",
      "Train Loss: 0.02469259314239025\n",
      "Train Loss: 0.02469141222536564\n",
      "Train Loss: 0.02469228208065033\n",
      "Train Loss: 0.024689005687832832\n",
      "Train Loss: 0.02469305507838726\n",
      "Train Loss: 0.02469443343579769\n",
      "Train Loss: 0.024692242965102196\n",
      "Train Loss: 0.02469407208263874\n",
      "Train Loss: 0.024698399007320404\n",
      "Train Loss: 0.02469681203365326\n",
      "Train Loss: 0.024693001061677933\n",
      "Train Loss: 0.02469206415116787\n",
      "Train Loss: 0.024690646678209305\n",
      "Train Loss: 0.024686725810170174\n",
      "Train Loss: 0.024686940014362335\n",
      "Train Loss: 0.02468188665807247\n",
      "Train Loss: 0.024675127118825912\n",
      "Train Loss: 0.024669624865055084\n",
      "Train Loss: 0.024663005024194717\n",
      "Train Loss: 0.024654923006892204\n",
      "Train Loss: 0.024646004661917686\n",
      "Train Loss: 0.02463608607649803\n",
      "Train Loss: 0.024627694860100746\n",
      "Train Loss: 0.02461869828402996\n",
      "Train Loss: 0.024612627923488617\n",
      "Train Loss: 0.024608423933386803\n",
      "Train Loss: 0.024601425975561142\n",
      "Train Loss: 0.024599526077508926\n",
      "Train Loss: 0.024598464369773865\n",
      "Train Loss: 0.024591835215687752\n",
      "Train Loss: 0.02458488568663597\n",
      "Train Loss: 0.024581627920269966\n",
      "Train Loss: 0.02457255870103836\n",
      "Train Loss: 0.024565204977989197\n",
      "Train Loss: 0.024558130651712418\n",
      "Train Loss: 0.024554762989282608\n",
      "Train Loss: 0.02455133944749832\n",
      "Train Loss: 0.024545585736632347\n",
      "Train Loss: 0.024539543315768242\n",
      "Train Loss: 0.024537310004234314\n",
      "Train Loss: 0.024529477581381798\n",
      "Train Loss: 0.02452239952981472\n",
      "Train Loss: 0.024516817182302475\n",
      "Train Loss: 0.024508517235517502\n",
      "Train Loss: 0.024501018226146698\n",
      "Train Loss: 0.024493582546710968\n",
      "Train Loss: 0.024483362212777138\n",
      "Train Loss: 0.024472752586007118\n",
      "Train Loss: 0.024461710825562477\n",
      "Train Loss: 0.024450847879052162\n",
      "Train Loss: 0.024442054331302643\n",
      "Train Loss: 0.02443050779402256\n",
      "Train Loss: 0.024420693516731262\n",
      "Train Loss: 0.02441251464188099\n",
      "Train Loss: 0.024401938542723656\n",
      "Train Loss: 0.02439379319548607\n",
      "Train Loss: 0.0243853572756052\n",
      "Train Loss: 0.02437444031238556\n",
      "Train Loss: 0.024365466088056564\n",
      "Train Loss: 0.024357188493013382\n",
      "Train Loss: 0.024347711354494095\n",
      "Train Loss: 0.024338917806744576\n",
      "Train Loss: 0.0243303794413805\n",
      "Train Loss: 0.024320855736732483\n",
      "Train Loss: 0.024310147389769554\n",
      "Train Loss: 0.024299653246998787\n",
      "Train Loss: 0.024290377274155617\n",
      "Train Loss: 0.024281634017825127\n",
      "Train Loss: 0.024271823465824127\n",
      "Train Loss: 0.02426132746040821\n",
      "Train Loss: 0.024252232164144516\n",
      "Train Loss: 0.024242904037237167\n",
      "Train Loss: 0.02423529513180256\n",
      "Train Loss: 0.02422592230141163\n",
      "Train Loss: 0.02421644888818264\n",
      "Train Loss: 0.02420884557068348\n",
      "Train Loss: 0.024200312793254852\n",
      "Train Loss: 0.02419080212712288\n",
      "Train Loss: 0.024184919893741608\n",
      "Train Loss: 0.024180961772799492\n",
      "Train Loss: 0.024177324026823044\n",
      "Train Loss: 0.024171749129891396\n",
      "Train Loss: 0.024167438969016075\n",
      "Train Loss: 0.024161236360669136\n",
      "Train Loss: 0.02415603958070278\n",
      "Train Loss: 0.024153195321559906\n",
      "Train Loss: 0.024154869839549065\n",
      "Train Loss: 0.02415846288204193\n",
      "Train Loss: 0.02415856346487999\n",
      "Train Loss: 0.02415562979876995\n",
      "Train Loss: 0.024152562022209167\n",
      "Train Loss: 0.024151025339961052\n",
      "Train Loss: 0.02414722554385662\n",
      "Train Loss: 0.024139542132616043\n",
      "Train Loss: 0.0241315346211195\n",
      "Train Loss: 0.02412376180291176\n",
      "Train Loss: 0.024116089567542076\n",
      "Train Loss: 0.024107210338115692\n",
      "Train Loss: 0.02409888245165348\n",
      "Train Loss: 0.024090783670544624\n",
      "Train Loss: 0.024082114920020103\n",
      "Train Loss: 0.024075210094451904\n",
      "Train Loss: 0.024070385843515396\n",
      "Train Loss: 0.02406606636941433\n",
      "Train Loss: 0.024060679599642754\n",
      "Train Loss: 0.024054091423749924\n",
      "Train Loss: 0.02404938079416752\n",
      "Train Loss: 0.02404705621302128\n",
      "Train Loss: 0.024043258279561996\n",
      "Train Loss: 0.024036549031734467\n",
      "Train Loss: 0.02402910590171814\n",
      "Train Loss: 0.024021826684474945\n",
      "Train Loss: 0.02401319518685341\n",
      "Train Loss: 0.02400319278240204\n",
      "Train Loss: 0.023995377123355865\n",
      "Train Loss: 0.02398768626153469\n",
      "Train Loss: 0.023977812379598618\n",
      "Train Loss: 0.023967698216438293\n",
      "Train Loss: 0.023959027603268623\n",
      "Train Loss: 0.023951822891831398\n",
      "Train Loss: 0.02394278533756733\n",
      "Train Loss: 0.023934246972203255\n",
      "Train Loss: 0.02392716147005558\n",
      "Train Loss: 0.02392181009054184\n",
      "Train Loss: 0.02392318844795227\n",
      "Train Loss: 0.023921726271510124\n",
      "Train Loss: 0.02391848899424076\n",
      "Train Loss: 0.02391311526298523\n",
      "Train Loss: 0.02390623837709427\n",
      "Train Loss: 0.023899463936686516\n",
      "Train Loss: 0.02389354445040226\n",
      "Train Loss: 0.02388955093920231\n",
      "Train Loss: 0.02388361282646656\n",
      "Train Loss: 0.023877425119280815\n",
      "Train Loss: 0.0238701980561018\n",
      "Train Loss: 0.023864397779107094\n",
      "Train Loss: 0.02386031113564968\n",
      "Train Loss: 0.02385449782013893\n",
      "Train Loss: 0.023847617208957672\n",
      "Train Loss: 0.02384091727435589\n",
      "Train Loss: 0.02383626252412796\n",
      "Train Loss: 0.023831140249967575\n",
      "Train Loss: 0.02382521703839302\n",
      "Train Loss: 0.0238183680921793\n",
      "Train Loss: 0.02381092682480812\n",
      "Train Loss: 0.023803016170859337\n",
      "Train Loss: 0.02379567362368107\n",
      "Train Loss: 0.02378787472844124\n",
      "Train Loss: 0.023781146854162216\n",
      "Train Loss: 0.023773398250341415\n",
      "Train Loss: 0.023767514154314995\n",
      "Train Loss: 0.02376062050461769\n",
      "Train Loss: 0.023752287030220032\n",
      "Train Loss: 0.023750633001327515\n",
      "Train Loss: 0.023745132610201836\n",
      "Train Loss: 0.023737026378512383\n",
      "Train Loss: 0.023727258667349815\n",
      "Train Loss: 0.023716917261481285\n",
      "Train Loss: 0.02370988018810749\n",
      "Train Loss: 0.023703396320343018\n",
      "Train Loss: 0.02369445189833641\n",
      "Train Loss: 0.02368902787566185\n",
      "Train Loss: 0.0236860029399395\n",
      "Train Loss: 0.023676913231611252\n",
      "Train Loss: 0.023667264729738235\n",
      "Train Loss: 0.02366054430603981\n",
      "Train Loss: 0.02365771122276783\n",
      "Train Loss: 0.023651208728551865\n",
      "Train Loss: 0.023651303723454475\n",
      "Train Loss: 0.023649590089917183\n",
      "Train Loss: 0.023643968626856804\n",
      "Train Loss: 0.02363646775484085\n",
      "Train Loss: 0.02362944930791855\n",
      "Train Loss: 0.023624371737241745\n",
      "Train Loss: 0.023616895079612732\n",
      "Train Loss: 0.023609785363078117\n",
      "Train Loss: 0.02360280603170395\n",
      "Train Loss: 0.023596517741680145\n",
      "Train Loss: 0.02359014004468918\n",
      "Train Loss: 0.023585842922329903\n",
      "Train Loss: 0.023587046191096306\n",
      "Train Loss: 0.023593729361891747\n",
      "Train Loss: 0.023588797077536583\n",
      "Train Loss: 0.023587053641676903\n",
      "Train Loss: 0.023593870922923088\n",
      "Train Loss: 0.023602798581123352\n",
      "Train Loss: 0.02361338399350643\n",
      "Train Loss: 0.02361820451915264\n",
      "Train Loss: 0.02362089231610298\n",
      "Train Loss: 0.023619720712304115\n",
      "Train Loss: 0.023616189137101173\n",
      "Train Loss: 0.02361181750893593\n",
      "Train Loss: 0.02360721305012703\n",
      "Train Loss: 0.023601112887263298\n",
      "Train Loss: 0.023595215752720833\n",
      "Train Loss: 0.023589307442307472\n",
      "Train Loss: 0.023582698777318\n",
      "Train Loss: 0.0235761608928442\n",
      "Train Loss: 0.02357184886932373\n",
      "Train Loss: 0.02356693148612976\n",
      "Train Loss: 0.02356019988656044\n",
      "Train Loss: 0.023553907871246338\n",
      "Train Loss: 0.023549741134047508\n",
      "Train Loss: 0.02354307845234871\n",
      "Train Loss: 0.023535646498203278\n",
      "Train Loss: 0.02352970466017723\n",
      "Train Loss: 0.023525405675172806\n",
      "Train Loss: 0.02351880632340908\n",
      "Train Loss: 0.02351020649075508\n",
      "Train Loss: 0.02350461669266224\n",
      "Train Loss: 0.023497706279158592\n",
      "Train Loss: 0.023488372564315796\n",
      "Train Loss: 0.023479515686631203\n",
      "Train Loss: 0.023473624140024185\n",
      "Train Loss: 0.023465383797883987\n",
      "Train Loss: 0.023456929251551628\n",
      "Train Loss: 0.023452263325452805\n",
      "Train Loss: 0.023442817851901054\n",
      "Train Loss: 0.02343291975557804\n",
      "Train Loss: 0.023422325029969215\n",
      "Train Loss: 0.023412752896547318\n",
      "Train Loss: 0.023405643180012703\n",
      "Train Loss: 0.023398295044898987\n",
      "Train Loss: 0.02339208498597145\n",
      "Train Loss: 0.023384977132081985\n",
      "Train Loss: 0.023375172168016434\n",
      "Train Loss: 0.023364759981632233\n",
      "Train Loss: 0.023356307297945023\n",
      "Train Loss: 0.02334822528064251\n",
      "Train Loss: 0.023340201005339622\n",
      "Train Loss: 0.023332972079515457\n",
      "Train Loss: 0.02332436293363571\n",
      "Train Loss: 0.02331462875008583\n",
      "Train Loss: 0.02330610528588295\n",
      "Train Loss: 0.023300329223275185\n",
      "Train Loss: 0.02329300157725811\n",
      "Train Loss: 0.023283492773771286\n",
      "Train Loss: 0.023273197934031487\n",
      "Train Loss: 0.02326355315744877\n",
      "Train Loss: 0.023256493732333183\n",
      "Train Loss: 0.023249242454767227\n",
      "Train Loss: 0.023241722956299782\n",
      "Train Loss: 0.023232705891132355\n",
      "Train Loss: 0.0232236310839653\n",
      "Train Loss: 0.023214470595121384\n",
      "Train Loss: 0.023206030949950218\n",
      "Train Loss: 0.023199928924441338\n",
      "Train Loss: 0.02319403551518917\n",
      "Train Loss: 0.02318677306175232\n",
      "Train Loss: 0.02317887172102928\n",
      "Train Loss: 0.023171478882431984\n",
      "Train Loss: 0.023163361474871635\n",
      "Train Loss: 0.023154016584157944\n",
      "Train Loss: 0.02314496412873268\n",
      "Train Loss: 0.02313673309981823\n",
      "Train Loss: 0.023130081593990326\n",
      "Train Loss: 0.02312338910996914\n",
      "Train Loss: 0.02311491221189499\n",
      "Train Loss: 0.02310789003968239\n",
      "Train Loss: 0.023102033883333206\n",
      "Train Loss: 0.023094480857253075\n",
      "Train Loss: 0.023085998371243477\n",
      "Train Loss: 0.023078888654708862\n",
      "Train Loss: 0.023071203380823135\n",
      "Train Loss: 0.02306228131055832\n",
      "Train Loss: 0.023053696379065514\n",
      "Train Loss: 0.023045916110277176\n",
      "Train Loss: 0.02303668111562729\n",
      "Train Loss: 0.023027800023555756\n",
      "Train Loss: 0.023020140826702118\n",
      "Train Loss: 0.023011356592178345\n",
      "Train Loss: 0.023003099486231804\n",
      "Train Loss: 0.022996066138148308\n",
      "Train Loss: 0.02298722416162491\n",
      "Train Loss: 0.02297743409872055\n",
      "Train Loss: 0.022967757657170296\n",
      "Train Loss: 0.0229607205837965\n",
      "Train Loss: 0.02295598015189171\n",
      "Train Loss: 0.022947736084461212\n",
      "Train Loss: 0.022939123213291168\n",
      "Train Loss: 0.022931843996047974\n",
      "Train Loss: 0.022925477474927902\n",
      "Train Loss: 0.02291821874678135\n",
      "Train Loss: 0.022912688553333282\n",
      "Train Loss: 0.02290450409054756\n",
      "Train Loss: 0.022896429523825645\n",
      "Train Loss: 0.022891119122505188\n",
      "Train Loss: 0.02288626693189144\n",
      "Train Loss: 0.02288149483501911\n",
      "Train Loss: 0.022876093164086342\n",
      "Train Loss: 0.0228740181773901\n",
      "Train Loss: 0.02287008985877037\n",
      "Train Loss: 0.02286720648407936\n",
      "Train Loss: 0.022865569218993187\n",
      "Train Loss: 0.022865556180477142\n",
      "Train Loss: 0.02286359667778015\n",
      "Train Loss: 0.02286144718527794\n",
      "Train Loss: 0.022854086011648178\n",
      "Train Loss: 0.022848336026072502\n",
      "Train Loss: 0.022843776270747185\n",
      "Train Loss: 0.02284858375787735\n",
      "Train Loss: 0.022847415879368782\n",
      "Train Loss: 0.02284839004278183\n",
      "Train Loss: 0.022850291803479195\n",
      "Train Loss: 0.022868182510137558\n",
      "Train Loss: 0.022875862196087837\n",
      "Train Loss: 0.02288033813238144\n",
      "Train Loss: 0.02290099486708641\n",
      "Train Loss: 0.022922826930880547\n",
      "Train Loss: 0.0229355376213789\n",
      "Train Loss: 0.022943831980228424\n",
      "Train Loss: 0.02294904924929142\n",
      "Train Loss: 0.02295263484120369\n",
      "Train Loss: 0.02295517921447754\n",
      "Train Loss: 0.022958392277359962\n",
      "Train Loss: 0.022960351780056953\n",
      "Train Loss: 0.022960908710956573\n",
      "Train Loss: 0.022959258407354355\n",
      "Train Loss: 0.022957144305109978\n",
      "Train Loss: 0.02295421063899994\n",
      "Train Loss: 0.02295033261179924\n",
      "Train Loss: 0.02294575609266758\n",
      "Train Loss: 0.022941775619983673\n",
      "Train Loss: 0.022937292233109474\n",
      "Train Loss: 0.022933125495910645\n",
      "Train Loss: 0.022927822545170784\n",
      "Train Loss: 0.022921748459339142\n",
      "Train Loss: 0.02291584573686123\n",
      "Train Loss: 0.022909844294190407\n",
      "Train Loss: 0.022904273122549057\n",
      "Train Loss: 0.02289881557226181\n",
      "Train Loss: 0.022893212735652924\n",
      "Train Loss: 0.022887662053108215\n",
      "Train Loss: 0.022882508113980293\n",
      "Train Loss: 0.022876853123307228\n",
      "Train Loss: 0.02287270687520504\n",
      "Train Loss: 0.02287064678966999\n",
      "Train Loss: 0.022867750376462936\n",
      "Train Loss: 0.02286418527364731\n",
      "Train Loss: 0.022860689088702202\n",
      "Train Loss: 0.022862771525979042\n",
      "Train Loss: 0.02286341041326523\n",
      "Train Loss: 0.02285963110625744\n",
      "Train Loss: 0.0228560920804739\n",
      "Train Loss: 0.022854313254356384\n",
      "Train Loss: 0.022851750254631042\n",
      "Train Loss: 0.022848840802907944\n",
      "Train Loss: 0.022849585860967636\n",
      "Train Loss: 0.022845014929771423\n",
      "Train Loss: 0.022840801626443863\n",
      "Train Loss: 0.022835945710539818\n",
      "Train Loss: 0.022830603644251823\n",
      "Train Loss: 0.022827589884400368\n",
      "Train Loss: 0.02282627671957016\n",
      "Train Loss: 0.022821243852376938\n",
      "Train Loss: 0.022814318537712097\n",
      "Train Loss: 0.02280827797949314\n",
      "Train Loss: 0.02280345931649208\n",
      "Train Loss: 0.02279883623123169\n",
      "Train Loss: 0.02279707044363022\n",
      "Train Loss: 0.022793937474489212\n",
      "Train Loss: 0.022789105772972107\n",
      "Train Loss: 0.02278715930879116\n",
      "Train Loss: 0.02278655581176281\n",
      "Train Loss: 0.022781357169151306\n",
      "Train Loss: 0.022774942219257355\n",
      "Train Loss: 0.02277183346450329\n",
      "Train Loss: 0.02276717498898506\n",
      "Train Loss: 0.02276037633419037\n",
      "Train Loss: 0.022754235193133354\n",
      "Train Loss: 0.02274799533188343\n",
      "Train Loss: 0.022742193192243576\n",
      "Train Loss: 0.022739775478839874\n",
      "Train Loss: 0.022737033665180206\n",
      "Train Loss: 0.022729845717549324\n",
      "Train Loss: 0.02272241748869419\n",
      "Train Loss: 0.02271789126098156\n",
      "Train Loss: 0.022713184356689453\n",
      "Train Loss: 0.022713104262948036\n",
      "Train Loss: 0.022713126614689827\n",
      "Train Loss: 0.0227070152759552\n",
      "Train Loss: 0.02270115725696087\n",
      "Train Loss: 0.022699272260069847\n",
      "Train Loss: 0.022694703191518784\n",
      "Train Loss: 0.022694354876875877\n",
      "Train Loss: 0.022692736238241196\n",
      "Train Loss: 0.022686386480927467\n",
      "Train Loss: 0.02268146350979805\n",
      "Train Loss: 0.02268093079328537\n",
      "Train Loss: 0.022675078362226486\n",
      "Train Loss: 0.02267453633248806\n",
      "Train Loss: 0.02267274633049965\n",
      "Train Loss: 0.02266516722738743\n",
      "Train Loss: 0.02265811525285244\n",
      "Train Loss: 0.022654568776488304\n",
      "Train Loss: 0.022654740139842033\n",
      "Train Loss: 0.022650273516774178\n",
      "Train Loss: 0.022649940103292465\n",
      "Train Loss: 0.02264704369008541\n",
      "Train Loss: 0.022641446441411972\n",
      "Train Loss: 0.022641621530056\n",
      "Train Loss: 0.02264046110212803\n",
      "Train Loss: 0.0226430706679821\n",
      "Train Loss: 0.02264159359037876\n",
      "Train Loss: 0.022635940462350845\n",
      "Train Loss: 0.022631919011473656\n",
      "Train Loss: 0.022632746025919914\n",
      "Train Loss: 0.022629503160715103\n",
      "Train Loss: 0.02262294664978981\n",
      "Train Loss: 0.022619588300585747\n",
      "Train Loss: 0.022614117711782455\n",
      "Train Loss: 0.022611157968640327\n",
      "Train Loss: 0.02260744199156761\n",
      "Train Loss: 0.022599544376134872\n",
      "Train Loss: 0.022590396925807\n",
      "Train Loss: 0.022581038996577263\n",
      "Train Loss: 0.022572074085474014\n",
      "Train Loss: 0.022564738988876343\n",
      "Train Loss: 0.022559557110071182\n",
      "Train Loss: 0.022554881870746613\n",
      "Train Loss: 0.022547822445631027\n",
      "Train Loss: 0.022540373727679253\n",
      "Train Loss: 0.022534405812621117\n",
      "Train Loss: 0.02252819575369358\n",
      "Train Loss: 0.022522255778312683\n",
      "Train Loss: 0.02251838892698288\n",
      "Train Loss: 0.022514818236231804\n",
      "Train Loss: 0.02251020260155201\n",
      "Train Loss: 0.022504320368170738\n",
      "Train Loss: 0.022501278668642044\n",
      "Train Loss: 0.02249901555478573\n",
      "Train Loss: 0.022495733574032784\n",
      "Train Loss: 0.02249574474990368\n",
      "Train Loss: 0.022493166849017143\n",
      "Train Loss: 0.022491417825222015\n",
      "Train Loss: 0.02248699590563774\n",
      "Train Loss: 0.022486239671707153\n",
      "Train Loss: 0.022486424073576927\n",
      "Train Loss: 0.022482892498373985\n",
      "Train Loss: 0.022480905055999756\n",
      "Train Loss: 0.022478746250271797\n",
      "Train Loss: 0.022482817992568016\n",
      "Train Loss: 0.022490907460451126\n",
      "Train Loss: 0.022487623617053032\n",
      "Train Loss: 0.02248474583029747\n",
      "Train Loss: 0.022483471781015396\n",
      "Train Loss: 0.022477220743894577\n",
      "Train Loss: 0.022472916170954704\n",
      "Train Loss: 0.022466367110610008\n",
      "Train Loss: 0.02245943248271942\n",
      "Train Loss: 0.022454597055912018\n",
      "Train Loss: 0.02244771644473076\n",
      "Train Loss: 0.022439444437623024\n",
      "Train Loss: 0.022430865094065666\n",
      "Train Loss: 0.022422656416893005\n",
      "Train Loss: 0.02241714484989643\n",
      "Train Loss: 0.0224114079028368\n",
      "Train Loss: 0.02240413799881935\n",
      "Train Loss: 0.02239629067480564\n",
      "Train Loss: 0.022388266399502754\n",
      "Train Loss: 0.022381670773029327\n",
      "Train Loss: 0.022374771535396576\n",
      "Train Loss: 0.022369172424077988\n",
      "Train Loss: 0.02236362360417843\n",
      "Train Loss: 0.022355735301971436\n",
      "Train Loss: 0.022349020466208458\n",
      "Train Loss: 0.02234215848147869\n",
      "Train Loss: 0.022334670647978783\n",
      "Train Loss: 0.02232840284705162\n",
      "Train Loss: 0.022320691496133804\n",
      "Train Loss: 0.022313974797725677\n",
      "Train Loss: 0.022308576852083206\n",
      "Train Loss: 0.02230055071413517\n",
      "Train Loss: 0.0222936999052763\n",
      "Train Loss: 0.02228771708905697\n",
      "Train Loss: 0.022280292585492134\n",
      "Train Loss: 0.022274477407336235\n",
      "Train Loss: 0.022267896682024002\n",
      "Train Loss: 0.02226014994084835\n",
      "Train Loss: 0.022252963855862617\n",
      "Train Loss: 0.0222465880215168\n",
      "Train Loss: 0.0222395621240139\n",
      "Train Loss: 0.02223275415599346\n",
      "Train Loss: 0.022225331515073776\n",
      "Train Loss: 0.022217299789190292\n",
      "Train Loss: 0.022209174931049347\n",
      "Train Loss: 0.022202681750059128\n",
      "Train Loss: 0.022195853292942047\n",
      "Train Loss: 0.022188492119312286\n",
      "Train Loss: 0.022181164473295212\n",
      "Train Loss: 0.022174399346113205\n",
      "Train Loss: 0.022167179733514786\n",
      "Train Loss: 0.02216023951768875\n",
      "Train Loss: 0.02215375378727913\n",
      "Train Loss: 0.02214640937745571\n",
      "Train Loss: 0.022139281034469604\n",
      "Train Loss: 0.022133003920316696\n",
      "Train Loss: 0.02212546020746231\n",
      "Train Loss: 0.0221179760992527\n",
      "Train Loss: 0.02211170271039009\n",
      "Train Loss: 0.02210458368062973\n",
      "Train Loss: 0.0220969058573246\n",
      "Train Loss: 0.022089675068855286\n",
      "Train Loss: 0.022082405164837837\n",
      "Train Loss: 0.022075893357396126\n",
      "Train Loss: 0.022069580852985382\n",
      "Train Loss: 0.022062120959162712\n",
      "Train Loss: 0.022054200991988182\n",
      "Train Loss: 0.02204764261841774\n",
      "Train Loss: 0.02204195410013199\n",
      "Train Loss: 0.022035295143723488\n",
      "Train Loss: 0.02202773094177246\n",
      "Train Loss: 0.022020874544978142\n",
      "Train Loss: 0.022015804424881935\n",
      "Train Loss: 0.02200961858034134\n",
      "Train Loss: 0.022002972662448883\n",
      "Train Loss: 0.021996408700942993\n",
      "Train Loss: 0.02199200727045536\n",
      "Train Loss: 0.02198830246925354\n",
      "Train Loss: 0.021983623504638672\n",
      "Train Loss: 0.021980904042720795\n",
      "Train Loss: 0.02197886072099209\n",
      "Train Loss: 0.0219785925000906\n",
      "Train Loss: 0.021977132186293602\n",
      "Train Loss: 0.02197718434035778\n",
      "Train Loss: 0.021977441385388374\n",
      "Train Loss: 0.021983860060572624\n",
      "Train Loss: 0.02199624665081501\n",
      "Train Loss: 0.022004326805472374\n",
      "Train Loss: 0.02202768065035343\n",
      "Train Loss: 0.022037304937839508\n",
      "Train Loss: 0.02204892598092556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = pl.Trainer(default_root_dir=\"model/\", max_epochs=2000)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: PossibleUserWarning:\n",
      "\n",
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810c0d36f74941f8ab3d94b6473c430f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.05884575843811035\n",
      "[{}]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.test(model=model, dataloaders=val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to predict btc Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predict': array([[[26192.94712865, 26096.51282143, 26187.2097397 , ...,\n",
      "         32764.27369493, 32655.60019463, 33027.80368398]],\n",
      "\n",
      "       [[27136.57148337, 27036.66304395, 27130.62739985, ...,\n",
      "         33944.63596847, 33832.04740809, 34217.65986114]],\n",
      "\n",
      "       [[26826.49428941, 26727.72745804, 26820.61812621, ...,\n",
      "         33556.76613467, 33445.46407228, 33826.67030816]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[36171.37419829, 36038.20241754, 36163.45110197, ...,\n",
      "         45246.10378258, 45096.03018354, 45610.02777323]],\n",
      "\n",
      "       [[35872.89242461, 35740.81956118, 35865.03470872, ...,\n",
      "         44872.73844581, 44723.90323583, 45233.65938003]],\n",
      "\n",
      "       [[34833.28445095, 34705.03910162, 34825.65445419, ...,\n",
      "         43572.31203647, 43427.7901188 , 43922.77336579]]]), 'actual': array([[[26096.20569737, 26000.12756209, 26090.48949897, ...,\n",
      "         32643.26162567, 32534.98950174, 32905.81828903]],\n",
      "\n",
      "       [[26286.36310109, 26189.58486523, 26280.60525   , ...,\n",
      "         32881.12600917, 32772.06492969, 33145.59586611]],\n",
      "\n",
      "       [[26129.86504671, 26033.66298814, 26124.14147546, ...,\n",
      "         32685.36548396, 32576.95370872, 32948.26079756]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[34966.97324556, 34838.23569556, 34959.31396517, ...,\n",
      "         43739.54087998, 43594.4642928 , 44091.3472663 ]],\n",
      "\n",
      "       [[34503.08154443, 34376.05189979, 34495.5238764 , ...,\n",
      "         43159.26732062, 43016.11540173, 43506.40644386]],\n",
      "\n",
      "       [[34460.34806103, 34333.47574781, 34452.79975348, ...,\n",
      "         43105.81279565, 42962.8381763 , 43452.52197283]]]), 'losses': tensor(0.0588), 'date': [Timestamp('2023-08-19 00:00:00+0000', tz='UTC'), Timestamp('2023-08-20 00:00:00+0000', tz='UTC'), Timestamp('2023-08-21 00:00:00+0000', tz='UTC'), Timestamp('2023-08-22 00:00:00+0000', tz='UTC'), Timestamp('2023-08-23 00:00:00+0000', tz='UTC'), Timestamp('2023-08-24 00:00:00+0000', tz='UTC'), Timestamp('2023-08-25 00:00:00+0000', tz='UTC'), Timestamp('2023-08-26 00:00:00+0000', tz='UTC'), Timestamp('2023-08-27 00:00:00+0000', tz='UTC'), Timestamp('2023-08-28 00:00:00+0000', tz='UTC'), Timestamp('2023-08-29 00:00:00+0000', tz='UTC'), Timestamp('2023-08-30 00:00:00+0000', tz='UTC'), Timestamp('2023-08-31 00:00:00+0000', tz='UTC'), Timestamp('2023-09-01 00:00:00+0000', tz='UTC'), Timestamp('2023-09-02 00:00:00+0000', tz='UTC'), Timestamp('2023-09-03 00:00:00+0000', tz='UTC'), Timestamp('2023-09-04 00:00:00+0000', tz='UTC'), Timestamp('2023-09-05 00:00:00+0000', tz='UTC'), Timestamp('2023-09-06 00:00:00+0000', tz='UTC'), Timestamp('2023-09-07 00:00:00+0000', tz='UTC'), Timestamp('2023-09-08 00:00:00+0000', tz='UTC'), Timestamp('2023-09-09 00:00:00+0000', tz='UTC'), Timestamp('2023-09-10 00:00:00+0000', tz='UTC'), Timestamp('2023-09-11 00:00:00+0000', tz='UTC'), Timestamp('2023-09-12 00:00:00+0000', tz='UTC'), Timestamp('2023-09-13 00:00:00+0000', tz='UTC'), Timestamp('2023-09-14 00:00:00+0000', tz='UTC'), Timestamp('2023-09-15 00:00:00+0000', tz='UTC'), Timestamp('2023-09-16 00:00:00+0000', tz='UTC'), Timestamp('2023-09-17 00:00:00+0000', tz='UTC'), Timestamp('2023-09-18 00:00:00+0000', tz='UTC'), Timestamp('2023-09-19 00:00:00+0000', tz='UTC'), Timestamp('2023-09-20 00:00:00+0000', tz='UTC'), Timestamp('2023-09-21 00:00:00+0000', tz='UTC'), Timestamp('2023-09-22 00:00:00+0000', tz='UTC'), Timestamp('2023-09-23 00:00:00+0000', tz='UTC'), Timestamp('2023-09-24 00:00:00+0000', tz='UTC'), Timestamp('2023-09-25 00:00:00+0000', tz='UTC'), Timestamp('2023-09-26 00:00:00+0000', tz='UTC'), Timestamp('2023-09-27 00:00:00+0000', tz='UTC'), Timestamp('2023-09-28 00:00:00+0000', tz='UTC'), Timestamp('2023-09-29 00:00:00+0000', tz='UTC'), Timestamp('2023-09-30 00:00:00+0000', tz='UTC'), Timestamp('2023-10-01 00:00:00+0000', tz='UTC'), Timestamp('2023-10-02 00:00:00+0000', tz='UTC'), Timestamp('2023-10-03 00:00:00+0000', tz='UTC'), Timestamp('2023-10-04 00:00:00+0000', tz='UTC'), Timestamp('2023-10-05 00:00:00+0000', tz='UTC'), Timestamp('2023-10-06 00:00:00+0000', tz='UTC'), Timestamp('2023-10-07 00:00:00+0000', tz='UTC'), Timestamp('2023-10-08 00:00:00+0000', tz='UTC'), Timestamp('2023-10-09 00:00:00+0000', tz='UTC'), Timestamp('2023-10-10 00:00:00+0000', tz='UTC'), Timestamp('2023-10-11 00:00:00+0000', tz='UTC'), Timestamp('2023-10-12 00:00:00+0000', tz='UTC'), Timestamp('2023-10-13 00:00:00+0000', tz='UTC'), Timestamp('2023-10-14 00:00:00+0000', tz='UTC'), Timestamp('2023-10-15 00:00:00+0000', tz='UTC'), Timestamp('2023-10-16 00:00:00+0000', tz='UTC'), Timestamp('2023-10-17 00:00:00+0000', tz='UTC'), Timestamp('2023-10-18 00:00:00+0000', tz='UTC'), Timestamp('2023-10-19 00:00:00+0000', tz='UTC'), Timestamp('2023-10-20 00:00:00+0000', tz='UTC'), Timestamp('2023-10-21 00:00:00+0000', tz='UTC'), Timestamp('2023-10-22 00:00:00+0000', tz='UTC'), Timestamp('2023-10-23 00:00:00+0000', tz='UTC'), Timestamp('2023-10-24 00:00:00+0000', tz='UTC'), Timestamp('2023-10-25 00:00:00+0000', tz='UTC'), Timestamp('2023-10-26 00:00:00+0000', tz='UTC'), Timestamp('2023-10-27 00:00:00+0000', tz='UTC'), Timestamp('2023-10-28 00:00:00+0000', tz='UTC'), Timestamp('2023-10-29 00:00:00+0000', tz='UTC'), Timestamp('2023-10-30 00:00:00+0000', tz='UTC'), Timestamp('2023-10-31 00:00:00+0000', tz='UTC'), Timestamp('2023-11-01 00:00:00+0000', tz='UTC'), Timestamp('2023-11-02 00:00:00+0000', tz='UTC'), Timestamp('2023-11-03 00:00:00+0000', tz='UTC'), Timestamp('2023-11-04 00:00:00+0000', tz='UTC'), Timestamp('2023-11-05 00:00:00+0000', tz='UTC'), Timestamp('2023-11-06 00:00:00+0000', tz='UTC'), Timestamp('2023-11-07 00:00:00+0000', tz='UTC'), Timestamp('2023-11-08 00:00:00+0000', tz='UTC'), Timestamp('2023-11-09 00:00:00+0000', tz='UTC'), Timestamp('2023-11-10 00:00:00+0000', tz='UTC'), Timestamp('2023-11-11 00:00:00+0000', tz='UTC'), Timestamp('2023-11-12 00:00:00+0000', tz='UTC'), Timestamp('2023-11-13 00:00:00+0000', tz='UTC'), Timestamp('2023-11-14 00:00:00+0000', tz='UTC'), Timestamp('2023-11-15 00:00:00+0000', tz='UTC'), Timestamp('2023-11-16 00:00:00+0000', tz='UTC'), Timestamp('2023-11-17 00:00:00+0000', tz='UTC'), Timestamp('2023-11-18 00:00:00+0000', tz='UTC'), Timestamp('2023-11-19 00:00:00+0000', tz='UTC'), Timestamp('2023-11-20 00:00:00+0000', tz='UTC'), Timestamp('2023-11-21 00:00:00+0000', tz='UTC'), Timestamp('2023-11-22 00:00:00+0000', tz='UTC'), Timestamp('2023-11-23 00:00:00+0000', tz='UTC'), Timestamp('2023-11-24 00:00:00+0000', tz='UTC'), Timestamp('2023-11-25 00:00:00+0000', tz='UTC'), Timestamp('2023-11-26 00:00:00+0000', tz='UTC'), Timestamp('2023-11-27 00:00:00+0000', tz='UTC'), Timestamp('2023-11-28 00:00:00+0000', tz='UTC'), Timestamp('2023-11-29 00:00:00+0000', tz='UTC'), Timestamp('2023-11-30 00:00:00+0000', tz='UTC'), Timestamp('2023-12-01 00:00:00+0000', tz='UTC'), Timestamp('2023-12-02 00:00:00+0000', tz='UTC'), Timestamp('2023-12-03 00:00:00+0000', tz='UTC'), Timestamp('2023-12-04 00:00:00+0000', tz='UTC'), Timestamp('2023-12-05 00:00:00+0000', tz='UTC'), Timestamp('2023-12-06 00:00:00+0000', tz='UTC'), Timestamp('2023-12-07 00:00:00+0000', tz='UTC'), Timestamp('2023-12-08 00:00:00+0000', tz='UTC'), Timestamp('2023-12-09 00:00:00+0000', tz='UTC'), Timestamp('2023-12-10 00:00:00+0000', tz='UTC'), Timestamp('2023-12-11 00:00:00+0000', tz='UTC'), Timestamp('2023-12-12 00:00:00+0000', tz='UTC'), Timestamp('2023-12-13 00:00:00+0000', tz='UTC'), Timestamp('2023-12-14 00:00:00+0000', tz='UTC'), Timestamp('2023-12-15 00:00:00+0000', tz='UTC'), Timestamp('2023-12-16 00:00:00+0000', tz='UTC'), Timestamp('2023-12-17 00:00:00+0000', tz='UTC'), Timestamp('2023-12-18 00:00:00+0000', tz='UTC'), Timestamp('2023-12-19 00:00:00+0000', tz='UTC'), Timestamp('2023-12-20 00:00:00+0000', tz='UTC'), Timestamp('2023-12-21 00:00:00+0000', tz='UTC'), Timestamp('2023-12-22 00:00:00+0000', tz='UTC'), Timestamp('2023-12-23 00:00:00+0000', tz='UTC'), Timestamp('2023-12-24 00:00:00+0000', tz='UTC'), Timestamp('2023-12-26 00:00:00+0000', tz='UTC')]}\n"
     ]
    }
   ],
   "source": [
    "x = val_loader.dataset.X\n",
    "y = val_loader.dataset.y\n",
    "ref = val_loader.dataset.initial_price\n",
    "date = val_loader.dataset.current_date\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "    \n",
    "predict_out = np.multiply( (np.array(y_hat)+1).flatten(), ref)    \n",
    "\n",
    "print({\n",
    "    'predict':ref*(1+np.array(y_hat)), \n",
    "    'actual':ref*(1+np.array(y)), \n",
    "    'losses':nn.L1Loss()(y, y_hat),\n",
    "    'date':date,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          29792.015625,
          29908.744140625,
          29771.802734375,
          30084.5390625,
          29176.916015625,
          29227.390625,
          29354.97265625,
          29210.689453125,
          29319.24609375,
          29356.91796875,
          29275.30859375,
          29230.111328125,
          29675.732421875,
          29151.958984375,
          29178.6796875,
          29074.091796875,
          29042.126953125,
          29041.85546875,
          29180.578125,
          29765.4921875,
          29561.494140625,
          29429.591796875,
          29397.71484375,
          29415.96484375,
          29282.9140625,
          29408.443359375,
          29170.34765625,
          28701.779296875,
          26664.55078125,
          26049.556640625,
          26096.205078125,
          26189.583984375,
          26124.140625,
          26031.65625,
          26431.640625,
          26162.373046875,
          26047.66796875,
          26008.462890625,
          26089.693359375,
          26106.150390625,
          27727.392578125,
          27297.265625,
          25931.47265625,
          25800.724609375,
          25868.798828125,
          25969.56640625,
          25812.416015625,
          25779.982421875,
          25753.236328125,
          26240.1953125,
          25905.654296875,
          25895.677734375,
          25832.2265625,
          25162.654296875,
          25833.34375,
          26228.32421875,
          26539.673828125,
          26608.693359375,
          26568.28125,
          26534.1875,
          26754.28125,
          27211.1171875,
          27132.0078125,
          26567.6328125,
          26579.568359375,
          26579.390625,
          26256.826171875,
          26298.48046875,
          26217.25,
          26352.716796875,
          27021.546875,
          26911.720703125,
          26967.916015625,
          27983.75,
          27530.78515625,
          27429.978515625,
          27799.39453125,
          27415.912109375,
          27946.59765625,
          27968.83984375,
          27935.08984375,
          27583.677734375,
          27391.01953125,
          26873.3203125,
          26756.798828125,
          26862.375,
          26861.70703125,
          27159.65234375,
          28519.466796875,
          28415.748046875,
          28328.341796875,
          28719.806640625,
          29682.94921875,
          29918.412109375,
          29993.896484375,
          33086.234375,
          33901.52734375,
          34502.8203125,
          34156.6484375,
          33909.80078125,
          34089.57421875,
          34538.48046875,
          34502.36328125,
          34667.78125,
          35437.25390625,
          34938.2421875,
          34732.32421875,
          35082.1953125,
          35049.35546875,
          35037.37109375,
          35443.5625,
          35655.27734375,
          36693.125,
          37313.96875,
          37138.05078125,
          37054.51953125,
          36502.35546875,
          35537.640625,
          37880.58203125,
          36154.76953125,
          36596.68359375,
          36585.703125,
          37386.546875,
          37476.95703125,
          35813.8125,
          37432.33984375,
          37289.62109375,
          37720.28125,
          37796.79296875,
          37479.12109375,
          37254.16796875,
          37831.0859375,
          37858.4921875,
          37712.74609375,
          38688.75,
          39476.33203125,
          39978.390625,
          41980.09765625,
          44080.6484375,
          43746.4453125,
          43292.6640625,
          44166.6015625,
          43725.984375,
          43779.69921875,
          41243.83203125,
          41450.22265625,
          42890.7421875,
          43023.97265625,
          41929.7578125,
          42240.1171875,
          41364.6640625,
          42623.5390625,
          42270.52734375,
          43652.25,
          43869.15234375,
          43997.90234375,
          43739.54296875,
          43016.1171875,
          43452.5234375
         ],
         "high": [
          30195.53125,
          30045.998046875,
          29991.615234375,
          30330.640625,
          30093.39453125,
          29353.16015625,
          29675.552734375,
          29560.966796875,
          29521.513671875,
          29396.84375,
          29443.169921875,
          29489.873046875,
          29675.732421875,
          29987.998046875,
          29375.70703125,
          29302.078125,
          29102.46484375,
          29160.822265625,
          29244.28125,
          30176.796875,
          30093.435546875,
          29688.564453125,
          29517.7734375,
          29465.11328125,
          29441.43359375,
          29660.25390625,
          29439.12109375,
          29221.9765625,
          28745.947265625,
          26808.1953125,
          26249.44921875,
          26260.681640625,
          26220.201171875,
          26135.5078125,
          26786.8984375,
          26554.91015625,
          26248.103515625,
          26107.384765625,
          26165.373046875,
          26198.578125,
          28089.337890625,
          27760.16015625,
          27456.078125,
          26125.869140625,
          25970.28515625,
          26087.1484375,
          26081.525390625,
          25858.375,
          25953.015625,
          26409.302734375,
          26414.005859375,
          25921.9765625,
          25978.130859375,
          25883.947265625,
          26451.939453125,
          26376.11328125,
          26774.623046875,
          26840.498046875,
          26754.76953125,
          26617.998046875,
          27414.734375,
          27488.763671875,
          27379.505859375,
          27152.939453125,
          26726.078125,
          26634.185546875,
          26716.05859375,
          26421.5078125,
          26389.884765625,
          26817.841796875,
          27259.5,
          27225.9375,
          27091.794921875,
          28047.23828125,
          28494.458984375,
          27667.19140625,
          27826.658203125,
          28091.861328125,
          28252.537109375,
          28028.091796875,
          28102.169921875,
          27989.470703125,
          27715.84765625,
          27474.115234375,
          26921.439453125,
          27092.697265625,
          26969,
          27289.169921875,
          29448.138671875,
          28618.751953125,
          28889.009765625,
          28892.474609375,
          30104.0859375,
          30287.482421875,
          30199.43359375,
          34370.4375,
          35150.43359375,
          35133.7578125,
          34832.91015625,
          34238.2109375,
          34399.390625,
          34743.26171875,
          34843.93359375,
          34719.25390625,
          35527.9296875,
          35919.84375,
          34942.47265625,
          35256.03125,
          35340.33984375,
          35286.02734375,
          35892.41796875,
          35994.41796875,
          37926.2578125,
          37493.80078125,
          37407.09375,
          37227.69140625,
          37405.1171875,
          36753.3515625,
          37964.89453125,
          37934.625,
          36704.484375,
          36839.28125,
          37509.35546875,
          37756.8203125,
          37631.140625,
          37856.98046875,
          37643.91796875,
          38415.33984375,
          37892.4296875,
          37820.30078125,
          37559.35546875,
          38368.48046875,
          38366.11328125,
          38141.75390625,
          38954.109375,
          39678.9375,
          40135.60546875,
          42371.75,
          44408.6640625,
          44265.76953125,
          44042.58984375,
          44705.515625,
          44361.2578125,
          44034.625,
          43808.375,
          42048.3046875,
          43429.78125,
          43390.859375,
          43087.82421875,
          42664.9453125,
          42359.49609375,
          42720.296875,
          43354.296875,
          44275.5859375,
          44240.66796875,
          44367.95703125,
          44015.69921875,
          43945.5234375,
          43599.84765625
         ],
         "low": [
          29638.095703125,
          29733.8515625,
          29664.12109375,
          29741.52734375,
          28934.294921875,
          29062.43359375,
          29113.912109375,
          29099.3515625,
          29125.845703125,
          29264.166015625,
          29059.501953125,
          29131.578125,
          28657.0234375,
          28946.509765625,
          28959.48828125,
          28885.3359375,
          28957.796875,
          28963.833984375,
          28724.140625,
          29113.814453125,
          29376.80078125,
          29354.447265625,
          29253.517578125,
          29357.587890625,
          29265.806640625,
          29124.10546875,
          29088.853515625,
          28701.779296875,
          25409.111328125,
          25668.921875,
          25802.408203125,
          26004.314453125,
          25846.087890625,
          25520.728515625,
          25804.998046875,
          25914.92578125,
          25786.8125,
          25983.87890625,
          25965.09765625,
          25880.599609375,
          25912.62890625,
          27069.20703125,
          25752.9296875,
          25362.609375,
          25753.09375,
          25817.03125,
          25657.025390625,
          25589.98828125,
          25404.359375,
          25608.201171875,
          25677.48046875,
          25810.494140625,
          25640.26171875,
          24930.296875,
          25133.078125,
          25781.123046875,
          26171.451171875,
          26240.701171875,
          26473.890625,
          26445.07421875,
          26415.515625,
          26681.60546875,
          26864.08203125,
          26389.30078125,
          26495.533203125,
          26520.51953125,
          26221.05078125,
          26011.46875,
          26090.712890625,
          26111.46484375,
          26327.322265625,
          26721.763671875,
          26888.96875,
          26965.09375,
          27347.787109375,
          27216.001953125,
          27248.10546875,
          27375.6015625,
          27215.552734375,
          27870.423828125,
          27740.662109375,
          27302.5625,
          27301.654296875,
          26561.099609375,
          26558.3203125,
          26686.322265625,
          26814.5859375,
          26817.89453125,
          27130.47265625,
          28110.185546875,
          28174.251953125,
          28177.98828125,
          28601.669921875,
          29481.751953125,
          29720.3125,
          30097.828125,
          32880.76171875,
          33709.109375,
          33762.32421875,
          33416.88671875,
          33874.8046875,
          33947.56640625,
          34110.97265625,
          34083.30859375,
          34170.69140625,
          34401.57421875,
          34133.44140625,
          34616.69140625,
          34594.2421875,
          34765.36328125,
          34545.81640625,
          35147.80078125,
          35592.1015625,
          36362.75390625,
          36773.66796875,
          36779.1171875,
          36399.60546875,
          34948.5,
          35383.78125,
          35545.47265625,
          35901.234375,
          36233.3125,
          36414.59765625,
          36882.53125,
          35813.8125,
          35670.97265625,
          36923.86328125,
          37261.60546875,
          37617.41796875,
          37162.75,
          36750.12890625,
          36891.08984375,
          37612.6328125,
          37531.140625,
          37629.359375,
          38652.59375,
          39298.1640625,
          39978.62890625,
          41421.1484375,
          43478.08203125,
          42880.6484375,
          43125.296875,
          43627.59765625,
          43593.28515625,
          40234.578125,
          40667.5625,
          40676.8671875,
          41767.08984375,
          41692.96875,
          41723.11328125,
          41274.54296875,
          40530.2578125,
          41826.3359375,
          42223.81640625,
          43330.05078125,
          43441.96875,
          43351.35546875,
          42786.91796875,
          43413.09375
         ],
         "name": "Actual Price",
         "open": [
          29915.25,
          29805.111328125,
          29908.697265625,
          29790.111328125,
          30081.662109375,
          29178.970703125,
          29225.759765625,
          29353.798828125,
          29212.1640625,
          29319.4453125,
          29357.09375,
          29278.314453125,
          29230.873046875,
          29704.146484375,
          29161.8125,
          29174.3828125,
          29075.388671875,
          29043.701171875,
          29038.513671875,
          29180.01953125,
          29766.6953125,
          29563.97265625,
          29424.90234375,
          29399.787109375,
          29416.59375,
          29283.263671875,
          29408.048828125,
          29169.07421875,
          28699.802734375,
          26636.078125,
          26047.83203125,
          26096.861328125,
          26188.69140625,
          26130.748046875,
          26040.474609375,
          26431.51953125,
          26163.6796875,
          26047.234375,
          26008.2421875,
          26089.615234375,
          26102.486328125,
          27726.083984375,
          27301.9296875,
          25934.021484375,
          25800.91015625,
          25869.47265625,
          25968.169921875,
          25814.95703125,
          25783.931640625,
          25748.3125,
          26245.208984375,
          25905.42578125,
          25895.2109375,
          25831.71484375,
          25160.658203125,
          25837.5546875,
          26228.27734375,
          26533.818359375,
          26606.19921875,
          26567.927734375,
          26532.994140625,
          26760.8515625,
          27210.228515625,
          27129.83984375,
          26564.056640625,
          26578.556640625,
          26579.373046875,
          26253.775390625,
          26294.7578125,
          26209.498046875,
          26355.8125,
          27024.841796875,
          26911.689453125,
          26967.396484375,
          27976.798828125,
          27508.251953125,
          27429.07421875,
          27798.646484375,
          27412.123046875,
          27946.78125,
          27971.677734375,
          27934.47265625,
          27589.201171875,
          27392.076171875,
          26873.29296875,
          26752.87890625,
          26866.203125,
          26858.01171875,
          27162.62890625,
          28522.09765625,
          28413.53125,
          28332.416015625,
          28732.8125,
          29683.380859375,
          29918.654296875,
          30140.685546875,
          33077.3046875,
          33916.04296875,
          34504.2890625,
          34156.5,
          33907.72265625,
          34089.37109375,
          34531.7421875,
          34500.078125,
          34657.2734375,
          35441.578125,
          34942.47265625,
          34736.32421875,
          35090.01171875,
          35044.7890625,
          35047.79296875,
          35419.4765625,
          35633.6328125,
          36702.25,
          37310.0703125,
          37133.9921875,
          37070.3046875,
          36491.7890625,
          35548.11328125,
          37879.98046875,
          36164.82421875,
          36625.37109375,
          36585.765625,
          37374.07421875,
          37469.16015625,
          35756.5546875,
          37420.43359375,
          37296.31640625,
          37721.4140625,
          37796.828125,
          37454.19140625,
          37247.9921875,
          37826.10546875,
          37861.1171875,
          37718.0078125,
          38689.27734375,
          39472.20703125,
          39978.62890625,
          41986.265625,
          44080.0234375,
          43769.1328125,
          43293.13671875,
          44180.01953125,
          43728.3828125,
          43792.01953125,
          41238.734375,
          41468.46484375,
          42884.26171875,
          43028.25,
          41937.7421875,
          42236.109375,
          41348.203125,
          42641.51171875,
          42261.30078125,
          43648.125,
          43868.98828125,
          44012.19921875,
          43728.3671875,
          43599.84765625
         ],
         "type": "candlestick",
         "x": [
          "2023-07-20T00:00:00+00:00",
          "2023-07-21T00:00:00+00:00",
          "2023-07-22T00:00:00+00:00",
          "2023-07-23T00:00:00+00:00",
          "2023-07-24T00:00:00+00:00",
          "2023-07-25T00:00:00+00:00",
          "2023-07-26T00:00:00+00:00",
          "2023-07-27T00:00:00+00:00",
          "2023-07-28T00:00:00+00:00",
          "2023-07-29T00:00:00+00:00",
          "2023-07-30T00:00:00+00:00",
          "2023-07-31T00:00:00+00:00",
          "2023-08-01T00:00:00+00:00",
          "2023-08-02T00:00:00+00:00",
          "2023-08-03T00:00:00+00:00",
          "2023-08-04T00:00:00+00:00",
          "2023-08-05T00:00:00+00:00",
          "2023-08-06T00:00:00+00:00",
          "2023-08-07T00:00:00+00:00",
          "2023-08-08T00:00:00+00:00",
          "2023-08-09T00:00:00+00:00",
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "name": "Predicted Price",
         "type": "scatter",
         "x": [
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          26192.947128653526,
          27036.663043951616,
          26820.61812620901,
          25858.891797481687,
          27396.557350620045,
          27992.080000113463,
          25704.6158127588,
          26876.691684015677,
          27345.775099025108,
          25954.499898877926,
          27025.970687001944,
          29196.052820045268,
          28364.558273219736,
          27414.80548842298,
          27315.12021848187,
          27959.417657987215,
          27390.58543447836,
          27917.710919122328,
          27528.55117158685,
          27765.051171211526,
          28374.869612394832,
          28404.786274397513,
          28775.97032088088,
          27985.802952626836,
          26418.84701417014,
          28151.319611175335,
          28229.750774917193,
          29269.447784471326,
          28929.05949323508,
          29486.220863526687,
          28661.664243710693,
          30359.686822073534,
          27732.422849878203,
          29847.325226748362,
          25640.036650811322,
          29422.29324457515,
          27429.828285715543,
          28892.772316098213,
          27729.55422724597,
          28322.824578638887,
          27476.663333082106,
          29071.17093304172,
          30521.6900583636,
          29335.651945476886,
          28583.380308860913,
          29533.81153202383,
          29337.57127242163,
          29810.766117511317,
          29739.827718263725,
          29344.374024823308,
          29827.808311352972,
          29522.837137316354,
          29726.038339753635,
          28924.176425982267,
          28995.376887561288,
          28147.78149522934,
          28778.845595736988,
          28850.97200140939,
          28965.479149492458,
          30737.092476722784,
          30245.670871414943,
          30141.55999367684,
          30365.092009824933,
          32198.175894567277,
          31169.73640670115,
          31725.871367613086,
          33280.46049725474,
          36777.997109658085,
          36451.2903247159,
          35042.9240593405,
          36180.85390430689,
          35977.53455135133,
          36532.970171264606,
          35459.04174643103,
          37344.87864545267,
          38718.08839930454,
          37204.848942874,
          35891.52908581169,
          37487.17153498251,
          37186.34758528322,
          38615.26342397975,
          38480.9298398206,
          36772.02941360045,
          38115.15304870857,
          36950.85151359299,
          37393.7955675032,
          37600.07470067218,
          36865.693202672526,
          36160.86366339214,
          39100.51285992935,
          38199.31197999418,
          38046.217767625116,
          37996.01360782981,
          39513.49407103052,
          39961.38105074479,
          38074.5237642352,
          39121.40882560052,
          38975.62798351329,
          39020.675148328766,
          39117.92172789574,
          38627.951569617726,
          38069.06953532249,
          38835.38293306157,
          38804.01284898631,
          38553.08339317143,
          39227.16820238717,
          40642.0191362449,
          39659.02968313964,
          43338.23558702972,
          48797.30936520919,
          45767.86337211728,
          45582.97685986664,
          47323.939076317474,
          48689.732624053955,
          45458.5096669551,
          44303.66654879879,
          42803.2704946436,
          43107.93473408092,
          44831.09209925495,
          43989.68503642827,
          47331.305535491556,
          42733.38354924694,
          43835.12310472317,
          42522.16692315927,
          45094.57319683395,
          46447.35622437112,
          45246.10378257604,
          44723.903235832695,
          43922.77336579375
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "BTC-USD price predictions",
         "x": 0.3
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = prices_df_val.iloc[200:]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "        go.Candlestick(\n",
    "            x=df['Date'],\n",
    "            open=df['Open'],\n",
    "            high=df['High'],\n",
    "            low=df['Low'],\n",
    "            close=df['Close'],\n",
    "            name='Actual Price'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out,\n",
    "            line=dict(color='blue'),\n",
    "            name='Predicted Price'\n",
    "            )\n",
    "    ]).update_layout(title_text=tickers+' price predictions', title_x=0.3)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
