{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas_ta as ta\n",
    "import getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_param = {\n",
    "    'win_size':22,\n",
    "    'stride':1,\n",
    "    'split':True,\n",
    "    'number_y':1,\n",
    "    'random_state':420,\n",
    "    'test_size':0.2,\n",
    "}\n",
    "\n",
    "v_preprocess_param = {\n",
    "    'win_size':22,\n",
    "    'stride':1,\n",
    "    'split':False,\n",
    "    'number_y':1,\n",
    "    'random_state':420,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = 'BTC-USD'\n",
    "\n",
    "prices_df = getData.loader(tickers=tickers, interval=\"1d\", period='max', end=\"2023-01-01\").dataframe\n",
    "prices_df_val = getData.loader(tickers=tickers, interval=\"1d\", start='2023-01-01').dataframe\n",
    "\n",
    "datasets = getData.preprocessor(prices_df, preprocess_param=preprocess_param).dataset\n",
    "val_sets = getData.preprocessor(prices_df_val, preprocess_param=v_preprocess_param).dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PriceHistoryDataset(Dataset):\n",
    "    def __init__(self, dataset, to_predict=['Open', 'High', 'Low', 'Close']):\n",
    "        y = dataset['y'][:,:,self.__map_to_indices(to_predict)]\n",
    "        x = dataset['x']\n",
    "        self.columns = dataset['columns']\n",
    "        self.initial_price = dataset['initial price']\n",
    "        self.current_date = dataset['current date']\n",
    "        \n",
    "        self.X = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __map_to_indices(self, args):\n",
    "        mapping = {'Open': 0, 'High': 1, 'Low': 2, 'Close': 3}\n",
    "        return [mapping[arg] for arg in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = datasets['columns']\n",
    "\n",
    "to_predict = ['High', 'Low', 'Close']\n",
    "\n",
    "train_set = PriceHistoryDataset(datasets['train'], to_predict)\n",
    "test_set = PriceHistoryDataset(datasets['test'], to_predict)\n",
    "val_set = PriceHistoryDataset(val_sets, to_predict)\n",
    "\n",
    "train_loader= DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: otobuf is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LSTMModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hidden_size, lstm_layers, input_size=8, output_size=3, dropout=0.05):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=lstm_layers, dropout=dropout)\n",
    "        \n",
    "        self.out_linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # keep track of losses function.\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.loss_func = nn.L1Loss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:,-1:,:]\n",
    "        \n",
    "        output = self.out_linear(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.train_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.test_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.test_losses).mean()\n",
    "        print(f'Test Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.train_losses).mean()\n",
    "        print(f'Train Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = LSTMModel(output_size=len(to_predict), hidden_size=256, lstm_layers=6, dropout=0.0)\n",
    "\n",
    "# checkpoint = torch.load(\"model\\LSTM_BTC\\checkpoints\\epoch=999-step=9000.ckpt\")\n",
    "# model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2904835\n",
      "Number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Number of layers:\", len(list(model.children())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: otobuf is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type   | Params\n",
      "--------------------------------------\n",
      "0 | lstm       | LSTM   | 2.9 M \n",
      "1 | out_linear | Linear | 771   \n",
      "2 | loss_func  | L1Loss | 0     \n",
      "--------------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.619    Total estimated model params size (MB)\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a1f22890844e64aa333eb2a5e04ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8840090036392212\n",
      "Train Loss: 0.5281427502632141\n",
      "Train Loss: 0.40561819076538086\n",
      "Train Loss: 0.34356728196144104\n",
      "Train Loss: 0.3058818578720093\n",
      "Train Loss: 0.28047439455986023\n",
      "Train Loss: 0.2624101936817169\n",
      "Train Loss: 0.24886280298233032\n",
      "Train Loss: 0.23815207183361053\n",
      "Train Loss: 0.22950105369091034\n",
      "Train Loss: 0.2224210500717163\n",
      "Train Loss: 0.21652688086032867\n",
      "Train Loss: 0.21153706312179565\n",
      "Train Loss: 0.20725451409816742\n",
      "Train Loss: 0.20354041457176208\n",
      "Train Loss: 0.20029059052467346\n",
      "Train Loss: 0.19742253422737122\n",
      "Train Loss: 0.19488702714443207\n",
      "Train Loss: 0.192610964179039\n",
      "Train Loss: 0.19056235253810883\n",
      "Train Loss: 0.18871037662029266\n",
      "Train Loss: 0.18702828884124756\n",
      "Train Loss: 0.18549567461013794\n",
      "Train Loss: 0.18409326672554016\n",
      "Train Loss: 0.18280351161956787\n",
      "Train Loss: 0.18161506950855255\n",
      "Train Loss: 0.18051232397556305\n",
      "Train Loss: 0.17948530614376068\n",
      "Train Loss: 0.17852699756622314\n",
      "Train Loss: 0.17763052880764008\n",
      "Train Loss: 0.1767897605895996\n",
      "Train Loss: 0.17599941790103912\n",
      "Train Loss: 0.1752588152885437\n",
      "Train Loss: 0.174566388130188\n",
      "Train Loss: 0.17392709851264954\n",
      "Train Loss: 0.17333181202411652\n",
      "Train Loss: 0.1727694869041443\n",
      "Train Loss: 0.1722441166639328\n",
      "Train Loss: 0.1717483252286911\n",
      "Train Loss: 0.17127223312854767\n",
      "Train Loss: 0.17080701887607574\n",
      "Train Loss: 0.17035914957523346\n",
      "Train Loss: 0.16992545127868652\n",
      "Train Loss: 0.16951793432235718\n",
      "Train Loss: 0.16912390291690826\n",
      "Train Loss: 0.1687479466199875\n",
      "Train Loss: 0.1683884710073471\n",
      "Train Loss: 0.16804087162017822\n",
      "Train Loss: 0.16770897805690765\n",
      "Train Loss: 0.16739508509635925\n",
      "Train Loss: 0.16709111630916595\n",
      "Train Loss: 0.166799858212471\n",
      "Train Loss: 0.16651707887649536\n",
      "Train Loss: 0.16624408960342407\n",
      "Train Loss: 0.16597872972488403\n",
      "Train Loss: 0.16572456061840057\n",
      "Train Loss: 0.1680784970521927\n",
      "Train Loss: 0.1687278300523758\n",
      "Train Loss: 0.16865697503089905\n",
      "Train Loss: 0.168556347489357\n",
      "Train Loss: 0.16839680075645447\n",
      "Train Loss: 0.16821259260177612\n",
      "Train Loss: 0.16800682246685028\n",
      "Train Loss: 0.16782821714878082\n",
      "Train Loss: 0.16763775050640106\n",
      "Train Loss: 0.16744646430015564\n",
      "Train Loss: 0.16724225878715515\n",
      "Train Loss: 0.16706058382987976\n",
      "Train Loss: 0.16687574982643127\n",
      "Train Loss: 0.16669343411922455\n",
      "Train Loss: 0.16653308272361755\n",
      "Train Loss: 0.16639471054077148\n",
      "Train Loss: 0.1664678156375885\n",
      "Train Loss: 0.16641061007976532\n",
      "Train Loss: 0.16626481711864471\n",
      "Train Loss: 0.16610677540302277\n",
      "Train Loss: 0.16593535244464874\n",
      "Train Loss: 0.1657598912715912\n",
      "Train Loss: 0.16558977961540222\n",
      "Train Loss: 0.16542208194732666\n",
      "Train Loss: 0.1652558147907257\n",
      "Train Loss: 0.16510453820228577\n",
      "Train Loss: 0.16497932374477386\n",
      "Train Loss: 0.16489465534687042\n",
      "Train Loss: 0.16481956839561462\n",
      "Train Loss: 0.16468481719493866\n",
      "Train Loss: 0.16456320881843567\n",
      "Train Loss: 0.16447238624095917\n",
      "Train Loss: 0.16441556811332703\n",
      "Train Loss: 0.1643393188714981\n",
      "Train Loss: 0.1642206758260727\n",
      "Train Loss: 0.16408641636371613\n",
      "Train Loss: 0.1639525294303894\n",
      "Train Loss: 0.16382284462451935\n",
      "Train Loss: 0.16370050609111786\n",
      "Train Loss: 0.16358135640621185\n",
      "Train Loss: 0.16346926987171173\n",
      "Train Loss: 0.16335710883140564\n",
      "Train Loss: 0.16324280202388763\n",
      "Train Loss: 0.163126602768898\n",
      "Train Loss: 0.16301751136779785\n",
      "Train Loss: 0.1629115641117096\n",
      "Train Loss: 0.16280589997768402\n",
      "Train Loss: 0.16270090639591217\n",
      "Train Loss: 0.16259917616844177\n",
      "Train Loss: 0.1624976396560669\n",
      "Train Loss: 0.16239848732948303\n",
      "Train Loss: 0.16230063140392303\n",
      "Train Loss: 0.1622065156698227\n",
      "Train Loss: 0.16211871802806854\n",
      "Train Loss: 0.16203050315380096\n",
      "Train Loss: 0.16194210946559906\n",
      "Train Loss: 0.16184987127780914\n",
      "Train Loss: 0.1617536097764969\n",
      "Train Loss: 0.16166089475154877\n",
      "Train Loss: 0.1615796983242035\n",
      "Train Loss: 0.16150356829166412\n",
      "Train Loss: 0.1614266335964203\n",
      "Train Loss: 0.16134464740753174\n",
      "Train Loss: 0.16126038134098053\n",
      "Train Loss: 0.1611728072166443\n",
      "Train Loss: 0.1610897183418274\n",
      "Train Loss: 0.16101528704166412\n",
      "Train Loss: 0.16094331443309784\n",
      "Train Loss: 0.16087637841701508\n",
      "Train Loss: 0.160807266831398\n",
      "Train Loss: 0.16073577105998993\n",
      "Train Loss: 0.1606622189283371\n",
      "Train Loss: 0.16058480739593506\n",
      "Train Loss: 0.1605096310377121\n",
      "Train Loss: 0.16043943166732788\n",
      "Train Loss: 0.1603735238313675\n",
      "Train Loss: 0.16031090915203094\n",
      "Train Loss: 0.1602460891008377\n",
      "Train Loss: 0.1601785272359848\n",
      "Train Loss: 0.1601104736328125\n",
      "Train Loss: 0.16004100441932678\n",
      "Train Loss: 0.15997396409511566\n",
      "Train Loss: 0.1599087119102478\n",
      "Train Loss: 0.15984979271888733\n",
      "Train Loss: 0.15979138016700745\n",
      "Train Loss: 0.1597297191619873\n",
      "Train Loss: 0.15966659784317017\n",
      "Train Loss: 0.15960688889026642\n",
      "Train Loss: 0.1595476120710373\n",
      "Train Loss: 0.15948857367038727\n",
      "Train Loss: 0.15943388640880585\n",
      "Train Loss: 0.15938299894332886\n",
      "Train Loss: 0.15933625400066376\n",
      "Train Loss: 0.15928258001804352\n",
      "Train Loss: 0.15922383964061737\n",
      "Train Loss: 0.15916702151298523\n",
      "Train Loss: 0.15911909937858582\n",
      "Train Loss: 0.1590714454650879\n",
      "Train Loss: 0.15902738273143768\n",
      "Train Loss: 0.15898358821868896\n",
      "Train Loss: 0.1589280515909195\n",
      "Train Loss: 0.158874973654747\n",
      "Train Loss: 0.15882717072963715\n",
      "Train Loss: 0.15878410637378693\n",
      "Train Loss: 0.1587369292974472\n",
      "Train Loss: 0.15868748724460602\n",
      "Train Loss: 0.15863576531410217\n",
      "Train Loss: 0.15858802199363708\n",
      "Train Loss: 0.1585448980331421\n",
      "Train Loss: 0.1585036963224411\n",
      "Train Loss: 0.15846115350723267\n",
      "Train Loss: 0.15841294825077057\n",
      "Train Loss: 0.15836544334888458\n",
      "Train Loss: 0.158318892121315\n",
      "Train Loss: 0.1582733392715454\n",
      "Train Loss: 0.15822964906692505\n",
      "Train Loss: 0.1581876575946808\n",
      "Train Loss: 0.15814535319805145\n",
      "Train Loss: 0.15810468792915344\n",
      "Train Loss: 0.1580619066953659\n",
      "Train Loss: 0.1580205261707306\n",
      "Train Loss: 0.1579822450876236\n",
      "Train Loss: 0.15794888138771057\n",
      "Train Loss: 0.15791559219360352\n",
      "Train Loss: 0.15787500143051147\n",
      "Train Loss: 0.15783315896987915\n",
      "Train Loss: 0.15779753029346466\n",
      "Train Loss: 0.15776695311069489\n",
      "Train Loss: 0.1577313244342804\n",
      "Train Loss: 0.15769024193286896\n",
      "Train Loss: 0.15765349566936493\n",
      "Train Loss: 0.15761928260326385\n",
      "Train Loss: 0.15758270025253296\n",
      "Train Loss: 0.1575453281402588\n",
      "Train Loss: 0.15750546753406525\n",
      "Train Loss: 0.1574677973985672\n",
      "Train Loss: 0.15743349492549896\n",
      "Train Loss: 0.1574006974697113\n",
      "Train Loss: 0.15736806392669678\n",
      "Train Loss: 0.15733173489570618\n",
      "Train Loss: 0.1572950929403305\n",
      "Train Loss: 0.15725839138031006\n",
      "Train Loss: 0.1572253257036209\n",
      "Train Loss: 0.15719418227672577\n",
      "Train Loss: 0.15716274082660675\n",
      "Train Loss: 0.1571267694234848\n",
      "Train Loss: 0.15709522366523743\n",
      "Train Loss: 0.1570647954940796\n",
      "Train Loss: 0.15703177452087402\n",
      "Train Loss: 0.15699665248394012\n",
      "Train Loss: 0.15696367621421814\n",
      "Train Loss: 0.15693385899066925\n",
      "Train Loss: 0.1569012701511383\n",
      "Train Loss: 0.15687033534049988\n",
      "Train Loss: 0.1568400114774704\n",
      "Train Loss: 0.15680859982967377\n",
      "Train Loss: 0.15677949786186218\n",
      "Train Loss: 0.15675216913223267\n",
      "Train Loss: 0.1567196398973465\n",
      "Train Loss: 0.15668779611587524\n",
      "Train Loss: 0.15665733814239502\n",
      "Train Loss: 0.15662874281406403\n",
      "Train Loss: 0.15660113096237183\n",
      "Train Loss: 0.15657518804073334\n",
      "Train Loss: 0.15654517710208893\n",
      "Train Loss: 0.1565159559249878\n",
      "Train Loss: 0.15648908913135529\n",
      "Train Loss: 0.1564623862504959\n",
      "Train Loss: 0.1564357429742813\n",
      "Train Loss: 0.15640711784362793\n",
      "Train Loss: 0.15638130903244019\n",
      "Train Loss: 0.15635783970355988\n",
      "Train Loss: 0.15633229911327362\n",
      "Train Loss: 0.1563059687614441\n",
      "Train Loss: 0.15628311038017273\n",
      "Train Loss: 0.15625730156898499\n",
      "Train Loss: 0.15622983872890472\n",
      "Train Loss: 0.15620331466197968\n",
      "Train Loss: 0.15617603063583374\n",
      "Train Loss: 0.1561492681503296\n",
      "Train Loss: 0.15612494945526123\n",
      "Train Loss: 0.15610069036483765\n",
      "Train Loss: 0.15607771277427673\n",
      "Train Loss: 0.15605518221855164\n",
      "Train Loss: 0.15603244304656982\n",
      "Train Loss: 0.15601135790348053\n",
      "Train Loss: 0.15599210560321808\n",
      "Train Loss: 0.1559695303440094\n",
      "Train Loss: 0.15594501793384552\n",
      "Train Loss: 0.1559228152036667\n",
      "Train Loss: 0.1559010148048401\n",
      "Train Loss: 0.15587830543518066\n",
      "Train Loss: 0.15585428476333618\n",
      "Train Loss: 0.15583312511444092\n",
      "Train Loss: 0.15581034123897552\n",
      "Train Loss: 0.15578702092170715\n",
      "Train Loss: 0.15576590597629547\n",
      "Train Loss: 0.15574783086776733\n",
      "Train Loss: 0.15572711825370789\n",
      "Train Loss: 0.15570330619812012\n",
      "Train Loss: 0.15568134188652039\n",
      "Train Loss: 0.15566305816173553\n",
      "Train Loss: 0.15564319491386414\n",
      "Train Loss: 0.15562188625335693\n",
      "Train Loss: 0.15560382604599\n",
      "Train Loss: 0.15558472275733948\n",
      "Train Loss: 0.15556325018405914\n",
      "Train Loss: 0.1555424928665161\n",
      "Train Loss: 0.1555214375257492\n",
      "Train Loss: 0.15550775825977325\n",
      "Train Loss: 0.15550215542316437\n",
      "Train Loss: 0.15548843145370483\n",
      "Train Loss: 0.1554737091064453\n",
      "Train Loss: 0.15546081960201263\n",
      "Train Loss: 0.1554473638534546\n",
      "Train Loss: 0.15543125569820404\n",
      "Train Loss: 0.1554180234670639\n",
      "Train Loss: 0.15540383756160736\n",
      "Train Loss: 0.15538878738880157\n",
      "Train Loss: 0.15537385642528534\n",
      "Train Loss: 0.15535801649093628\n",
      "Train Loss: 0.1553429365158081\n",
      "Train Loss: 0.15532764792442322\n",
      "Train Loss: 0.15531298518180847\n",
      "Train Loss: 0.15529808402061462\n",
      "Train Loss: 0.15528272092342377\n",
      "Train Loss: 0.15526701509952545\n",
      "Train Loss: 0.15525199472904205\n",
      "Train Loss: 0.15523703396320343\n",
      "Train Loss: 0.15522229671478271\n",
      "Train Loss: 0.15520769357681274\n",
      "Train Loss: 0.15519285202026367\n",
      "Train Loss: 0.15517877042293549\n",
      "Train Loss: 0.15516509115695953\n",
      "Train Loss: 0.15514962375164032\n",
      "Train Loss: 0.15513496100902557\n",
      "Train Loss: 0.15511971712112427\n",
      "Train Loss: 0.15510430932044983\n",
      "Train Loss: 0.1550893634557724\n",
      "Train Loss: 0.15507498383522034\n",
      "Train Loss: 0.15505965054035187\n",
      "Train Loss: 0.1550453156232834\n",
      "Train Loss: 0.15503036975860596\n",
      "Train Loss: 0.15501533448696136\n",
      "Train Loss: 0.15500053763389587\n",
      "Train Loss: 0.1549854576587677\n",
      "Train Loss: 0.15497085452079773\n",
      "Train Loss: 0.1549578160047531\n",
      "Train Loss: 0.1549457460641861\n",
      "Train Loss: 0.15493233501911163\n",
      "Train Loss: 0.1549185812473297\n",
      "Train Loss: 0.1549055278301239\n",
      "Train Loss: 0.1548919528722763\n",
      "Train Loss: 0.15487860143184662\n",
      "Train Loss: 0.15486516058444977\n",
      "Train Loss: 0.15485212206840515\n",
      "Train Loss: 0.1548391431570053\n",
      "Train Loss: 0.1548258364200592\n",
      "Train Loss: 0.1548120528459549\n",
      "Train Loss: 0.15479882061481476\n",
      "Train Loss: 0.1547851860523224\n",
      "Train Loss: 0.15477178990840912\n",
      "Train Loss: 0.15475884079933167\n",
      "Train Loss: 0.15474659204483032\n",
      "Train Loss: 0.1547354906797409\n",
      "Train Loss: 0.15472525358200073\n",
      "Train Loss: 0.15471164882183075\n",
      "Train Loss: 0.15469911694526672\n",
      "Train Loss: 0.1546863466501236\n",
      "Train Loss: 0.1546735167503357\n",
      "Train Loss: 0.1546611338853836\n",
      "Train Loss: 0.15464834868907928\n",
      "Train Loss: 0.15463729202747345\n",
      "Train Loss: 0.1546262502670288\n",
      "Train Loss: 0.15461356937885284\n",
      "Train Loss: 0.154602512717247\n",
      "Train Loss: 0.1545904576778412\n",
      "Train Loss: 0.15457859635353088\n",
      "Train Loss: 0.15456780791282654\n",
      "Train Loss: 0.1545558124780655\n",
      "Train Loss: 0.15454302728176117\n",
      "Train Loss: 0.15453015267848969\n",
      "Train Loss: 0.1545182764530182\n",
      "Train Loss: 0.15450666844844818\n",
      "Train Loss: 0.15449462831020355\n",
      "Train Loss: 0.15448394417762756\n",
      "Train Loss: 0.15447399020195007\n",
      "Train Loss: 0.1544673889875412\n",
      "Train Loss: 0.1544594019651413\n",
      "Train Loss: 0.15444926917552948\n",
      "Train Loss: 0.15444126725196838\n",
      "Train Loss: 0.1544373780488968\n",
      "Train Loss: 0.154443621635437\n",
      "Train Loss: 0.15444834530353546\n",
      "Train Loss: 0.1544426679611206\n",
      "Train Loss: 0.1544346958398819\n",
      "Train Loss: 0.15442654490470886\n",
      "Train Loss: 0.15441752970218658\n",
      "Train Loss: 0.1544080376625061\n",
      "Train Loss: 0.1543986201286316\n",
      "Train Loss: 0.15438933670520782\n",
      "Train Loss: 0.1543794423341751\n",
      "Train Loss: 0.1543695330619812\n",
      "Train Loss: 0.15435966849327087\n",
      "Train Loss: 0.15435007214546204\n",
      "Train Loss: 0.15434002876281738\n",
      "Train Loss: 0.15433044731616974\n",
      "Train Loss: 0.15432049334049225\n",
      "Train Loss: 0.1543104499578476\n",
      "Train Loss: 0.15430033206939697\n",
      "Train Loss: 0.15429028868675232\n",
      "Train Loss: 0.15428052842617035\n",
      "Train Loss: 0.15427114069461823\n",
      "Train Loss: 0.1542610228061676\n",
      "Train Loss: 0.1542510837316513\n",
      "Train Loss: 0.15424060821533203\n",
      "Train Loss: 0.1542304903268814\n",
      "Train Loss: 0.15422071516513824\n",
      "Train Loss: 0.15421107411384583\n",
      "Train Loss: 0.15420088171958923\n",
      "Train Loss: 0.1541900485754013\n",
      "Train Loss: 0.15417999029159546\n",
      "Train Loss: 0.15416981279850006\n",
      "Train Loss: 0.1541597694158554\n",
      "Train Loss: 0.15415063500404358\n",
      "Train Loss: 0.15414288640022278\n",
      "Train Loss: 0.1541355550289154\n",
      "Train Loss: 0.15412762761116028\n",
      "Train Loss: 0.15411899983882904\n",
      "Train Loss: 0.15411058068275452\n",
      "Train Loss: 0.15410116314888\n",
      "Train Loss: 0.15409205853939056\n",
      "Train Loss: 0.154082790017128\n",
      "Train Loss: 0.15407370030879974\n",
      "Train Loss: 0.1540641486644745\n",
      "Train Loss: 0.15405429899692535\n",
      "Train Loss: 0.15404406189918518\n",
      "Train Loss: 0.15403412282466888\n",
      "Train Loss: 0.1540244221687317\n",
      "Train Loss: 0.15401598811149597\n",
      "Train Loss: 0.15400876104831696\n",
      "Train Loss: 0.15400001406669617\n",
      "Train Loss: 0.15399101376533508\n",
      "Train Loss: 0.15398183465003967\n",
      "Train Loss: 0.15397247672080994\n",
      "Train Loss: 0.15396398305892944\n",
      "Train Loss: 0.1539558470249176\n",
      "Train Loss: 0.15394791960716248\n",
      "Train Loss: 0.1539391428232193\n",
      "Train Loss: 0.15393008291721344\n",
      "Train Loss: 0.15392082929611206\n",
      "Train Loss: 0.15391141176223755\n",
      "Train Loss: 0.15390220284461975\n",
      "Train Loss: 0.15389323234558105\n",
      "Train Loss: 0.15388517081737518\n",
      "Train Loss: 0.15387733280658722\n",
      "Train Loss: 0.15386858582496643\n",
      "Train Loss: 0.15386082231998444\n",
      "Train Loss: 0.153852641582489\n",
      "Train Loss: 0.1538448929786682\n",
      "Train Loss: 0.1538366675376892\n",
      "Train Loss: 0.15382838249206543\n",
      "Train Loss: 0.15382002294063568\n",
      "Train Loss: 0.15381166338920593\n",
      "Train Loss: 0.15380331873893738\n",
      "Train Loss: 0.1537950336933136\n",
      "Train Loss: 0.15379083156585693\n",
      "Train Loss: 0.15378554165363312\n",
      "Train Loss: 0.15378041565418243\n",
      "Train Loss: 0.15377682447433472\n",
      "Train Loss: 0.15377098321914673\n",
      "Train Loss: 0.15376468002796173\n",
      "Train Loss: 0.15375830233097076\n",
      "Train Loss: 0.15375101566314697\n",
      "Train Loss: 0.15374355018138885\n",
      "Train Loss: 0.15373574197292328\n",
      "Train Loss: 0.15372787415981293\n",
      "Train Loss: 0.15372039377689362\n",
      "Train Loss: 0.15371328592300415\n",
      "Train Loss: 0.15370531380176544\n",
      "Train Loss: 0.15369820594787598\n",
      "Train Loss: 0.15369126200675964\n",
      "Train Loss: 0.15368501842021942\n",
      "Train Loss: 0.1536792516708374\n",
      "Train Loss: 0.15367241203784943\n",
      "Train Loss: 0.15366511046886444\n",
      "Train Loss: 0.1536579728126526\n",
      "Train Loss: 0.15365050733089447\n",
      "Train Loss: 0.15364430844783783\n",
      "Train Loss: 0.15363705158233643\n",
      "Train Loss: 0.1536300629377365\n",
      "Train Loss: 0.15362296998500824\n",
      "Train Loss: 0.15361641347408295\n",
      "Train Loss: 0.15361011028289795\n",
      "Train Loss: 0.15360285341739655\n",
      "Train Loss: 0.15359582006931305\n",
      "Train Loss: 0.15358851850032806\n",
      "Train Loss: 0.15358199179172516\n",
      "Train Loss: 0.15357570350170135\n",
      "Train Loss: 0.15356917679309845\n",
      "Train Loss: 0.1535625159740448\n",
      "Train Loss: 0.15355545282363892\n",
      "Train Loss: 0.15354767441749573\n",
      "Train Loss: 0.15353959798812866\n",
      "Train Loss: 0.15353155136108398\n",
      "Train Loss: 0.153524249792099\n",
      "Train Loss: 0.15351739525794983\n",
      "Train Loss: 0.1535101681947708\n",
      "Train Loss: 0.15350258350372314\n",
      "Train Loss: 0.15349505841732025\n",
      "Train Loss: 0.15348748862743378\n",
      "Train Loss: 0.1534808874130249\n",
      "Train Loss: 0.15347537398338318\n",
      "Train Loss: 0.15346907079219818\n",
      "Train Loss: 0.15346474945545197\n",
      "Train Loss: 0.15346212685108185\n",
      "Train Loss: 0.15345695614814758\n",
      "Train Loss: 0.1534510850906372\n",
      "Train Loss: 0.1534450650215149\n",
      "Train Loss: 0.1534385085105896\n",
      "Train Loss: 0.15343298017978668\n",
      "Train Loss: 0.1534266620874405\n",
      "Train Loss: 0.15342159569263458\n",
      "Train Loss: 0.15341493487358093\n",
      "Train Loss: 0.15341031551361084\n",
      "Train Loss: 0.15340401232242584\n",
      "Train Loss: 0.15339812636375427\n",
      "Train Loss: 0.15339115262031555\n",
      "Train Loss: 0.1533847451210022\n",
      "Train Loss: 0.1533777266740799\n",
      "Train Loss: 0.15337121486663818\n",
      "Train Loss: 0.15336498618125916\n",
      "Train Loss: 0.1533585786819458\n",
      "Train Loss: 0.15335245430469513\n",
      "Train Loss: 0.15334683656692505\n",
      "Train Loss: 0.15334077179431915\n",
      "Train Loss: 0.15333400666713715\n",
      "Train Loss: 0.15332724153995514\n",
      "Train Loss: 0.15332061052322388\n",
      "Train Loss: 0.15331383049488068\n",
      "Train Loss: 0.1533079743385315\n",
      "Train Loss: 0.15330520272254944\n",
      "Train Loss: 0.15329983830451965\n",
      "Train Loss: 0.1532949060201645\n",
      "Train Loss: 0.15328913927078247\n",
      "Train Loss: 0.15328405797481537\n",
      "Train Loss: 0.1532779186964035\n",
      "Train Loss: 0.1532721370458603\n",
      "Train Loss: 0.1532660871744156\n",
      "Train Loss: 0.15325947105884552\n",
      "Train Loss: 0.15325462818145752\n",
      "Train Loss: 0.15324817597866058\n",
      "Train Loss: 0.15324226021766663\n",
      "Train Loss: 0.15323704481124878\n",
      "Train Loss: 0.15324033796787262\n",
      "Train Loss: 0.15324629843235016\n",
      "Train Loss: 0.1532532125711441\n",
      "Train Loss: 0.15325500071048737\n",
      "Train Loss: 0.15325121581554413\n",
      "Train Loss: 0.15324686467647552\n",
      "Train Loss: 0.15324141085147858\n",
      "Train Loss: 0.15323682129383087\n",
      "Train Loss: 0.15323157608509064\n",
      "Train Loss: 0.15322645008563995\n",
      "Train Loss: 0.15322339534759521\n",
      "Train Loss: 0.15321889519691467\n",
      "Train Loss: 0.15321491658687592\n",
      "Train Loss: 0.15321098268032074\n",
      "Train Loss: 0.15320619940757751\n",
      "Train Loss: 0.1532016545534134\n",
      "Train Loss: 0.15319690108299255\n",
      "Train Loss: 0.15319181978702545\n",
      "Train Loss: 0.15318675339221954\n",
      "Train Loss: 0.15318186581134796\n",
      "Train Loss: 0.15317700803279877\n",
      "Train Loss: 0.15317192673683167\n",
      "Train Loss: 0.15316641330718994\n",
      "Train Loss: 0.1531616896390915\n",
      "Train Loss: 0.15315796434879303\n",
      "Train Loss: 0.15315379202365875\n",
      "Train Loss: 0.15315048396587372\n",
      "Train Loss: 0.15314988791942596\n",
      "Train Loss: 0.15314607322216034\n",
      "Train Loss: 0.15314333140850067\n",
      "Train Loss: 0.15313930809497833\n",
      "Train Loss: 0.1531340777873993\n",
      "Train Loss: 0.15312910079956055\n",
      "Train Loss: 0.15312375128269196\n",
      "Train Loss: 0.1531195342540741\n",
      "Train Loss: 0.1531173139810562\n",
      "Train Loss: 0.15311658382415771\n",
      "Train Loss: 0.15311464667320251\n",
      "Train Loss: 0.1531122624874115\n",
      "Train Loss: 0.15310944616794586\n",
      "Train Loss: 0.1531064510345459\n",
      "Train Loss: 0.15310320258140564\n",
      "Train Loss: 0.1530996859073639\n",
      "Train Loss: 0.153096541762352\n",
      "Train Loss: 0.15309269726276398\n",
      "Train Loss: 0.15308931469917297\n",
      "Train Loss: 0.15308542549610138\n",
      "Train Loss: 0.15308162569999695\n",
      "Train Loss: 0.15307749807834625\n",
      "Train Loss: 0.15307284891605377\n",
      "Train Loss: 0.15306834876537323\n",
      "Train Loss: 0.15306301414966583\n",
      "Train Loss: 0.1530579775571823\n",
      "Train Loss: 0.1530526727437973\n",
      "Train Loss: 0.15304690599441528\n",
      "Train Loss: 0.15304304659366608\n",
      "Train Loss: 0.15304063260555267\n",
      "Train Loss: 0.15303708612918854\n",
      "Train Loss: 0.15303345024585724\n",
      "Train Loss: 0.15302982926368713\n",
      "Train Loss: 0.1530262678861618\n",
      "Train Loss: 0.1530224084854126\n",
      "Train Loss: 0.15301837027072906\n",
      "Train Loss: 0.15301445126533508\n",
      "Train Loss: 0.15301012992858887\n",
      "Train Loss: 0.15300588309764862\n",
      "Train Loss: 0.15300151705741882\n",
      "Train Loss: 0.15299762785434723\n",
      "Train Loss: 0.15299344062805176\n",
      "Train Loss: 0.15298901498317719\n",
      "Train Loss: 0.1529846489429474\n",
      "Train Loss: 0.15298029780387878\n",
      "Train Loss: 0.15297609567642212\n",
      "Train Loss: 0.15297184884548187\n",
      "Train Loss: 0.15296758711338043\n",
      "Train Loss: 0.15296371281147003\n",
      "Train Loss: 0.15296044945716858\n",
      "Train Loss: 0.1529567390680313\n",
      "Train Loss: 0.1529526561498642\n",
      "Train Loss: 0.1529484987258911\n",
      "Train Loss: 0.1529441624879837\n",
      "Train Loss: 0.15293990075588226\n",
      "Train Loss: 0.15293554961681366\n",
      "Train Loss: 0.15293172001838684\n",
      "Train Loss: 0.15292832255363464\n",
      "Train Loss: 0.15292498469352722\n",
      "Train Loss: 0.15292100608348846\n",
      "Train Loss: 0.15291641652584076\n",
      "Train Loss: 0.15291240811347961\n",
      "Train Loss: 0.152908056974411\n",
      "Train Loss: 0.1529051959514618\n",
      "Train Loss: 0.15290285646915436\n",
      "Train Loss: 0.15290014445781708\n",
      "Train Loss: 0.15289685130119324\n",
      "Train Loss: 0.15289270877838135\n",
      "Train Loss: 0.15288878977298737\n",
      "Train Loss: 0.1528855413198471\n",
      "Train Loss: 0.15288223326206207\n",
      "Train Loss: 0.15287892520427704\n",
      "Train Loss: 0.15287499129772186\n",
      "Train Loss: 0.15287069976329803\n",
      "Train Loss: 0.1528664231300354\n",
      "Train Loss: 0.1528625339269638\n",
      "Train Loss: 0.15285834670066833\n",
      "Train Loss: 0.15285372734069824\n",
      "Train Loss: 0.1528497189283371\n",
      "Train Loss: 0.15284636616706848\n",
      "Train Loss: 0.15284521877765656\n",
      "Train Loss: 0.15284298360347748\n",
      "Train Loss: 0.15283918380737305\n",
      "Train Loss: 0.15283532440662384\n",
      "Train Loss: 0.15283165872097015\n",
      "Train Loss: 0.15282787382602692\n",
      "Train Loss: 0.1528242975473404\n",
      "Train Loss: 0.15282167494297028\n",
      "Train Loss: 0.152821883559227\n",
      "Train Loss: 0.1528213918209076\n",
      "Train Loss: 0.1528194099664688\n",
      "Train Loss: 0.15281638503074646\n",
      "Train Loss: 0.1528121680021286\n",
      "Train Loss: 0.15280842781066895\n",
      "Train Loss: 0.15280486643314362\n",
      "Train Loss: 0.1528010070323944\n",
      "Train Loss: 0.1527978926897049\n",
      "Train Loss: 0.15279419720172882\n",
      "Train Loss: 0.15279310941696167\n",
      "Train Loss: 0.15279041230678558\n",
      "Train Loss: 0.1527898758649826\n",
      "Train Loss: 0.15278689563274384\n",
      "Train Loss: 0.15278515219688416\n",
      "Train Loss: 0.1527826189994812\n",
      "Train Loss: 0.15278781950473785\n",
      "Train Loss: 0.15278764069080353\n",
      "Train Loss: 0.15278685092926025\n",
      "Train Loss: 0.15278534591197968\n",
      "Train Loss: 0.1527845412492752\n",
      "Train Loss: 0.1527826189994812\n",
      "Train Loss: 0.1527833640575409\n",
      "Train Loss: 0.15278451144695282\n",
      "Train Loss: 0.15279485285282135\n",
      "Train Loss: 0.15281368792057037\n",
      "Train Loss: 0.15283295512199402\n",
      "Train Loss: 0.15283983945846558\n",
      "Train Loss: 0.15284086763858795\n",
      "Train Loss: 0.15283985435962677\n",
      "Train Loss: 0.15284064412117004\n",
      "Train Loss: 0.15284140408039093\n",
      "Train Loss: 0.1528475135564804\n",
      "Train Loss: 0.1528489887714386\n",
      "Train Loss: 0.15284864604473114\n",
      "Train Loss: 0.1528473198413849\n",
      "Train Loss: 0.1528453230857849\n",
      "Train Loss: 0.15284307301044464\n",
      "Train Loss: 0.15284065902233124\n",
      "Train Loss: 0.15283843874931335\n",
      "Train Loss: 0.15283578634262085\n",
      "Train Loss: 0.15283328294754028\n",
      "Train Loss: 0.1528310328722\n",
      "Train Loss: 0.15282855927944183\n",
      "Train Loss: 0.152826189994812\n",
      "Train Loss: 0.1528237760066986\n",
      "Train Loss: 0.15282101929187775\n",
      "Train Loss: 0.15281832218170166\n",
      "Train Loss: 0.15281561017036438\n",
      "Train Loss: 0.15281273424625397\n",
      "Train Loss: 0.15280987322330475\n",
      "Train Loss: 0.15280689299106598\n",
      "Train Loss: 0.15280403196811676\n",
      "Train Loss: 0.15280137956142426\n",
      "Train Loss: 0.15279820561408997\n",
      "Train Loss: 0.15279510617256165\n",
      "Train Loss: 0.15279196202754974\n",
      "Train Loss: 0.15278945863246918\n",
      "Train Loss: 0.15278656780719757\n",
      "Train Loss: 0.15278348326683044\n",
      "Train Loss: 0.15278062224388123\n",
      "Train Loss: 0.1527780294418335\n",
      "Train Loss: 0.1527753621339798\n",
      "Train Loss: 0.1527729332447052\n",
      "Train Loss: 0.1527717113494873\n",
      "Train Loss: 0.15277113020420074\n",
      "Train Loss: 0.15277570486068726\n",
      "Train Loss: 0.15277627110481262\n",
      "Train Loss: 0.15277476608753204\n",
      "Train Loss: 0.15277250111103058\n",
      "Train Loss: 0.1527698040008545\n",
      "Train Loss: 0.15276697278022766\n",
      "Train Loss: 0.15276417136192322\n",
      "Train Loss: 0.15276139974594116\n",
      "Train Loss: 0.15275882184505463\n",
      "Train Loss: 0.15275627374649048\n",
      "Train Loss: 0.15275374054908752\n",
      "Train Loss: 0.15275107324123383\n",
      "Train Loss: 0.15274815261363983\n",
      "Train Loss: 0.1527450978755951\n",
      "Train Loss: 0.15274226665496826\n",
      "Train Loss: 0.15273955464363098\n",
      "Train Loss: 0.15273676812648773\n",
      "Train Loss: 0.15273408591747284\n",
      "Train Loss: 0.1527312695980072\n",
      "Train Loss: 0.15272845327854156\n",
      "Train Loss: 0.15272541344165802\n",
      "Train Loss: 0.1527225375175476\n",
      "Train Loss: 0.1527194380760193\n",
      "Train Loss: 0.1527162492275238\n",
      "Train Loss: 0.15271317958831787\n",
      "Train Loss: 0.15271008014678955\n",
      "Train Loss: 0.15270692110061646\n",
      "Train Loss: 0.1527036428451538\n",
      "Train Loss: 0.15270060300827026\n",
      "Train Loss: 0.15269756317138672\n",
      "Train Loss: 0.1526944935321808\n",
      "Train Loss: 0.15269137918949127\n",
      "Train Loss: 0.1526884138584137\n",
      "Train Loss: 0.15268531441688538\n",
      "Train Loss: 0.15268203616142273\n",
      "Train Loss: 0.15267959237098694\n",
      "Train Loss: 0.15267707407474518\n",
      "Train Loss: 0.15267427265644073\n",
      "Train Loss: 0.1526717245578766\n",
      "Train Loss: 0.1526690125465393\n",
      "Train Loss: 0.1526661217212677\n",
      "Train Loss: 0.1526634246110916\n",
      "Train Loss: 0.1526605635881424\n",
      "Train Loss: 0.15265747904777527\n",
      "Train Loss: 0.15265467762947083\n",
      "Train Loss: 0.15265169739723206\n",
      "Train Loss: 0.1526491492986679\n",
      "Train Loss: 0.15264639258384705\n",
      "Train Loss: 0.15264374017715454\n",
      "Train Loss: 0.1526414006948471\n",
      "Train Loss: 0.15263895690441132\n",
      "Train Loss: 0.15263620018959045\n",
      "Train Loss: 0.15263359248638153\n",
      "Train Loss: 0.15263117849826813\n",
      "Train Loss: 0.15262873470783234\n",
      "Train Loss: 0.1526261419057846\n",
      "Train Loss: 0.15262380242347717\n",
      "Train Loss: 0.15262259542942047\n",
      "Train Loss: 0.15262171626091003\n",
      "Train Loss: 0.15262211859226227\n",
      "Train Loss: 0.15262438356876373\n",
      "Train Loss: 0.15262819826602936\n",
      "Train Loss: 0.15263088047504425\n",
      "Train Loss: 0.15263213217258453\n",
      "Train Loss: 0.15263204276561737\n",
      "Train Loss: 0.15263023972511292\n",
      "Train Loss: 0.15262901782989502\n",
      "Train Loss: 0.15262749791145325\n",
      "Train Loss: 0.152626171708107\n",
      "Train Loss: 0.15262486040592194\n",
      "Train Loss: 0.15262538194656372\n",
      "Train Loss: 0.15262532234191895\n",
      "Train Loss: 0.15262475609779358\n",
      "Train Loss: 0.15262360870838165\n",
      "Train Loss: 0.15262237191200256\n",
      "Train Loss: 0.15262067317962646\n",
      "Train Loss: 0.15261885523796082\n",
      "Train Loss: 0.15261676907539368\n",
      "Train Loss: 0.15261495113372803\n",
      "Train Loss: 0.15261369943618774\n",
      "Train Loss: 0.15261317789554596\n",
      "Train Loss: 0.15261384844779968\n",
      "Train Loss: 0.15261562168598175\n",
      "Train Loss: 0.15261615812778473\n",
      "Train Loss: 0.1526155322790146\n",
      "Train Loss: 0.15261498093605042\n",
      "Train Loss: 0.15261442959308624\n",
      "Train Loss: 0.15261386334896088\n",
      "Train Loss: 0.15261302888393402\n",
      "Train Loss: 0.15261197090148926\n",
      "Train Loss: 0.15261045098304749\n",
      "Train Loss: 0.15260887145996094\n",
      "Train Loss: 0.15260696411132812\n",
      "Train Loss: 0.15260516107082367\n",
      "Train Loss: 0.15260349214076996\n",
      "Train Loss: 0.1526017189025879\n",
      "Train Loss: 0.1525999754667282\n",
      "Train Loss: 0.15259814262390137\n",
      "Train Loss: 0.15259642899036407\n",
      "Train Loss: 0.1525951623916626\n",
      "Train Loss: 0.15259477496147156\n",
      "Train Loss: 0.1525941640138626\n",
      "Train Loss: 0.15259329974651337\n",
      "Train Loss: 0.1525919884443283\n",
      "Train Loss: 0.15258994698524475\n",
      "Train Loss: 0.15258748829364777\n",
      "Train Loss: 0.15258517861366272\n",
      "Train Loss: 0.15258273482322693\n",
      "Train Loss: 0.15258033573627472\n",
      "Train Loss: 0.15257799625396729\n",
      "Train Loss: 0.15257589519023895\n",
      "Train Loss: 0.15257440507411957\n",
      "Train Loss: 0.1525730937719345\n",
      "Train Loss: 0.1525713950395584\n",
      "Train Loss: 0.15256981551647186\n",
      "Train Loss: 0.15256789326667786\n",
      "Train Loss: 0.15256595611572266\n",
      "Train Loss: 0.15256357192993164\n",
      "Train Loss: 0.1525612324476242\n",
      "Train Loss: 0.15255896747112274\n",
      "Train Loss: 0.15255682170391083\n",
      "Train Loss: 0.1525547057390213\n",
      "Train Loss: 0.1525525599718094\n",
      "Train Loss: 0.1525508314371109\n",
      "Train Loss: 0.15254905819892883\n",
      "Train Loss: 0.15254728496074677\n",
      "Train Loss: 0.1525457799434662\n",
      "Train Loss: 0.15254442393779755\n",
      "Train Loss: 0.15254294872283936\n",
      "Train Loss: 0.15254107117652893\n",
      "Train Loss: 0.1525392383337021\n",
      "Train Loss: 0.15253715217113495\n",
      "Train Loss: 0.15253478288650513\n",
      "Train Loss: 0.15253235399723053\n",
      "Train Loss: 0.1525302231311798\n",
      "Train Loss: 0.15252794325351715\n",
      "Train Loss: 0.15252584218978882\n",
      "Train Loss: 0.15252414345741272\n",
      "Train Loss: 0.15252208709716797\n",
      "Train Loss: 0.15252013504505157\n",
      "Train Loss: 0.15251877903938293\n",
      "Train Loss: 0.1525176763534546\n",
      "Train Loss: 0.15251660346984863\n",
      "Train Loss: 0.15251576900482178\n",
      "Train Loss: 0.15251460671424866\n",
      "Train Loss: 0.1525133103132248\n",
      "Train Loss: 0.152511328458786\n",
      "Train Loss: 0.15250933170318604\n",
      "Train Loss: 0.15250717103481293\n",
      "Train Loss: 0.15250492095947266\n",
      "Train Loss: 0.15250255167484283\n",
      "Train Loss: 0.15250037610530853\n",
      "Train Loss: 0.15249785780906677\n",
      "Train Loss: 0.15249553322792053\n",
      "Train Loss: 0.15249301493167877\n",
      "Train Loss: 0.15249136090278625\n",
      "Train Loss: 0.15249089896678925\n",
      "Train Loss: 0.15248984098434448\n",
      "Train Loss: 0.15248824656009674\n",
      "Train Loss: 0.15248598158359528\n",
      "Train Loss: 0.15248385071754456\n",
      "Train Loss: 0.15248188376426697\n",
      "Train Loss: 0.1524803340435028\n",
      "Train Loss: 0.15247899293899536\n",
      "Train Loss: 0.15247756242752075\n",
      "Train Loss: 0.1524766981601715\n",
      "Train Loss: 0.15247508883476257\n",
      "Train Loss: 0.15247297286987305\n",
      "Train Loss: 0.1524709016084671\n",
      "Train Loss: 0.15246893465518951\n",
      "Train Loss: 0.1524667888879776\n",
      "Train Loss: 0.15246477723121643\n",
      "Train Loss: 0.15246227383613586\n",
      "Train Loss: 0.152459979057312\n",
      "Train Loss: 0.15245771408081055\n",
      "Train Loss: 0.15245553851127625\n",
      "Train Loss: 0.1524536907672882\n",
      "Train Loss: 0.15245287120342255\n",
      "Train Loss: 0.15245114266872406\n",
      "Train Loss: 0.15244914591312408\n",
      "Train Loss: 0.15244723856449127\n",
      "Train Loss: 0.15244491398334503\n",
      "Train Loss: 0.15244251489639282\n",
      "Train Loss: 0.1524416208267212\n",
      "Train Loss: 0.15244095027446747\n",
      "Train Loss: 0.15243984758853912\n",
      "Train Loss: 0.15243908762931824\n",
      "Train Loss: 0.15243875980377197\n",
      "Train Loss: 0.15243886411190033\n",
      "Train Loss: 0.15243837237358093\n",
      "Train Loss: 0.15243712067604065\n",
      "Train Loss: 0.15243615210056305\n",
      "Train Loss: 0.15243647992610931\n",
      "Train Loss: 0.1524355113506317\n",
      "Train Loss: 0.15243370831012726\n",
      "Train Loss: 0.1524316370487213\n",
      "Train Loss: 0.1524294912815094\n",
      "Train Loss: 0.15242736041545868\n",
      "Train Loss: 0.15242497622966766\n",
      "Train Loss: 0.15242303907871246\n",
      "Train Loss: 0.15242142975330353\n",
      "Train Loss: 0.15241996943950653\n",
      "Train Loss: 0.15241868793964386\n",
      "Train Loss: 0.1524171233177185\n",
      "Train Loss: 0.15241557359695435\n",
      "Train Loss: 0.15241484344005585\n",
      "Train Loss: 0.15241380035877228\n",
      "Train Loss: 0.1524123102426529\n",
      "Train Loss: 0.15241126716136932\n",
      "Train Loss: 0.15241001546382904\n",
      "Train Loss: 0.15240825712680817\n",
      "Train Loss: 0.15240712463855743\n",
      "Train Loss: 0.15240535140037537\n",
      "Train Loss: 0.15240323543548584\n",
      "Train Loss: 0.1524008959531784\n",
      "Train Loss: 0.15239855647087097\n",
      "Train Loss: 0.15239621698856354\n",
      "Train Loss: 0.15239396691322327\n",
      "Train Loss: 0.1523914486169815\n",
      "Train Loss: 0.152389258146286\n",
      "Train Loss: 0.1523870825767517\n",
      "Train Loss: 0.15238532423973083\n",
      "Train Loss: 0.15238435566425323\n",
      "Train Loss: 0.15238353610038757\n",
      "Train Loss: 0.15238238871097565\n",
      "Train Loss: 0.1523808091878891\n",
      "Train Loss: 0.15237939357757568\n",
      "Train Loss: 0.15237818658351898\n",
      "Train Loss: 0.1523774415254593\n",
      "Train Loss: 0.15237636864185333\n",
      "Train Loss: 0.15237486362457275\n",
      "Train Loss: 0.15237361192703247\n",
      "Train Loss: 0.15237250924110413\n",
      "Train Loss: 0.15237094461917877\n",
      "Train Loss: 0.1523694396018982\n",
      "Train Loss: 0.15236864984035492\n",
      "Train Loss: 0.15236777067184448\n",
      "Train Loss: 0.15236644446849823\n",
      "Train Loss: 0.1523652821779251\n",
      "Train Loss: 0.15236467123031616\n",
      "Train Loss: 0.1523636281490326\n",
      "Train Loss: 0.15236273407936096\n",
      "Train Loss: 0.1523614078760147\n",
      "Train Loss: 0.15235985815525055\n",
      "Train Loss: 0.1523580104112625\n",
      "Train Loss: 0.15235653519630432\n",
      "Train Loss: 0.1523553431034088\n",
      "Train Loss: 0.15235404670238495\n",
      "Train Loss: 0.15235216915607452\n",
      "Train Loss: 0.15234977006912231\n",
      "Train Loss: 0.1523476392030716\n",
      "Train Loss: 0.15234558284282684\n",
      "Train Loss: 0.1523435264825821\n",
      "Train Loss: 0.15234100818634033\n",
      "Train Loss: 0.1523386538028717\n",
      "Train Loss: 0.1523364931344986\n",
      "Train Loss: 0.15233519673347473\n",
      "Train Loss: 0.15233327448368073\n",
      "Train Loss: 0.15233150124549866\n",
      "Train Loss: 0.15232974290847778\n",
      "Train Loss: 0.15232817828655243\n",
      "Train Loss: 0.1523268073797226\n",
      "Train Loss: 0.1523253172636032\n",
      "Train Loss: 0.15232431888580322\n",
      "Train Loss: 0.1523231863975525\n",
      "Train Loss: 0.1523217111825943\n",
      "Train Loss: 0.15232011675834656\n",
      "Train Loss: 0.15231844782829285\n",
      "Train Loss: 0.15231704711914062\n",
      "Train Loss: 0.15231595933437347\n",
      "Train Loss: 0.1523154377937317\n",
      "Train Loss: 0.15231437981128693\n",
      "Train Loss: 0.15231266617774963\n",
      "Train Loss: 0.15231020748615265\n",
      "Train Loss: 0.15230806171894073\n",
      "Train Loss: 0.15230591595172882\n",
      "Train Loss: 0.15230393409729004\n",
      "Train Loss: 0.15230225026607513\n",
      "Train Loss: 0.15230073034763336\n",
      "Train Loss: 0.15229904651641846\n",
      "Train Loss: 0.1522980034351349\n",
      "Train Loss: 0.15229681134223938\n",
      "Train Loss: 0.15229494869709015\n",
      "Train Loss: 0.15229301154613495\n",
      "Train Loss: 0.1522911638021469\n",
      "Train Loss: 0.15228892862796783\n",
      "Train Loss: 0.15228646993637085\n",
      "Train Loss: 0.15228398144245148\n",
      "Train Loss: 0.15228210389614105\n",
      "Train Loss: 0.15228022634983063\n",
      "Train Loss: 0.1522781103849411\n",
      "Train Loss: 0.15227572619915009\n",
      "Train Loss: 0.15227395296096802\n",
      "Train Loss: 0.15227165818214417\n",
      "Train Loss: 0.15226924419403076\n",
      "Train Loss: 0.15226691961288452\n",
      "Train Loss: 0.15226413309574127\n",
      "Train Loss: 0.1522616147994995\n",
      "Train Loss: 0.15225987136363983\n",
      "Train Loss: 0.15225835144519806\n",
      "Train Loss: 0.15225663781166077\n",
      "Train Loss: 0.15225496888160706\n",
      "Train Loss: 0.15225325524806976\n",
      "Train Loss: 0.15225151181221008\n",
      "Train Loss: 0.15224972367286682\n",
      "Train Loss: 0.15224796533584595\n",
      "Train Loss: 0.15224623680114746\n",
      "Train Loss: 0.15224455296993256\n",
      "Train Loss: 0.1522430032491684\n",
      "Train Loss: 0.1522413194179535\n",
      "Train Loss: 0.1522398591041565\n",
      "Train Loss: 0.15223819017410278\n",
      "Train Loss: 0.152237206697464\n",
      "Train Loss: 0.1522367000579834\n",
      "Train Loss: 0.1522364765405655\n",
      "Train Loss: 0.15223586559295654\n",
      "Train Loss: 0.15223507583141327\n",
      "Train Loss: 0.15223382413387299\n",
      "Train Loss: 0.15223295986652374\n",
      "Train Loss: 0.15223194658756256\n",
      "Train Loss: 0.15223099291324615\n",
      "Train Loss: 0.15223248302936554\n",
      "Train Loss: 0.15223371982574463\n",
      "Train Loss: 0.1522335559129715\n",
      "Train Loss: 0.1522326022386551\n",
      "Train Loss: 0.1522313803434372\n",
      "Train Loss: 0.15223009884357452\n",
      "Train Loss: 0.15222878754138947\n",
      "Train Loss: 0.15222792327404022\n",
      "Train Loss: 0.1522277593612671\n",
      "Train Loss: 0.15222793817520142\n",
      "Train Loss: 0.15222693979740143\n",
      "Train Loss: 0.15222583711147308\n",
      "Train Loss: 0.15222465991973877\n",
      "Train Loss: 0.15222392976284027\n",
      "Train Loss: 0.1522241234779358\n",
      "Train Loss: 0.1522240787744522\n",
      "Train Loss: 0.15222351253032684\n",
      "Train Loss: 0.15222221612930298\n",
      "Train Loss: 0.15222036838531494\n",
      "Train Loss: 0.15221908688545227\n",
      "Train Loss: 0.15221814811229706\n",
      "Train Loss: 0.15221725404262543\n",
      "Train Loss: 0.1522168070077896\n",
      "Train Loss: 0.15221650898456573\n",
      "Train Loss: 0.1522166132926941\n",
      "Train Loss: 0.1522168666124344\n",
      "Train Loss: 0.15221814811229706\n",
      "Train Loss: 0.15221962332725525\n",
      "Train Loss: 0.15222112834453583\n",
      "Train Loss: 0.15222178399562836\n",
      "Train Loss: 0.15222162008285522\n",
      "Train Loss: 0.1522209793329239\n",
      "Train Loss: 0.1522195041179657\n",
      "Train Loss: 0.15221737325191498\n",
      "Train Loss: 0.1522154062986374\n",
      "Train Loss: 0.15221336483955383\n",
      "Train Loss: 0.15221136808395386\n",
      "Train Loss: 0.15220928192138672\n",
      "Train Loss: 0.1522074043750763\n",
      "Train Loss: 0.15220539271831512\n",
      "Train Loss: 0.15220363438129425\n",
      "Train Loss: 0.15220201015472412\n",
      "Train Loss: 0.15220025181770325\n",
      "Train Loss: 0.15219847857952118\n",
      "Train Loss: 0.1521969586610794\n",
      "Train Loss: 0.1521957814693451\n",
      "Train Loss: 0.1521943360567093\n",
      "Train Loss: 0.15219293534755707\n",
      "Train Loss: 0.15219180285930634\n",
      "Train Loss: 0.15219153463840485\n",
      "Train Loss: 0.15219135582447052\n",
      "Train Loss: 0.15219223499298096\n",
      "Train Loss: 0.15219219028949738\n",
      "Train Loss: 0.15219159424304962\n",
      "Train Loss: 0.1521909087896347\n",
      "Train Loss: 0.15219014883041382\n",
      "Train Loss: 0.15218976140022278\n",
      "Train Loss: 0.15218967199325562\n",
      "Train Loss: 0.15218983590602875\n",
      "Train Loss: 0.15219037234783173\n",
      "Train Loss: 0.1521916538476944\n",
      "Train Loss: 0.15219295024871826\n",
      "Train Loss: 0.15219536423683167\n",
      "Train Loss: 0.15219713747501373\n",
      "Train Loss: 0.1521991491317749\n",
      "Train Loss: 0.15220113098621368\n",
      "Train Loss: 0.1522020548582077\n",
      "Train Loss: 0.1522030234336853\n",
      "Train Loss: 0.15220315754413605\n",
      "Train Loss: 0.1522028148174286\n",
      "Train Loss: 0.15220212936401367\n",
      "Train Loss: 0.15220095217227936\n",
      "Train Loss: 0.15219956636428833\n",
      "Train Loss: 0.15219804644584656\n",
      "Train Loss: 0.15219654142856598\n",
      "Train Loss: 0.1521950513124466\n",
      "Train Loss: 0.15219348669052124\n",
      "Train Loss: 0.1521919220685959\n",
      "Train Loss: 0.15219023823738098\n",
      "Train Loss: 0.15218883752822876\n",
      "Train Loss: 0.15218733251094818\n",
      "Train Loss: 0.15218637883663177\n",
      "Train Loss: 0.15218569338321686\n",
      "Train Loss: 0.15218570828437805\n",
      "Train Loss: 0.1521858423948288\n",
      "Train Loss: 0.15218588709831238\n",
      "Train Loss: 0.15218548476696014\n",
      "Train Loss: 0.15218500792980194\n",
      "Train Loss: 0.15218430757522583\n",
      "Train Loss: 0.15218402445316315\n",
      "Train Loss: 0.15218305587768555\n",
      "Train Loss: 0.15218277275562286\n",
      "Train Loss: 0.1521827131509781\n",
      "Train Loss: 0.15218281745910645\n",
      "Train Loss: 0.15218374133110046\n",
      "Train Loss: 0.15218380093574524\n",
      "Train Loss: 0.15218289196491241\n",
      "Train Loss: 0.15218177437782288\n",
      "Train Loss: 0.15218119323253632\n",
      "Train Loss: 0.15218037366867065\n",
      "Train Loss: 0.1521792858839035\n",
      "Train Loss: 0.15217885375022888\n",
      "Train Loss: 0.1521778404712677\n",
      "Train Loss: 0.15217643976211548\n",
      "Train Loss: 0.15217480063438416\n",
      "Train Loss: 0.15217268466949463\n",
      "Train Loss: 0.15217100083827972\n",
      "Train Loss: 0.1521690934896469\n",
      "Train Loss: 0.15216784179210663\n",
      "Train Loss: 0.15216664969921112\n",
      "Train Loss: 0.15216606855392456\n",
      "Train Loss: 0.15216541290283203\n",
      "Train Loss: 0.15216432511806488\n",
      "Train Loss: 0.152164027094841\n",
      "Train Loss: 0.15216414630413055\n",
      "Train Loss: 0.1521649807691574\n",
      "Train Loss: 0.15216557681560516\n",
      "Train Loss: 0.15216568112373352\n",
      "Train Loss: 0.15216562151908875\n",
      "Train Loss: 0.15216580033302307\n",
      "Train Loss: 0.15216617286205292\n",
      "Train Loss: 0.1521664559841156\n",
      "Train Loss: 0.1521664559841156\n",
      "Train Loss: 0.15216650068759918\n",
      "Train Loss: 0.15216626226902008\n",
      "Train Loss: 0.15216577053070068\n",
      "Train Loss: 0.1521650105714798\n",
      "Train Loss: 0.1521637886762619\n",
      "Train Loss: 0.15216216444969177\n",
      "Train Loss: 0.15216106176376343\n",
      "Train Loss: 0.15215986967086792\n",
      "Train Loss: 0.15215854346752167\n",
      "Train Loss: 0.15215714275836945\n",
      "Train Loss: 0.15215569734573364\n",
      "Train Loss: 0.15215469896793365\n",
      "Train Loss: 0.152154341340065\n",
      "Train Loss: 0.1521531641483307\n",
      "Train Loss: 0.1521526277065277\n",
      "Train Loss: 0.1521531194448471\n",
      "Train Loss: 0.15215405821800232\n",
      "Train Loss: 0.15215443074703217\n",
      "Train Loss: 0.15215443074703217\n",
      "Train Loss: 0.1521541029214859\n",
      "Train Loss: 0.15215450525283813\n",
      "Train Loss: 0.15215469896793365\n",
      "Train Loss: 0.15215584635734558\n",
      "Train Loss: 0.15215741097927094\n",
      "Train Loss: 0.15215693414211273\n",
      "Train Loss: 0.15215598046779633\n",
      "Train Loss: 0.15215452015399933\n",
      "Train Loss: 0.15215325355529785\n",
      "Train Loss: 0.15215222537517548\n",
      "Train Loss: 0.15215133130550385\n",
      "Train Loss: 0.15215052664279938\n",
      "Train Loss: 0.152149960398674\n",
      "Train Loss: 0.15214945375919342\n",
      "Train Loss: 0.15214890241622925\n",
      "Train Loss: 0.15214848518371582\n",
      "Train Loss: 0.15214818716049194\n",
      "Train Loss: 0.15214750170707703\n",
      "Train Loss: 0.15214648842811584\n",
      "Train Loss: 0.15214619040489197\n",
      "Train Loss: 0.15214623510837555\n",
      "Train Loss: 0.15214717388153076\n",
      "Train Loss: 0.15214698016643524\n",
      "Train Loss: 0.15214669704437256\n",
      "Train Loss: 0.1521470844745636\n",
      "Train Loss: 0.15214818716049194\n",
      "Train Loss: 0.15214881300926208\n",
      "Train Loss: 0.15214885771274567\n",
      "Train Loss: 0.1521485149860382\n",
      "Train Loss: 0.1521480232477188\n",
      "Train Loss: 0.15214769542217255\n",
      "Train Loss: 0.15214741230010986\n",
      "Train Loss: 0.15214727818965912\n",
      "Train Loss: 0.15214690566062927\n",
      "Train Loss: 0.15214650332927704\n",
      "Train Loss: 0.15214623510837555\n",
      "Train Loss: 0.15214607119560242\n",
      "Train Loss: 0.15214622020721436\n",
      "Train Loss: 0.15214641392230988\n",
      "Train Loss: 0.152146577835083\n",
      "Train Loss: 0.15214680135250092\n",
      "Train Loss: 0.1521468311548233\n",
      "Train Loss: 0.15214715898036957\n",
      "Train Loss: 0.15214717388153076\n",
      "Train Loss: 0.1521470844745636\n",
      "Train Loss: 0.15214699506759644\n",
      "Train Loss: 0.15214700996875763\n",
      "Train Loss: 0.15214644372463226\n",
      "Train Loss: 0.15214593708515167\n",
      "Train Loss: 0.15214526653289795\n",
      "Train Loss: 0.1521444171667099\n",
      "Train Loss: 0.15214382112026215\n",
      "Train Loss: 0.1521434783935547\n",
      "Train Loss: 0.15214350819587708\n",
      "Train Loss: 0.15214325487613678\n",
      "Train Loss: 0.152143195271492\n",
      "Train Loss: 0.15214279294013977\n",
      "Train Loss: 0.15214213728904724\n",
      "Train Loss: 0.15214136242866516\n",
      "Train Loss: 0.15214070677757263\n",
      "Train Loss: 0.1521397978067398\n",
      "Train Loss: 0.1521388739347458\n",
      "Train Loss: 0.15213823318481445\n",
      "Train Loss: 0.1521373689174652\n",
      "Train Loss: 0.15213662385940552\n",
      "Train Loss: 0.1521356701850891\n",
      "Train Loss: 0.1521347463130951\n",
      "Train Loss: 0.15213383734226227\n",
      "Train Loss: 0.15213249623775482\n",
      "Train Loss: 0.1521315723657608\n",
      "Train Loss: 0.152130588889122\n",
      "Train Loss: 0.15212999284267426\n",
      "Train Loss: 0.15212911367416382\n",
      "Train Loss: 0.15212838351726532\n",
      "Train Loss: 0.15212790668010712\n",
      "Train Loss: 0.15212741494178772\n",
      "Train Loss: 0.15212702751159668\n",
      "Train Loss: 0.1521265208721161\n",
      "Train Loss: 0.15212582051753998\n",
      "Train Loss: 0.15212495625019073\n",
      "Train Loss: 0.15212400257587433\n",
      "Train Loss: 0.15212252736091614\n",
      "Train Loss: 0.15212184190750122\n",
      "Train Loss: 0.15212148427963257\n",
      "Train Loss: 0.15212088823318481\n",
      "Train Loss: 0.15212014317512512\n",
      "Train Loss: 0.15211936831474304\n",
      "Train Loss: 0.1521185040473938\n",
      "Train Loss: 0.1521175652742386\n",
      "Train Loss: 0.15211674571037292\n",
      "Train Loss: 0.15211638808250427\n",
      "Train Loss: 0.1521160900592804\n",
      "Train Loss: 0.1521158665418625\n",
      "Train Loss: 0.15211597084999084\n",
      "Train Loss: 0.15211622416973114\n",
      "Train Loss: 0.1521165817975998\n",
      "Train Loss: 0.1521163433790207\n",
      "Train Loss: 0.1521158665418625\n",
      "Train Loss: 0.1521148383617401\n",
      "Train Loss: 0.15211360156536102\n",
      "Train Loss: 0.1521121859550476\n",
      "Train Loss: 0.1521109938621521\n",
      "Train Loss: 0.15210969746112823\n",
      "Train Loss: 0.15210892260074615\n",
      "Train Loss: 0.15210795402526855\n",
      "Train Loss: 0.15210764110088348\n",
      "Train Loss: 0.1521073579788208\n",
      "Train Loss: 0.15210667252540588\n",
      "Train Loss: 0.15210624039173126\n",
      "Train Loss: 0.15210600197315216\n",
      "Train Loss: 0.15210595726966858\n",
      "Train Loss: 0.15210600197315216\n",
      "Train Loss: 0.1521061509847641\n",
      "Train Loss: 0.1521073579788208\n",
      "Train Loss: 0.1521088182926178\n",
      "Train Loss: 0.1521092653274536\n",
      "Train Loss: 0.15211030840873718\n",
      "Train Loss: 0.15210984647274017\n",
      "Train Loss: 0.15210898220539093\n",
      "Train Loss: 0.15210819244384766\n",
      "Train Loss: 0.15210725367069244\n",
      "Train Loss: 0.15210632979869843\n",
      "Train Loss: 0.1521061807870865\n",
      "Train Loss: 0.1521058827638626\n",
      "Train Loss: 0.1521054208278656\n",
      "Train Loss: 0.15210586786270142\n",
      "Train Loss: 0.15210658311843872\n",
      "Train Loss: 0.1521073877811432\n",
      "Train Loss: 0.1521078497171402\n",
      "Train Loss: 0.15210746228694916\n",
      "Train Loss: 0.15210673213005066\n",
      "Train Loss: 0.15210646390914917\n",
      "Train Loss: 0.15210629999637604\n",
      "Train Loss: 0.15210607647895813\n",
      "Train Loss: 0.1521058827638626\n",
      "Train Loss: 0.1521056741476059\n",
      "Train Loss: 0.15210580825805664\n",
      "Train Loss: 0.15210562944412231\n",
      "Train Loss: 0.15210478007793427\n",
      "Train Loss: 0.15210384130477905\n",
      "Train Loss: 0.15210311114788055\n",
      "Train Loss: 0.15210263431072235\n",
      "Train Loss: 0.15210235118865967\n",
      "Train Loss: 0.15210214257240295\n",
      "Train Loss: 0.1521020382642746\n",
      "Train Loss: 0.15210218727588654\n",
      "Train Loss: 0.15210199356079102\n",
      "Train Loss: 0.15210115909576416\n",
      "Train Loss: 0.15210013091564178\n",
      "Train Loss: 0.15209931135177612\n",
      "Train Loss: 0.15209868550300598\n",
      "Train Loss: 0.1520983725786209\n",
      "Train Loss: 0.15209825336933136\n",
      "Train Loss: 0.15209807455539703\n",
      "Train Loss: 0.15209779143333435\n",
      "Train Loss: 0.15209735929965973\n",
      "Train Loss: 0.1520964801311493\n",
      "Train Loss: 0.15209540724754333\n",
      "Train Loss: 0.15209439396858215\n",
      "Train Loss: 0.15209358930587769\n",
      "Train Loss: 0.1520928293466568\n",
      "Train Loss: 0.1520923674106598\n",
      "Train Loss: 0.152091845870018\n",
      "Train Loss: 0.15209132432937622\n",
      "Train Loss: 0.15209117531776428\n",
      "Train Loss: 0.15209126472473145\n",
      "Train Loss: 0.1520913541316986\n",
      "Train Loss: 0.15209126472473145\n",
      "Train Loss: 0.152091383934021\n",
      "Train Loss: 0.1520913988351822\n",
      "Train Loss: 0.15209098160266876\n",
      "Train Loss: 0.15209037065505981\n",
      "Train Loss: 0.15208962559700012\n",
      "Train Loss: 0.152088925242424\n",
      "Train Loss: 0.15208789706230164\n",
      "Train Loss: 0.15208689868450165\n",
      "Train Loss: 0.15208590030670166\n",
      "Train Loss: 0.15208522975444794\n",
      "Train Loss: 0.15208466351032257\n",
      "Train Loss: 0.15208402276039124\n",
      "Train Loss: 0.1520833522081375\n",
      "Train Loss: 0.15208278596401215\n",
      "Train Loss: 0.15208251774311066\n",
      "Train Loss: 0.15208223462104797\n",
      "Train Loss: 0.15208196640014648\n",
      "Train Loss: 0.15208187699317932\n",
      "Train Loss: 0.15208196640014648\n",
      "Train Loss: 0.15208186209201813\n",
      "Train Loss: 0.1520814299583435\n",
      "Train Loss: 0.15208099782466888\n",
      "Train Loss: 0.15208029747009277\n",
      "Train Loss: 0.15207916498184204\n",
      "Train Loss: 0.15207792818546295\n",
      "Train Loss: 0.15207704901695251\n",
      "Train Loss: 0.15207579731941223\n",
      "Train Loss: 0.15207453072071075\n",
      "Train Loss: 0.15207333862781525\n",
      "Train Loss: 0.15207262337207794\n",
      "Train Loss: 0.15207217633724213\n",
      "Train Loss: 0.1520712971687317\n",
      "Train Loss: 0.1520703136920929\n",
      "Train Loss: 0.15206970274448395\n",
      "Train Loss: 0.15206904709339142\n",
      "Train Loss: 0.15206851065158844\n",
      "Train Loss: 0.15206754207611084\n",
      "Train Loss: 0.1520663946866989\n",
      "Train Loss: 0.15206527709960938\n",
      "Train Loss: 0.1520649790763855\n",
      "Train Loss: 0.15206414461135864\n",
      "Train Loss: 0.15206335484981537\n",
      "Train Loss: 0.15206246078014374\n",
      "Train Loss: 0.15206198394298553\n",
      "Train Loss: 0.15206144750118256\n",
      "Train Loss: 0.1520608812570572\n",
      "Train Loss: 0.1520613729953766\n",
      "Train Loss: 0.15206171572208405\n",
      "Train Loss: 0.15206164121627808\n",
      "Train Loss: 0.15206149220466614\n",
      "Train Loss: 0.15206094086170197\n",
      "Train Loss: 0.1520601212978363\n",
      "Train Loss: 0.15205958485603333\n",
      "Train Loss: 0.15205895900726318\n",
      "Train Loss: 0.15205974876880646\n",
      "Train Loss: 0.15206052362918854\n",
      "Train Loss: 0.15206049382686615\n",
      "Train Loss: 0.1520603448152542\n",
      "Train Loss: 0.15206007659435272\n",
      "Train Loss: 0.15205974876880646\n",
      "Train Loss: 0.1520591676235199\n",
      "Train Loss: 0.1520584672689438\n",
      "Train Loss: 0.1520577371120453\n",
      "Train Loss: 0.15205703675746918\n",
      "Train Loss: 0.1520615667104721\n",
      "Train Loss: 0.15206444263458252\n",
      "Train Loss: 0.15206600725650787\n",
      "Train Loss: 0.1520673632621765\n",
      "Train Loss: 0.1520693451166153\n",
      "Train Loss: 0.15207144618034363\n",
      "Train Loss: 0.15207353234291077\n",
      "Train Loss: 0.1520753651857376\n",
      "Train Loss: 0.15207621455192566\n",
      "Train Loss: 0.15207631886005402\n",
      "Train Loss: 0.1520763635635376\n",
      "Train Loss: 0.1520761102437973\n",
      "Train Loss: 0.1520761251449585\n",
      "Train Loss: 0.1520763635635376\n",
      "Train Loss: 0.1520761102437973\n",
      "Train Loss: 0.1520758718252182\n",
      "Train Loss: 0.15207558870315552\n",
      "Train Loss: 0.15207570791244507\n",
      "Train Loss: 0.15207669138908386\n",
      "Train Loss: 0.1520780324935913\n",
      "Train Loss: 0.15207894146442413\n",
      "Train Loss: 0.15207920968532562\n",
      "Train Loss: 0.15207885205745697\n",
      "Train Loss: 0.1520782709121704\n",
      "Train Loss: 0.15207792818546295\n",
      "Train Loss: 0.15207752585411072\n",
      "Train Loss: 0.1520773023366928\n",
      "Train Loss: 0.15207736194133759\n",
      "Train Loss: 0.15207774937152863\n",
      "Train Loss: 0.15207788348197937\n",
      "Train Loss: 0.15207770466804504\n",
      "Train Loss: 0.15207785367965698\n",
      "Train Loss: 0.15207809209823608\n",
      "Train Loss: 0.15207867324352264\n",
      "Train Loss: 0.15207920968532562\n",
      "Train Loss: 0.15207962691783905\n",
      "Train Loss: 0.15207979083061218\n",
      "Train Loss: 0.15208004415035248\n",
      "Train Loss: 0.15208059549331665\n",
      "Train Loss: 0.15208059549331665\n",
      "Train Loss: 0.15208052098751068\n",
      "Train Loss: 0.15208038687705994\n",
      "Train Loss: 0.1520802527666092\n",
      "Train Loss: 0.15207991003990173\n",
      "Train Loss: 0.15207967162132263\n",
      "Train Loss: 0.1520794779062271\n",
      "Train Loss: 0.152079239487648\n",
      "Train Loss: 0.15207897126674652\n",
      "Train Loss: 0.15207873284816742\n",
      "Train Loss: 0.1520785540342331\n",
      "Train Loss: 0.15207834541797638\n",
      "Train Loss: 0.1520780622959137\n",
      "Train Loss: 0.15207776427268982\n",
      "Train Loss: 0.15207743644714355\n",
      "Train Loss: 0.15207715332508087\n",
      "Train Loss: 0.15207694470882416\n",
      "Train Loss: 0.15207670629024506\n",
      "Train Loss: 0.15207649767398834\n",
      "Train Loss: 0.15207625925540924\n",
      "Train Loss: 0.15207596123218536\n",
      "Train Loss: 0.15207551419734955\n",
      "Train Loss: 0.15207494795322418\n",
      "Train Loss: 0.15207448601722717\n",
      "Train Loss: 0.15207400918006897\n",
      "Train Loss: 0.15207350254058838\n",
      "Train Loss: 0.1520729511976242\n",
      "Train Loss: 0.15207254886627197\n",
      "Train Loss: 0.15207204222679138\n",
      "Train Loss: 0.15207180380821228\n",
      "Train Loss: 0.15207189321517944\n",
      "Train Loss: 0.15207205712795258\n",
      "Train Loss: 0.1520724892616272\n",
      "Train Loss: 0.15207308530807495\n",
      "Train Loss: 0.1520727127790451\n",
      "Train Loss: 0.15207235515117645\n",
      "Train Loss: 0.15207220613956451\n",
      "Train Loss: 0.1520722210407257\n",
      "Train Loss: 0.15207244455814362\n",
      "Train Loss: 0.1520727127790451\n",
      "Train Loss: 0.1520729959011078\n",
      "Train Loss: 0.1520727425813675\n",
      "Train Loss: 0.1520732343196869\n",
      "Train Loss: 0.1520746499300003\n",
      "Train Loss: 0.15207643806934357\n",
      "Train Loss: 0.15207813680171967\n",
      "Train Loss: 0.15207882225513458\n",
      "Train Loss: 0.15207867324352264\n",
      "Train Loss: 0.15207818150520325\n",
      "Train Loss: 0.1520775556564331\n",
      "Train Loss: 0.1520768702030182\n",
      "Train Loss: 0.15207627415657043\n",
      "Train Loss: 0.15207572281360626\n",
      "Train Loss: 0.1520751565694809\n",
      "Train Loss: 0.1520746797323227\n",
      "Train Loss: 0.15207435190677643\n",
      "Train Loss: 0.15207433700561523\n",
      "Train Loss: 0.15207457542419434\n",
      "Train Loss: 0.15207447111606598\n",
      "Train Loss: 0.1520742028951645\n",
      "Train Loss: 0.1520739644765854\n",
      "Train Loss: 0.1520739197731018\n",
      "Train Loss: 0.1520744413137436\n",
      "Train Loss: 0.1520746648311615\n",
      "Train Loss: 0.15207454562187195\n",
      "Train Loss: 0.15207414329051971\n",
      "Train Loss: 0.15207350254058838\n",
      "Train Loss: 0.15207292139530182\n",
      "Train Loss: 0.15207241475582123\n",
      "Train Loss: 0.1520720273256302\n",
      "Train Loss: 0.1520717293024063\n",
      "Train Loss: 0.15207169950008392\n",
      "Train Loss: 0.152072012424469\n",
      "Train Loss: 0.15207219123840332\n",
      "Train Loss: 0.1520719677209854\n",
      "Train Loss: 0.15207155048847198\n",
      "Train Loss: 0.15207087993621826\n",
      "Train Loss: 0.15207017958164215\n",
      "Train Loss: 0.15206946432590485\n",
      "Train Loss: 0.1520688533782959\n",
      "Train Loss: 0.15206855535507202\n",
      "Train Loss: 0.15206831693649292\n",
      "Train Loss: 0.15206857025623322\n",
      "Train Loss: 0.15206889808177948\n",
      "Train Loss: 0.15206901729106903\n",
      "Train Loss: 0.1520688831806183\n",
      "Train Loss: 0.15206824243068695\n",
      "Train Loss: 0.152067631483078\n",
      "Train Loss: 0.1520671844482422\n",
      "Train Loss: 0.15206697583198547\n",
      "Train Loss: 0.15206687152385712\n",
      "Train Loss: 0.1520669311285019\n",
      "Train Loss: 0.1520669162273407\n",
      "Train Loss: 0.15206710994243622\n",
      "Train Loss: 0.15206730365753174\n",
      "Train Loss: 0.15206760168075562\n",
      "Train Loss: 0.15206804871559143\n",
      "Train Loss: 0.15206916630268097\n",
      "Train Loss: 0.15207046270370483\n",
      "Train Loss: 0.15207113325595856\n",
      "Train Loss: 0.15207138657569885\n",
      "Train Loss: 0.15207108855247498\n",
      "Train Loss: 0.15207071602344513\n",
      "Train Loss: 0.15207011997699738\n",
      "Train Loss: 0.15206962823867798\n",
      "Train Loss: 0.1520691215991974\n",
      "Train Loss: 0.152068629860878\n",
      "Train Loss: 0.15206830203533173\n",
      "Train Loss: 0.15206827223300934\n",
      "Train Loss: 0.15206831693649292\n",
      "Train Loss: 0.1520681232213974\n",
      "Train Loss: 0.1520676612854004\n",
      "Train Loss: 0.15206702053546906\n",
      "Train Loss: 0.15206633508205414\n",
      "Train Loss: 0.15206559002399445\n",
      "Train Loss: 0.15206502377986908\n",
      "Train Loss: 0.15206481516361237\n",
      "Train Loss: 0.1520649790763855\n",
      "Train Loss: 0.15206500887870789\n",
      "Train Loss: 0.15206454694271088\n",
      "Train Loss: 0.15206389129161835\n",
      "Train Loss: 0.15206317603588104\n",
      "Train Loss: 0.1520625203847885\n",
      "Train Loss: 0.15206193923950195\n",
      "Train Loss: 0.15206168591976166\n",
      "Train Loss: 0.15206189453601837\n",
      "Train Loss: 0.15206246078014374\n",
      "Train Loss: 0.15206292271614075\n",
      "Train Loss: 0.15206323564052582\n",
      "Train Loss: 0.15206259489059448\n",
      "Train Loss: 0.15206196904182434\n",
      "Train Loss: 0.1520615518093109\n",
      "Train Loss: 0.15206119418144226\n",
      "Train Loss: 0.15206089615821838\n",
      "Train Loss: 0.15206103026866913\n",
      "Train Loss: 0.15206171572208405\n",
      "Train Loss: 0.1520632803440094\n",
      "Train Loss: 0.15206439793109894\n",
      "Train Loss: 0.15206462144851685\n",
      "Train Loss: 0.15206441283226013\n",
      "Train Loss: 0.15206412971019745\n",
      "Train Loss: 0.15206393599510193\n",
      "Train Loss: 0.1520644873380661\n",
      "Train Loss: 0.1520652025938034\n",
      "Train Loss: 0.15206539630889893\n",
      "Train Loss: 0.15206533670425415\n",
      "Train Loss: 0.1520649939775467\n",
      "Train Loss: 0.15206459164619446\n",
      "Train Loss: 0.1520642787218094\n",
      "Train Loss: 0.1520640105009079\n",
      "Train Loss: 0.15206415951251984\n",
      "Train Loss: 0.15206453204154968\n",
      "Train Loss: 0.15206468105316162\n",
      "Train Loss: 0.1520645171403885\n",
      "Train Loss: 0.15206414461135864\n",
      "Train Loss: 0.15206372737884521\n",
      "Train Loss: 0.1520633101463318\n",
      "Train Loss: 0.15206299722194672\n",
      "Train Loss: 0.15206275880336761\n",
      "Train Loss: 0.152062326669693\n",
      "Train Loss: 0.15206173062324524\n",
      "Train Loss: 0.15206117928028107\n",
      "Train Loss: 0.1520605832338333\n",
      "Train Loss: 0.15206009149551392\n",
      "Train Loss: 0.1520596593618393\n",
      "Train Loss: 0.15205949544906616\n",
      "Train Loss: 0.15205979347229004\n",
      "Train Loss: 0.1520599126815796\n",
      "Train Loss: 0.1520594209432602\n",
      "Train Loss: 0.15205876529216766\n",
      "Train Loss: 0.15205855667591095\n",
      "Train Loss: 0.15205824375152588\n",
      "Train Loss: 0.15205831825733185\n",
      "Train Loss: 0.15205973386764526\n",
      "Train Loss: 0.15206070244312286\n",
      "Train Loss: 0.15206077694892883\n",
      "Train Loss: 0.1520605981349945\n",
      "Train Loss: 0.15206041932106018\n",
      "Train Loss: 0.15206032991409302\n",
      "Train Loss: 0.15206071734428406\n",
      "Train Loss: 0.15206196904182434\n",
      "Train Loss: 0.15206283330917358\n",
      "Train Loss: 0.15206298232078552\n",
      "Train Loss: 0.1520627737045288\n",
      "Train Loss: 0.15206244587898254\n",
      "Train Loss: 0.1520620733499527\n",
      "Train Loss: 0.15206214785575867\n",
      "Train Loss: 0.15206220746040344\n",
      "Train Loss: 0.15206201374530792\n",
      "Train Loss: 0.1520615518093109\n",
      "Train Loss: 0.1520623415708542\n",
      "Train Loss: 0.15206217765808105\n",
      "Train Loss: 0.15206187963485718\n",
      "Train Loss: 0.1520616114139557\n",
      "Train Loss: 0.15206198394298553\n",
      "Train Loss: 0.15206225216388702\n",
      "Train Loss: 0.15206211805343628\n",
      "Train Loss: 0.15206210315227509\n",
      "Train Loss: 0.15206338465213776\n",
      "Train Loss: 0.15206420421600342\n",
      "Train Loss: 0.1520644724369049\n",
      "Train Loss: 0.15206454694271088\n",
      "Train Loss: 0.15206867456436157\n",
      "Train Loss: 0.15207038819789886\n",
      "Train Loss: 0.15207064151763916\n",
      "Train Loss: 0.15207064151763916\n",
      "Train Loss: 0.152070552110672\n",
      "Train Loss: 0.152070552110672\n",
      "Train Loss: 0.15207058191299438\n",
      "Train Loss: 0.15207092463970184\n",
      "Train Loss: 0.15207146108150482\n",
      "Train Loss: 0.15207228064537048\n",
      "Train Loss: 0.15207406878471375\n",
      "Train Loss: 0.15207500755786896\n",
      "Train Loss: 0.15207499265670776\n",
      "Train Loss: 0.15207454562187195\n",
      "Train Loss: 0.15207405388355255\n",
      "Train Loss: 0.1520737111568451\n",
      "Train Loss: 0.1520734280347824\n",
      "Train Loss: 0.15207324922084808\n",
      "Train Loss: 0.15207314491271973\n",
      "Train Loss: 0.1520732343196869\n",
      "Train Loss: 0.15207310020923615\n",
      "Train Loss: 0.1520734429359436\n",
      "Train Loss: 0.15207764506340027\n",
      "Train Loss: 0.15207968652248383\n",
      "Train Loss: 0.15208017826080322\n",
      "Train Loss: 0.15208005905151367\n",
      "Train Loss: 0.1520797312259674\n",
      "Train Loss: 0.15207941830158234\n",
      "Train Loss: 0.1520792543888092\n",
      "Train Loss: 0.15207922458648682\n",
      "Train Loss: 0.15207961201667786\n",
      "Train Loss: 0.1520809531211853\n",
      "Train Loss: 0.152081698179245\n",
      "Train Loss: 0.15208172798156738\n",
      "Train Loss: 0.1520814597606659\n",
      "Train Loss: 0.15208105742931366\n",
      "Train Loss: 0.15208061039447784\n",
      "Train Loss: 0.15208007395267487\n",
      "Train Loss: 0.15207962691783905\n",
      "Train Loss: 0.15207920968532562\n",
      "Train Loss: 0.15207867324352264\n",
      "Train Loss: 0.15207819640636444\n",
      "Train Loss: 0.15207773447036743\n",
      "Train Loss: 0.15207742154598236\n",
      "Train Loss: 0.15207728743553162\n",
      "Train Loss: 0.15207728743553162\n",
      "Train Loss: 0.15207719802856445\n",
      "Train Loss: 0.15207692980766296\n",
      "Train Loss: 0.15207651257514954\n",
      "Train Loss: 0.15207597613334656\n",
      "Train Loss: 0.15207539498806\n",
      "Train Loss: 0.15207484364509583\n",
      "Train Loss: 0.15207432210445404\n",
      "Train Loss: 0.15207384526729584\n",
      "Train Loss: 0.15207333862781525\n",
      "Train Loss: 0.15207287669181824\n",
      "Train Loss: 0.1520724594593048\n",
      "Train Loss: 0.15207228064537048\n",
      "Train Loss: 0.15207220613956451\n",
      "Train Loss: 0.15207208693027496\n",
      "Train Loss: 0.15207190811634064\n",
      "Train Loss: 0.15207137167453766\n",
      "Train Loss: 0.1520708203315735\n",
      "Train Loss: 0.15207020938396454\n",
      "Train Loss: 0.15206971764564514\n",
      "Train Loss: 0.15206937491893768\n",
      "Train Loss: 0.15206921100616455\n",
      "Train Loss: 0.15206918120384216\n",
      "Train Loss: 0.1520691066980362\n",
      "Train Loss: 0.1520691066980362\n",
      "Train Loss: 0.15206894278526306\n",
      "Train Loss: 0.1520685851573944\n",
      "Train Loss: 0.15206825733184814\n",
      "Train Loss: 0.15206782519817352\n",
      "Train Loss: 0.1520674228668213\n",
      "Train Loss: 0.15206703543663025\n",
      "Train Loss: 0.1520666480064392\n",
      "Train Loss: 0.15206634998321533\n",
      "Train Loss: 0.1520659327507019\n",
      "Train Loss: 0.15206538140773773\n",
      "Train Loss: 0.1520649492740631\n",
      "Train Loss: 0.15206459164619446\n",
      "Train Loss: 0.1520647257566452\n",
      "Train Loss: 0.15206481516361237\n",
      "Train Loss: 0.15206477046012878\n",
      "Train Loss: 0.15206456184387207\n",
      "Train Loss: 0.15206436812877655\n",
      "Train Loss: 0.15206407010555267\n",
      "Train Loss: 0.15206389129161835\n",
      "Train Loss: 0.15206357836723328\n",
      "Train Loss: 0.15206332504749298\n",
      "Train Loss: 0.15206293761730194\n",
      "Train Loss: 0.15206266939640045\n",
      "Train Loss: 0.15206289291381836\n",
      "Train Loss: 0.15206363797187805\n",
      "Train Loss: 0.1520649641752243\n",
      "Train Loss: 0.15206575393676758\n",
      "Train Loss: 0.15206605195999146\n",
      "Train Loss: 0.15206627547740936\n",
      "Train Loss: 0.1520663946866989\n",
      "Train Loss: 0.1520662158727646\n",
      "Train Loss: 0.15206585824489594\n",
      "Train Loss: 0.15206561982631683\n",
      "Train Loss: 0.1520652025938034\n",
      "Train Loss: 0.15206478536128998\n",
      "Train Loss: 0.1520642787218094\n",
      "Train Loss: 0.15206372737884521\n",
      "Train Loss: 0.15206319093704224\n",
      "Train Loss: 0.1520627737045288\n",
      "Train Loss: 0.15206240117549896\n",
      "Train Loss: 0.15206201374530792\n",
      "Train Loss: 0.15206150710582733\n",
      "Train Loss: 0.15206103026866913\n",
      "Train Loss: 0.1520606130361557\n",
      "Train Loss: 0.15206025540828705\n",
      "Train Loss: 0.15205974876880646\n",
      "Train Loss: 0.15205928683280945\n",
      "Train Loss: 0.15205909311771393\n",
      "Train Loss: 0.15205952525138855\n",
      "Train Loss: 0.15206053853034973\n",
      "Train Loss: 0.15206140279769897\n",
      "Train Loss: 0.15206198394298553\n",
      "Train Loss: 0.15206193923950195\n",
      "Train Loss: 0.15206167101860046\n",
      "Train Loss: 0.15206125378608704\n",
      "Train Loss: 0.1520605981349945\n",
      "Train Loss: 0.15205997228622437\n",
      "Train Loss: 0.15205971896648407\n",
      "Train Loss: 0.1520593762397766\n",
      "Train Loss: 0.1520591825246811\n",
      "Train Loss: 0.15205927193164825\n",
      "Train Loss: 0.15205949544906616\n",
      "Train Loss: 0.15206095576286316\n",
      "Train Loss: 0.15206176042556763\n",
      "Train Loss: 0.15206198394298553\n",
      "Train Loss: 0.15206219255924225\n",
      "Train Loss: 0.15206241607666016\n",
      "Train Loss: 0.15206283330917358\n",
      "Train Loss: 0.15206347405910492\n",
      "Train Loss: 0.15206381678581238\n",
      "Train Loss: 0.15206381678581238\n",
      "Train Loss: 0.15206441283226013\n",
      "Train Loss: 0.1520644873380661\n",
      "Train Loss: 0.15206417441368103\n",
      "Train Loss: 0.15206409990787506\n",
      "Train Loss: 0.15206408500671387\n",
      "Train Loss: 0.15206415951251984\n",
      "Train Loss: 0.15206439793109894\n",
      "Train Loss: 0.15206457674503326\n",
      "Train Loss: 0.15206490457057953\n",
      "Train Loss: 0.15206527709960938\n",
      "Train Loss: 0.1520656943321228\n",
      "Train Loss: 0.1520664542913437\n",
      "Train Loss: 0.15206733345985413\n",
      "Train Loss: 0.15206779539585114\n",
      "Train Loss: 0.15206795930862427\n",
      "Train Loss: 0.15206775069236755\n",
      "Train Loss: 0.15206767618656158\n",
      "Train Loss: 0.1520697921514511\n",
      "Train Loss: 0.1520700752735138\n",
      "Train Loss: 0.1520698517560959\n",
      "Train Loss: 0.15206970274448395\n",
      "Train Loss: 0.15206991136074066\n",
      "Train Loss: 0.1520702987909317\n",
      "Train Loss: 0.15207089483737946\n",
      "Train Loss: 0.15207161009311676\n",
      "Train Loss: 0.15207239985466003\n",
      "Train Loss: 0.1520732045173645\n",
      "Train Loss: 0.15207397937774658\n",
      "Train Loss: 0.15207447111606598\n",
      "Train Loss: 0.15207482874393463\n",
      "Train Loss: 0.15207511186599731\n",
      "Train Loss: 0.15207526087760925\n",
      "Train Loss: 0.15207533538341522\n",
      "Train Loss: 0.1520754098892212\n",
      "Train Loss: 0.1520754098892212\n",
      "Train Loss: 0.15207521617412567\n",
      "Train Loss: 0.15207500755786896\n",
      "Train Loss: 0.152074933052063\n",
      "Train Loss: 0.15207503736019135\n",
      "Train Loss: 0.1520751565694809\n",
      "Train Loss: 0.15207524597644806\n",
      "Train Loss: 0.15207530558109283\n",
      "Train Loss: 0.15207533538341522\n",
      "Train Loss: 0.15207543969154358\n",
      "Train Loss: 0.15207554399967194\n",
      "Train Loss: 0.15207549929618835\n",
      "Train Loss: 0.1520756483078003\n",
      "Train Loss: 0.15207526087760925\n",
      "Train Loss: 0.15207506716251373\n",
      "Train Loss: 0.15207529067993164\n",
      "Train Loss: 0.15207494795322418\n",
      "Train Loss: 0.15207460522651672\n",
      "Train Loss: 0.15207433700561523\n",
      "Train Loss: 0.152073934674263\n",
      "Train Loss: 0.15207360684871674\n",
      "Train Loss: 0.15207338333129883\n",
      "Train Loss: 0.15207304060459137\n",
      "Train Loss: 0.15207283198833466\n",
      "Train Loss: 0.1520727276802063\n",
      "Train Loss: 0.15207263827323914\n",
      "Train Loss: 0.15207266807556152\n",
      "Train Loss: 0.15207286179065704\n",
      "Train Loss: 0.15207315981388092\n",
      "Train Loss: 0.1520734280347824\n",
      "Train Loss: 0.1520739048719406\n",
      "Train Loss: 0.15207445621490479\n",
      "Train Loss: 0.1520746946334839\n",
      "Train Loss: 0.1520759016275406\n",
      "Train Loss: 0.15207669138908386\n",
      "Train Loss: 0.1520768702030182\n",
      "Train Loss: 0.15207666158676147\n",
      "Train Loss: 0.15207640826702118\n",
      "Train Loss: 0.15207628905773163\n",
      "Train Loss: 0.15207622945308685\n",
      "Train Loss: 0.15207621455192566\n",
      "Train Loss: 0.15207618474960327\n",
      "Train Loss: 0.15207616984844208\n",
      "Train Loss: 0.15207624435424805\n",
      "Train Loss: 0.15207639336585999\n",
      "Train Loss: 0.1520765721797943\n",
      "Train Loss: 0.152076855301857\n",
      "Train Loss: 0.1520773023366928\n",
      "Train Loss: 0.15207788348197937\n",
      "Train Loss: 0.15207864344120026\n",
      "Train Loss: 0.15207943320274353\n",
      "Train Loss: 0.15208008885383606\n",
      "Train Loss: 0.15208066999912262\n",
      "Train Loss: 0.15208131074905396\n",
      "Train Loss: 0.15208183228969574\n",
      "Train Loss: 0.15208235383033752\n",
      "Train Loss: 0.15208272635936737\n",
      "Train Loss: 0.15208278596401215\n",
      "Train Loss: 0.1520838737487793\n",
      "Train Loss: 0.1520848125219345\n",
      "Train Loss: 0.15208512544631958\n",
      "Train Loss: 0.15208503603935242\n",
      "Train Loss: 0.15208470821380615\n",
      "Train Loss: 0.1520843803882599\n",
      "Train Loss: 0.15208424627780914\n",
      "Train Loss: 0.1520840972661972\n",
      "Train Loss: 0.15208393335342407\n",
      "Train Loss: 0.1520836353302002\n",
      "Train Loss: 0.15208320319652557\n",
      "Train Loss: 0.15208277106285095\n",
      "Train Loss: 0.15208232402801514\n",
      "Train Loss: 0.15208199620246887\n",
      "Train Loss: 0.15208201110363007\n",
      "Train Loss: 0.15208220481872559\n",
      "Train Loss: 0.15208221971988678\n",
      "Train Loss: 0.15208204090595245\n",
      "Train Loss: 0.15208181738853455\n",
      "Train Loss: 0.15208159387111664\n",
      "Train Loss: 0.15208128094673157\n",
      "Train Loss: 0.15208081901073456\n",
      "Train Loss: 0.15208040177822113\n",
      "Train Loss: 0.1520799696445465\n",
      "Train Loss: 0.15207959711551666\n",
      "Train Loss: 0.15207938849925995\n",
      "Train Loss: 0.15207940340042114\n",
      "Train Loss: 0.15207934379577637\n",
      "Train Loss: 0.15207912027835846\n",
      "Train Loss: 0.15207910537719727\n",
      "Train Loss: 0.15207913517951965\n",
      "Train Loss: 0.15207907557487488\n",
      "Train Loss: 0.15207882225513458\n",
      "Train Loss: 0.15207847952842712\n",
      "Train Loss: 0.15207812190055847\n",
      "Train Loss: 0.15207770466804504\n",
      "Train Loss: 0.15207727253437042\n",
      "Train Loss: 0.15207691490650177\n",
      "Train Loss: 0.15207639336585999\n",
      "Train Loss: 0.15207593142986298\n",
      "Train Loss: 0.15207549929618835\n",
      "Train Loss: 0.15207530558109283\n",
      "Train Loss: 0.1520753651857376\n",
      "Train Loss: 0.15207551419734955\n",
      "Train Loss: 0.15207543969154358\n",
      "Train Loss: 0.15207530558109283\n",
      "Train Loss: 0.1520751416683197\n",
      "Train Loss: 0.15207529067993164\n",
      "Train Loss: 0.15207527577877045\n",
      "Train Loss: 0.15207530558109283\n",
      "Train Loss: 0.15207518637180328\n",
      "Train Loss: 0.15207482874393463\n",
      "Train Loss: 0.15207435190677643\n",
      "Train Loss: 0.15207384526729584\n",
      "Train Loss: 0.15207350254058838\n",
      "Train Loss: 0.1520739495754242\n",
      "Train Loss: 0.1520746797323227\n",
      "Train Loss: 0.15207497775554657\n",
      "Train Loss: 0.1520746946334839\n",
      "Train Loss: 0.15207439661026\n",
      "Train Loss: 0.15207451581954956\n",
      "Train Loss: 0.15207470953464508\n",
      "Train Loss: 0.1520748883485794\n",
      "Train Loss: 0.15207472443580627\n",
      "Train Loss: 0.15207436680793762\n",
      "Train Loss: 0.15207400918006897\n",
      "Train Loss: 0.15207350254058838\n",
      "Train Loss: 0.1520729959011078\n",
      "Train Loss: 0.1520724594593048\n",
      "Train Loss: 0.15207192301750183\n",
      "Train Loss: 0.15207140147686005\n",
      "Train Loss: 0.15207092463970184\n",
      "Train Loss: 0.15207049250602722\n",
      "Train Loss: 0.15207041800022125\n",
      "Train Loss: 0.15207070112228394\n",
      "Train Loss: 0.15207107365131378\n",
      "Train Loss: 0.1520712822675705\n",
      "Train Loss: 0.1520712524652481\n",
      "Train Loss: 0.15207135677337646\n",
      "Train Loss: 0.15207146108150482\n",
      "Train Loss: 0.15207193791866302\n",
      "Train Loss: 0.1520722210407257\n",
      "Train Loss: 0.1520719677209854\n",
      "Train Loss: 0.1520715355873108\n",
      "Train Loss: 0.15207108855247498\n",
      "Train Loss: 0.15207064151763916\n",
      "Train Loss: 0.15207073092460632\n",
      "Train Loss: 0.15207085013389587\n",
      "Train Loss: 0.1520707905292511\n",
      "Train Loss: 0.15207117795944214\n",
      "Train Loss: 0.15207162499427795\n",
      "Train Loss: 0.15207181870937347\n",
      "Train Loss: 0.15207187831401825\n",
      "Train Loss: 0.15207189321517944\n",
      "Train Loss: 0.1520717591047287\n",
      "Train Loss: 0.1520717293024063\n",
      "Train Loss: 0.1520717740058899\n",
      "Train Loss: 0.15207184851169586\n",
      "Train Loss: 0.15207195281982422\n",
      "Train Loss: 0.15207207202911377\n",
      "Train Loss: 0.15207213163375854\n",
      "Train Loss: 0.15207213163375854\n",
      "Train Loss: 0.15207217633724213\n",
      "Train Loss: 0.15207228064537048\n",
      "Train Loss: 0.1520724594593048\n",
      "Train Loss: 0.1520727127790451\n",
      "Train Loss: 0.15207313001155853\n",
      "Train Loss: 0.15207432210445404\n",
      "Train Loss: 0.1520746946334839\n",
      "Train Loss: 0.1520744413137436\n",
      "Train Loss: 0.1520741581916809\n",
      "Train Loss: 0.152073934674263\n",
      "Train Loss: 0.15207386016845703\n",
      "Train Loss: 0.15207380056381226\n",
      "Train Loss: 0.1520739197731018\n",
      "Train Loss: 0.1520741581916809\n",
      "Train Loss: 0.15207451581954956\n",
      "Train Loss: 0.15207494795322418\n",
      "Train Loss: 0.15207555890083313\n",
      "Train Loss: 0.1520761400461197\n",
      "Train Loss: 0.15207669138908386\n",
      "Train Loss: 0.15207719802856445\n",
      "Train Loss: 0.1520778238773346\n",
      "Train Loss: 0.15207859873771667\n",
      "Train Loss: 0.15207943320274353\n",
      "Train Loss: 0.1520802527666092\n",
      "Train Loss: 0.15208099782466888\n",
      "Train Loss: 0.1520826369524002\n",
      "Train Loss: 0.15208297967910767\n",
      "Train Loss: 0.15208294987678528\n",
      "Train Loss: 0.1520829200744629\n",
      "Train Loss: 0.15208286046981812\n",
      "Train Loss: 0.15208277106285095\n",
      "Train Loss: 0.15208277106285095\n",
      "Train Loss: 0.15208274126052856\n",
      "Train Loss: 0.1520826816558838\n",
      "Train Loss: 0.15208250284194946\n",
      "Train Loss: 0.1520824283361435\n",
      "Train Loss: 0.15208236873149872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.15208230912685394\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = pl.Trainer(default_root_dir=\"model/\", max_epochs=2000)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304a37eb4264d138f50bc49b616fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.09540335088968277\n",
      "[{}]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.test(model=model, dataloaders=val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to predict btc Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'losses': tensor(0.0954)}\n"
     ]
    }
   ],
   "source": [
    "x = val_loader.dataset.X\n",
    "y = val_loader.dataset.y\n",
    "ref = val_loader.dataset.initial_price\n",
    "date = val_loader.dataset.current_date\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "    \n",
    "predict_out = np.multiply( (np.array(y_hat)+1)[:,0,:].T, ref)\n",
    "\n",
    "print({\n",
    "    'losses':nn.L1Loss()(y, y_hat),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          29792.015625,
          29908.744140625,
          29771.802734375,
          30084.5390625,
          29176.916015625,
          29227.390625,
          29354.97265625,
          29210.689453125,
          29319.24609375,
          29356.91796875,
          29275.30859375,
          29230.111328125,
          29675.732421875,
          29151.958984375,
          29178.6796875,
          29074.091796875,
          29042.126953125,
          29041.85546875,
          29180.578125,
          29765.4921875,
          29561.494140625,
          29429.591796875,
          29397.71484375,
          29415.96484375,
          29282.9140625,
          29408.443359375,
          29170.34765625,
          28701.779296875,
          26664.55078125,
          26049.556640625,
          26096.205078125,
          26189.583984375,
          26124.140625,
          26031.65625,
          26431.640625,
          26162.373046875,
          26047.66796875,
          26008.462890625,
          26089.693359375,
          26106.150390625,
          27727.392578125,
          27297.265625,
          25931.47265625,
          25800.724609375,
          25868.798828125,
          25969.56640625,
          25812.416015625,
          25779.982421875,
          25753.236328125,
          26240.1953125,
          25905.654296875,
          25895.677734375,
          25832.2265625,
          25162.654296875,
          25833.34375,
          26228.32421875,
          26539.673828125,
          26608.693359375,
          26568.28125,
          26534.1875,
          26754.28125,
          27211.1171875,
          27132.0078125,
          26567.6328125,
          26579.568359375,
          26579.390625,
          26256.826171875,
          26298.48046875,
          26217.25,
          26352.716796875,
          27021.546875,
          26911.720703125,
          26967.916015625,
          27983.75,
          27530.78515625,
          27429.978515625,
          27799.39453125,
          27415.912109375,
          27946.59765625,
          27968.83984375,
          27935.08984375,
          27583.677734375,
          27391.01953125,
          26873.3203125,
          26756.798828125,
          26862.375,
          26861.70703125,
          27159.65234375,
          28519.466796875,
          28415.748046875,
          28328.341796875,
          28719.806640625,
          29682.94921875,
          29918.412109375,
          29993.896484375,
          33086.234375,
          33901.52734375,
          34502.8203125,
          34156.6484375,
          33909.80078125,
          34089.57421875,
          34538.48046875,
          34502.36328125,
          34667.78125,
          35437.25390625,
          34938.2421875,
          34732.32421875,
          35082.1953125,
          35049.35546875,
          35037.37109375,
          35443.5625,
          35655.27734375,
          36693.125,
          37313.96875,
          37138.05078125,
          37054.51953125,
          36502.35546875,
          35537.640625,
          37880.58203125,
          36154.76953125,
          36596.68359375,
          36585.703125,
          37386.546875,
          37476.95703125,
          35813.8125,
          37432.33984375,
          37289.62109375,
          37720.28125,
          37796.79296875,
          37479.12109375,
          37254.16796875,
          37831.0859375,
          37858.4921875,
          37712.74609375,
          38688.75,
          39476.33203125,
          39978.390625,
          41980.09765625,
          44080.6484375,
          43746.4453125,
          43292.6640625,
          44166.6015625,
          43725.984375,
          43779.69921875,
          41243.83203125,
          41450.22265625,
          42890.7421875,
          43023.97265625,
          41929.7578125,
          42240.1171875,
          41364.6640625,
          42623.5390625,
          42270.52734375,
          43652.25,
          43869.15234375,
          43997.90234375,
          43739.54296875,
          43016.1171875,
          43613.140625,
          42653.109375
         ],
         "high": [
          30195.53125,
          30045.998046875,
          29991.615234375,
          30330.640625,
          30093.39453125,
          29353.16015625,
          29675.552734375,
          29560.966796875,
          29521.513671875,
          29396.84375,
          29443.169921875,
          29489.873046875,
          29675.732421875,
          29987.998046875,
          29375.70703125,
          29302.078125,
          29102.46484375,
          29160.822265625,
          29244.28125,
          30176.796875,
          30093.435546875,
          29688.564453125,
          29517.7734375,
          29465.11328125,
          29441.43359375,
          29660.25390625,
          29439.12109375,
          29221.9765625,
          28745.947265625,
          26808.1953125,
          26249.44921875,
          26260.681640625,
          26220.201171875,
          26135.5078125,
          26786.8984375,
          26554.91015625,
          26248.103515625,
          26107.384765625,
          26165.373046875,
          26198.578125,
          28089.337890625,
          27760.16015625,
          27456.078125,
          26125.869140625,
          25970.28515625,
          26087.1484375,
          26081.525390625,
          25858.375,
          25953.015625,
          26409.302734375,
          26414.005859375,
          25921.9765625,
          25978.130859375,
          25883.947265625,
          26451.939453125,
          26376.11328125,
          26774.623046875,
          26840.498046875,
          26754.76953125,
          26617.998046875,
          27414.734375,
          27488.763671875,
          27379.505859375,
          27152.939453125,
          26726.078125,
          26634.185546875,
          26716.05859375,
          26421.5078125,
          26389.884765625,
          26817.841796875,
          27259.5,
          27225.9375,
          27091.794921875,
          28047.23828125,
          28494.458984375,
          27667.19140625,
          27826.658203125,
          28091.861328125,
          28252.537109375,
          28028.091796875,
          28102.169921875,
          27989.470703125,
          27715.84765625,
          27474.115234375,
          26921.439453125,
          27092.697265625,
          26969,
          27289.169921875,
          29448.138671875,
          28618.751953125,
          28889.009765625,
          28892.474609375,
          30104.0859375,
          30287.482421875,
          30199.43359375,
          34370.4375,
          35150.43359375,
          35133.7578125,
          34832.91015625,
          34238.2109375,
          34399.390625,
          34743.26171875,
          34843.93359375,
          34719.25390625,
          35527.9296875,
          35919.84375,
          34942.47265625,
          35256.03125,
          35340.33984375,
          35286.02734375,
          35892.41796875,
          35994.41796875,
          37926.2578125,
          37493.80078125,
          37407.09375,
          37227.69140625,
          37405.1171875,
          36753.3515625,
          37964.89453125,
          37934.625,
          36704.484375,
          36839.28125,
          37509.35546875,
          37756.8203125,
          37631.140625,
          37856.98046875,
          37643.91796875,
          38415.33984375,
          37892.4296875,
          37820.30078125,
          37559.35546875,
          38368.48046875,
          38366.11328125,
          38141.75390625,
          38954.109375,
          39678.9375,
          40135.60546875,
          42371.75,
          44408.6640625,
          44265.76953125,
          44042.58984375,
          44705.515625,
          44361.2578125,
          44034.625,
          43808.375,
          42048.3046875,
          43429.78125,
          43390.859375,
          43087.82421875,
          42664.9453125,
          42359.49609375,
          42720.296875,
          43354.296875,
          44275.5859375,
          44240.66796875,
          44367.95703125,
          44015.69921875,
          43945.5234375,
          43765.09375,
          43599.84765625
         ],
         "low": [
          29638.095703125,
          29733.8515625,
          29664.12109375,
          29741.52734375,
          28934.294921875,
          29062.43359375,
          29113.912109375,
          29099.3515625,
          29125.845703125,
          29264.166015625,
          29059.501953125,
          29131.578125,
          28657.0234375,
          28946.509765625,
          28959.48828125,
          28885.3359375,
          28957.796875,
          28963.833984375,
          28724.140625,
          29113.814453125,
          29376.80078125,
          29354.447265625,
          29253.517578125,
          29357.587890625,
          29265.806640625,
          29124.10546875,
          29088.853515625,
          28701.779296875,
          25409.111328125,
          25668.921875,
          25802.408203125,
          26004.314453125,
          25846.087890625,
          25520.728515625,
          25804.998046875,
          25914.92578125,
          25786.8125,
          25983.87890625,
          25965.09765625,
          25880.599609375,
          25912.62890625,
          27069.20703125,
          25752.9296875,
          25362.609375,
          25753.09375,
          25817.03125,
          25657.025390625,
          25589.98828125,
          25404.359375,
          25608.201171875,
          25677.48046875,
          25810.494140625,
          25640.26171875,
          24930.296875,
          25133.078125,
          25781.123046875,
          26171.451171875,
          26240.701171875,
          26473.890625,
          26445.07421875,
          26415.515625,
          26681.60546875,
          26864.08203125,
          26389.30078125,
          26495.533203125,
          26520.51953125,
          26221.05078125,
          26011.46875,
          26090.712890625,
          26111.46484375,
          26327.322265625,
          26721.763671875,
          26888.96875,
          26965.09375,
          27347.787109375,
          27216.001953125,
          27248.10546875,
          27375.6015625,
          27215.552734375,
          27870.423828125,
          27740.662109375,
          27302.5625,
          27301.654296875,
          26561.099609375,
          26558.3203125,
          26686.322265625,
          26814.5859375,
          26817.89453125,
          27130.47265625,
          28110.185546875,
          28174.251953125,
          28177.98828125,
          28601.669921875,
          29481.751953125,
          29720.3125,
          30097.828125,
          32880.76171875,
          33709.109375,
          33762.32421875,
          33416.88671875,
          33874.8046875,
          33947.56640625,
          34110.97265625,
          34083.30859375,
          34170.69140625,
          34401.57421875,
          34133.44140625,
          34616.69140625,
          34594.2421875,
          34765.36328125,
          34545.81640625,
          35147.80078125,
          35592.1015625,
          36362.75390625,
          36773.66796875,
          36779.1171875,
          36399.60546875,
          34948.5,
          35383.78125,
          35545.47265625,
          35901.234375,
          36233.3125,
          36414.59765625,
          36882.53125,
          35813.8125,
          35670.97265625,
          36923.86328125,
          37261.60546875,
          37617.41796875,
          37162.75,
          36750.12890625,
          36891.08984375,
          37612.6328125,
          37531.140625,
          37629.359375,
          38652.59375,
          39298.1640625,
          39978.62890625,
          41421.1484375,
          43478.08203125,
          42880.6484375,
          43125.296875,
          43627.59765625,
          43593.28515625,
          40234.578125,
          40667.5625,
          40676.8671875,
          41767.08984375,
          41692.96875,
          41723.11328125,
          41274.54296875,
          40530.2578125,
          41826.3359375,
          42223.81640625,
          43330.05078125,
          43441.96875,
          43351.35546875,
          42786.91796875,
          42765.76953125,
          42379.10546875
         ],
         "name": "Actual Price",
         "open": [
          29915.25,
          29805.111328125,
          29908.697265625,
          29790.111328125,
          30081.662109375,
          29178.970703125,
          29225.759765625,
          29353.798828125,
          29212.1640625,
          29319.4453125,
          29357.09375,
          29278.314453125,
          29230.873046875,
          29704.146484375,
          29161.8125,
          29174.3828125,
          29075.388671875,
          29043.701171875,
          29038.513671875,
          29180.01953125,
          29766.6953125,
          29563.97265625,
          29424.90234375,
          29399.787109375,
          29416.59375,
          29283.263671875,
          29408.048828125,
          29169.07421875,
          28699.802734375,
          26636.078125,
          26047.83203125,
          26096.861328125,
          26188.69140625,
          26130.748046875,
          26040.474609375,
          26431.51953125,
          26163.6796875,
          26047.234375,
          26008.2421875,
          26089.615234375,
          26102.486328125,
          27726.083984375,
          27301.9296875,
          25934.021484375,
          25800.91015625,
          25869.47265625,
          25968.169921875,
          25814.95703125,
          25783.931640625,
          25748.3125,
          26245.208984375,
          25905.42578125,
          25895.2109375,
          25831.71484375,
          25160.658203125,
          25837.5546875,
          26228.27734375,
          26533.818359375,
          26606.19921875,
          26567.927734375,
          26532.994140625,
          26760.8515625,
          27210.228515625,
          27129.83984375,
          26564.056640625,
          26578.556640625,
          26579.373046875,
          26253.775390625,
          26294.7578125,
          26209.498046875,
          26355.8125,
          27024.841796875,
          26911.689453125,
          26967.396484375,
          27976.798828125,
          27508.251953125,
          27429.07421875,
          27798.646484375,
          27412.123046875,
          27946.78125,
          27971.677734375,
          27934.47265625,
          27589.201171875,
          27392.076171875,
          26873.29296875,
          26752.87890625,
          26866.203125,
          26858.01171875,
          27162.62890625,
          28522.09765625,
          28413.53125,
          28332.416015625,
          28732.8125,
          29683.380859375,
          29918.654296875,
          30140.685546875,
          33077.3046875,
          33916.04296875,
          34504.2890625,
          34156.5,
          33907.72265625,
          34089.37109375,
          34531.7421875,
          34500.078125,
          34657.2734375,
          35441.578125,
          34942.47265625,
          34736.32421875,
          35090.01171875,
          35044.7890625,
          35047.79296875,
          35419.4765625,
          35633.6328125,
          36702.25,
          37310.0703125,
          37133.9921875,
          37070.3046875,
          36491.7890625,
          35548.11328125,
          37879.98046875,
          36164.82421875,
          36625.37109375,
          36585.765625,
          37374.07421875,
          37469.16015625,
          35756.5546875,
          37420.43359375,
          37296.31640625,
          37721.4140625,
          37796.828125,
          37454.19140625,
          37247.9921875,
          37826.10546875,
          37861.1171875,
          37718.0078125,
          38689.27734375,
          39472.20703125,
          39978.62890625,
          41986.265625,
          44080.0234375,
          43769.1328125,
          43293.13671875,
          44180.01953125,
          43728.3828125,
          43792.01953125,
          41238.734375,
          41468.46484375,
          42884.26171875,
          43028.25,
          41937.7421875,
          42236.109375,
          41348.203125,
          42641.51171875,
          42261.30078125,
          43648.125,
          43868.98828125,
          44012.19921875,
          43728.3671875,
          43010.57421875,
          43599.84765625
         ],
         "type": "candlestick",
         "x": [
          "2023-07-20T00:00:00+00:00",
          "2023-07-21T00:00:00+00:00",
          "2023-07-22T00:00:00+00:00",
          "2023-07-23T00:00:00+00:00",
          "2023-07-24T00:00:00+00:00",
          "2023-07-25T00:00:00+00:00",
          "2023-07-26T00:00:00+00:00",
          "2023-07-27T00:00:00+00:00",
          "2023-07-28T00:00:00+00:00",
          "2023-07-29T00:00:00+00:00",
          "2023-07-30T00:00:00+00:00",
          "2023-07-31T00:00:00+00:00",
          "2023-08-01T00:00:00+00:00",
          "2023-08-02T00:00:00+00:00",
          "2023-08-03T00:00:00+00:00",
          "2023-08-04T00:00:00+00:00",
          "2023-08-05T00:00:00+00:00",
          "2023-08-06T00:00:00+00:00",
          "2023-08-07T00:00:00+00:00",
          "2023-08-08T00:00:00+00:00",
          "2023-08-09T00:00:00+00:00",
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "name": "Pred. High",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          29610.193981423974,
          26001.287631657673,
          34439.37247759476,
          31387.467179702595,
          31709.558070920175,
          30756.56621988956,
          30805.895440266235,
          30940.836136235157,
          30791.543770362623,
          30904.628764646128,
          30944.312700413167,
          30861.274126582313,
          30811.267755977344,
          31310.132709071273,
          30738.47686188668,
          30751.726801681332,
          30647.38043771428,
          30613.979719375493,
          30608.51521333959,
          30757.671754145995,
          31376.066854480654,
          31162.38375836797,
          31015.797952876892,
          30989.324830695754,
          31007.040144201368,
          30866.501402021153,
          30998.036735726055,
          30746.141624846496,
          30251.498311490286,
          28076.195501443,
          27456.145047559403,
          27507.82829396031,
          27604.623306568246,
          27543.547150304774,
          27448.392940492602,
          27860.580307812896,
          27578.262409709394,
          27455.521288320422,
          27414.420923553407,
          27500.193539125845,
          27513.76052989997,
          29225.140685392544,
          28778.057037844323,
          27336.19044663082,
          27195.882217981387,
          27268.15167139517,
          27372.18518009386,
          27210.691639625467,
          27177.98880241858,
          27140.443845406175,
          27664.20598054258,
          27306.051754162647,
          27295.28462551348,
          27228.358574418817,
          26521.019903904526,
          27234.51416109223,
          27646.362025311682,
          27968.42273180955,
          28044.717008240987,
          28004.376298921416,
          27967.55711299088,
          28207.734132822603,
          28681.40761027392,
          28596.672553235665,
          28000.29907336738,
          28015.58304682281,
          28016.443593819626,
          27673.24493487389,
          27716.443163764663,
          27626.57363672997,
          27780.798910565674,
          28486.000784412725,
          28366.730604144977,
          28425.44956160686,
          29489.42751833168,
          28995.550879638176,
          28912.092213143595,
          29301.64628040651,
          28894.224535931367,
          29457.790303699672,
          29484.03287558863,
          29444.81621661689,
          29080.876810014714,
          28873.09377905773,
          28326.261330840178,
          28199.33679627534,
          28318.791360562667,
          28310.157065516803,
          28631.244140512776,
          30064.215956937056,
          29949.779637333006,
          29864.278705633013,
          30286.323623917997,
          31288.286830920028,
          31536.28091323399,
          31770.31683617481,
          34865.71555323899,
          35749.7983670854,
          36369.85356532037,
          36003.260378837585,
          35741.03223826736,
          35932.501972950995,
          36398.79101522267,
          36365.41495248675,
          36531.10944010317,
          37357.81960889697,
          36831.728699468076,
          36614.43430829793,
          36987.24484670907,
          36939.57710944116,
          36942.74342972785,
          37334.52729002852,
          37560.26248252485,
          38686.65737655759,
          39327.341154151596,
          39141.74288447108,
          39074.61194585916,
          38464.817293723114,
          37470.11910427222,
          39928.065059368964,
          38120.17420278629,
          38605.6218022923,
          38563.87495571561,
          39394.8056063815,
          39495.03262476297,
          37689.83046438452,
          39443.675988457166,
          39312.8436053996,
          39760.929824078456,
          39840.42136280611,
          39479.259013303556,
          39261.91104610078,
          39871.28166958969,
          39908.18639670126,
          39757.33940546401,
          40781.123392148875,
          41606.384404641576,
          42140.18742167298,
          44256.37274161354
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "name": "Pred. Low",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          28348.452489629388,
          25505.078089465736,
          32175.935226906324,
          29620.04513342469,
          29919.981308615883,
          29020.765915689524,
          29067.29606767895,
          29194.619899774552,
          29053.75318704592,
          29160.454422972165,
          29197.89870386198,
          29119.548349499586,
          29072.365943770856,
          29543.07301243767,
          29003.681622255594,
          29016.183771826793,
          28917.72816318064,
          28886.212487792363,
          28881.05311687081,
          29021.793403815245,
          29605.28798647737,
          29403.666338414885,
          29265.35011434462,
          29240.371063654777,
          29257.088326094672,
          29124.481202870957,
          29248.591490155784,
          29010.912657160778,
          28544.18567283475,
          26491.652649941854,
          25906.596129414393,
          25955.359580865013,
          26046.69174024253,
          25989.064117412083,
          25899.280152901076,
          26288.20478411205,
          26021.818758360576,
          25906.004818833433,
          25867.224049146753,
          25948.155886827386,
          25960.958748619305,
          27575.753264682135,
          27153.898730757646,
          25793.407320597908,
          25661.017721577082,
          25729.210017909063,
          25827.372153225937,
          25674.989973166725,
          25644.132800239953,
          25608.708319053054,
          26102.91069185408,
          25764.969751431607,
          25754.810291307047,
          25691.660005251644,
          25024.24171113188,
          25697.468186435755,
          26086.07241582754,
          26389.958425426157,
          26461.94685331965,
          26423.882867659675,
          26389.13867539051,
          26615.7607095344,
          27062.70287156047,
          26982.750042794505,
          26420.034345383174,
          26434.45573076082,
          26435.26771070261,
          26111.436911491677,
          26152.197141490877,
          26067.399623492733,
          26212.920812547207,
          26878.322874436155,
          26765.784001788124,
          26821.190617035492,
          27825.12040635303,
          27359.113795196754,
          27280.365329154534,
          27647.933929588064,
          27263.506059395033,
          27795.265570895746,
          27820.02707701677,
          27783.025374182034,
          27439.625785816228,
          27243.569502804196,
          26727.598892816808,
          26607.837657478172,
          26720.547486791387,
          26712.400490408298,
          27015.367808972253,
          28367.46625396493,
          28259.488436108455,
          28178.812964705634,
          28577.03871568665,
          29522.453607033123,
          29756.451519079157,
          29977.280832135584,
          32897.97938554548,
          33732.17053101864,
          34317.22750735469,
          33971.32394850254,
          33723.89532626141,
          33904.558974909596,
          34344.53179798089,
          34313.04145539645,
          34469.3845585878,
          35249.43726334814,
          34753.03761527245,
          34548.00677863858,
          34899.77681828267,
          34854.79932944896,
          34857.78695050045,
          35227.45552094327,
          35440.4507569992,
          36503.276794523,
          37107.80194239598,
          36932.67838636506,
          36869.33615418989,
          36293.95682487544,
          35355.39697507722,
          37674.62245562347,
          35968.764549375046,
          36426.81467041606,
          36387.42391403578,
          37171.458865494,
          37266.03154875687,
          35562.71049317764,
          37217.56914373324,
          37094.124822968384,
          37516.91793076834,
          37591.92315694969,
          37251.14394766255,
          37046.062580500264,
          37621.04178174143,
          37655.86369421752,
          37513.53014680324,
          38479.534211024875,
          39258.219463460846,
          39761.89591342746,
          41758.648789322935
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "name": "Pred. Close",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          29631.200539380312,
          26646.90098778298,
          34075.916156920604,
          31368.27263017441,
          31687.7551084233,
          30735.667966450565,
          30784.96019207826,
          30919.81266924762,
          30770.62174352724,
          30883.62640681956,
          30923.283373933285,
          30840.301231967518,
          30790.32884499966,
          31288.854778784793,
          30717.5874209553,
          30730.828356275335,
          30626.552904644515,
          30593.178347247886,
          30587.71409488772,
          30736.76927099703,
          31354.744118678384,
          31141.20623860648,
          30994.71654543886,
          30968.26492174063,
          30985.968196168542,
          30845.524962139316,
          30976.967402952723,
          30725.243504913524,
          30230.93640009407,
          28057.1121410653,
          27437.486239962745,
          27489.131254286738,
          27585.860475434456,
          27524.825832565082,
          27429.736298961332,
          27841.643502995837,
          27559.514379213564,
          27436.856698697433,
          27395.78737487085,
          27481.50169090042,
          27495.059460207354,
          29205.27639268199,
          28758.493374915794,
          27317.606981633697,
          27177.39721188275,
          27249.617543842178,
          27353.580341099063,
          27192.19349229103,
          27159.51288685645,
          27121.993453346193,
          27645.39952883986,
          27287.488779903855,
          27276.728970875032,
          27209.848418988287,
          26502.9906055443,
          27215.999821022153,
          27627.567706264555,
          27949.409472141415,
          28025.651882834733,
          27985.338597599417,
          27948.54128095135,
          28188.558188644238,
          28661.90965738264,
          28577.232204216998,
          27981.26414594357,
          27996.537729176925,
          27997.397691163933,
          27654.429215072887,
          27697.598072425462,
          27607.792774308007,
          27761.913204029202,
          28466.63567353366,
          28347.446574505884,
          28406.125614145305,
          29469.38026642101,
          28975.836093735415,
          28892.43417283427,
          29281.72337273648,
          28874.578644293826,
          29437.761229880154,
          29463.985958801117,
          29424.799294245895,
          29061.107297809096,
          28853.465520129772,
          28307.00481485622,
          28180.16656503966,
          28299.53672222607,
          28290.908297848422,
          28611.77705789497,
          30043.77456176048,
          29929.41605020687,
          29843.97325267992,
          30265.731212310493,
          31267.01669857558,
          31514.84219169896,
          31748.719014364295,
          34842.009503822774,
          35725.49524858408,
          36345.12481582537,
          35978.780884981155,
          35716.731039712206,
          35908.07058945857,
          36374.042590450495,
          36340.689220912755,
          36506.27104881033,
          37332.419117219746,
          36806.685909816995,
          36589.539262397215,
          36962.09631784074,
          36914.4651686931,
          36917.625158457085,
          37309.142638237216,
          37534.72434789408,
          38660.35337868333,
          39300.60153981764,
          39115.12946290802,
          39048.04416825157,
          38438.664130308665,
          37444.6422603135,
          39900.91699826298,
          38094.255370579194,
          38579.37290247297,
          38537.65444059111,
          39368.02012132807,
          39468.17899292568,
          37664.20423333626,
          39416.857278277166,
          39286.1182943685,
          39733.89540533535,
          39813.32839010842,
          39452.411644476,
          39235.21592230536,
          39844.17221995536,
          39881.05185464956,
          39730.30742793344,
          40753.3953191014,
          41578.09521665331,
          42111.53528809082,
          44226.281762402505
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "BTC-USD price predictions",
         "x": 0.3
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = prices_df_val.iloc[200:]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "        go.Candlestick(\n",
    "            x=df['Date'],\n",
    "            open=df['Open'],\n",
    "            high=df['High'],\n",
    "            low=df['Low'],\n",
    "            close=df['Close'],\n",
    "            name='Actual Price'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[0],\n",
    "            line=dict(color='red'),\n",
    "            name='Pred. High'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[1],\n",
    "            line=dict(color='blue'),\n",
    "            name='Pred. Low'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[2],\n",
    "            line=dict(color='green'),\n",
    "            name='Pred. Close'\n",
    "        ),\n",
    "    ]).update_layout(title_text=tickers+' price predictions', title_x=0.3)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
