{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas_ta as ta\n",
    "import getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x = ['High_delta', 'Low_delta', 'Close_delta', 'RSI_14', 'WMA_100_delta', 'WMA_200_delta']\n",
    "features_y = ['Close_delta']\n",
    "number_y = 1\n",
    "random_state = 420\n",
    "test_size = 0.2\n",
    "win_size = 22\n",
    "\n",
    "preprocess_param = {\n",
    "    'win_size':win_size,\n",
    "    'stride':1,\n",
    "    'split':True,\n",
    "    'test_size':test_size,\n",
    "    'number_y': number_y,\n",
    "    'random_state':random_state,\n",
    "    'features_x':features_x,\n",
    "    'features_y':features_y,\n",
    "    'convert_to_torch':True,\n",
    "}\n",
    "\n",
    "v_preprocess_param = {\n",
    "    'win_size':win_size,\n",
    "    'stride':1,\n",
    "    'split':False,\n",
    "    'number_y': number_y,\n",
    "    'random_state':random_state,\n",
    "    'features_x':features_x,\n",
    "    'features_y':features_y,\n",
    "    'convert_to_torch':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = 'BTC-USD'\n",
    "\n",
    "prices_df = getData.loader(tickers=tickers, interval=\"1d\", period='max', end=\"2023-01-01\").dataframe\n",
    "prices_df_val = getData.loader(tickers=tickers, interval=\"1d\", start='2023-01-01').dataframe\n",
    "\n",
    "datasets = getData.preprocessor(prices_df, preprocess_param=preprocess_param).dataset\n",
    "val_sets = getData.preprocessor(prices_df_val, preprocess_param=v_preprocess_param).dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PriceHistoryDataset(Dataset):\n",
    "    def __init__(self, dataset, features_x=['High_delta', 'Low_delta', 'Close_delta', 'RSI_14', 'WMA_100_delta', 'WMA_200_delta'], features_y=['Close_delta'], num_y=1):\n",
    "        self.dataframes = dataset\n",
    "        self.features_x = features_x\n",
    "        self.features_y = features_y\n",
    "        self.num_y = num_y\n",
    "        \n",
    "        self.X, self.y = self.preprocess(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def preprocess(self, dataset):\n",
    "        X, y = self.__split_X_y(dataset, num_y=self.num_y)\n",
    "        X = self.__feature_select(X, self.features_x)\n",
    "        y = self.__feature_select(y, self.features_y)\n",
    "        X = self.__to_torch(X)\n",
    "        y = self.__to_torch(y)\n",
    "        return X, y\n",
    "    \n",
    "    def __split_X_y(self, dataset, num_y):\n",
    "        X = [df.iloc[:-num_y] for df in dataset]\n",
    "        y = [df.iloc[-num_y:] for df in dataset]\n",
    "        return X, y\n",
    "    \n",
    "    def __feature_select(self, dataset, feature):\n",
    "        return [df[feature] for df in dataset]\n",
    "    \n",
    "    def __to_torch(self, dataset):\n",
    "        arr = np.stack([df.to_numpy() for df in dataset])\n",
    "        return torch.from_numpy(arr).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x=['High_delta', 'Low_delta', 'Close_delta', 'RSI_14', 'WMA_100_delta', 'WMA_200_delta']\n",
    "features_y=['Close_delta']\n",
    "num_y=1\n",
    "\n",
    "train_set = PriceHistoryDataset(datasets['train'], features_x, features_y, num_y)\n",
    "test_set = PriceHistoryDataset(datasets['test'],  features_x, features_y, num_y)\n",
    "val_set = PriceHistoryDataset(val_sets,  features_x, features_y, num_y)\n",
    "\n",
    "train_loader= DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LSTMModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hidden_size, lstm_layers, input_size=8, output_size=3, dropout=0.05):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=lstm_layers, dropout=dropout)\n",
    "        \n",
    "        self.out_linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # keep track of losses function.\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.loss_func = nn.L1Loss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:,-1:,:]\n",
    "        \n",
    "        output = self.out_linear(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.train_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y, y_hat)#.mean()\n",
    "        self.test_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.test_losses).mean()\n",
    "        print(f'Test Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.train_losses).mean()\n",
    "        print(f'Train Loss: {avg_loss}')\n",
    "        return {'L1_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = LSTMModel(input_size=len(features_x), output_size=len(features_y), hidden_size=256, lstm_layers=6, dropout=0.0)\n",
    "\n",
    "# checkpoint = torch.load(\"model\\LSTM_BTC\\checkpoints\\epoch=999-step=9000.ckpt\")\n",
    "# model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2904321\n",
      "Number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Number of layers:\", len(list(model.children())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type   | Params\n",
      "--------------------------------------\n",
      "0 | lstm       | LSTM   | 2.9 M \n",
      "1 | out_linear | Linear | 257   \n",
      "2 | loss_func  | L1Loss | 0     \n",
      "--------------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.617    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c8c78dc7944e44a0290b93a5fea4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 8, got 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(default_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m         closure()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1289\u001b[0m \n\u001b[0;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 121\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    124\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[26], line 29\u001b[0m, in \u001b[0;36mLSTMModel.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     28\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 29\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(y, y_hat)\u001b[38;5;66;03m#.mean()\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 20\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 20\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m lstm_out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[0;32m     23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_linear(lstm_out)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 8, got 6"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = pl.Trainer(default_root_dir=\"model/\", max_epochs=20)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304a37eb4264d138f50bc49b616fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.09540335088968277\n",
      "[{}]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.test(model=model, dataloaders=val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to predict btc Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'losses': tensor(0.0954)}\n"
     ]
    }
   ],
   "source": [
    "x = val_loader.dataset.X\n",
    "y = val_loader.dataset.y\n",
    "ref = val_loader.dataset.initial_price\n",
    "date = val_loader.dataset.current_date\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "    \n",
    "predict_out = np.multiply( (np.array(y_hat)+1)[:,0,:].T, ref)\n",
    "\n",
    "print({\n",
    "    'losses':nn.L1Loss()(y, y_hat),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          29792.015625,
          29908.744140625,
          29771.802734375,
          30084.5390625,
          29176.916015625,
          29227.390625,
          29354.97265625,
          29210.689453125,
          29319.24609375,
          29356.91796875,
          29275.30859375,
          29230.111328125,
          29675.732421875,
          29151.958984375,
          29178.6796875,
          29074.091796875,
          29042.126953125,
          29041.85546875,
          29180.578125,
          29765.4921875,
          29561.494140625,
          29429.591796875,
          29397.71484375,
          29415.96484375,
          29282.9140625,
          29408.443359375,
          29170.34765625,
          28701.779296875,
          26664.55078125,
          26049.556640625,
          26096.205078125,
          26189.583984375,
          26124.140625,
          26031.65625,
          26431.640625,
          26162.373046875,
          26047.66796875,
          26008.462890625,
          26089.693359375,
          26106.150390625,
          27727.392578125,
          27297.265625,
          25931.47265625,
          25800.724609375,
          25868.798828125,
          25969.56640625,
          25812.416015625,
          25779.982421875,
          25753.236328125,
          26240.1953125,
          25905.654296875,
          25895.677734375,
          25832.2265625,
          25162.654296875,
          25833.34375,
          26228.32421875,
          26539.673828125,
          26608.693359375,
          26568.28125,
          26534.1875,
          26754.28125,
          27211.1171875,
          27132.0078125,
          26567.6328125,
          26579.568359375,
          26579.390625,
          26256.826171875,
          26298.48046875,
          26217.25,
          26352.716796875,
          27021.546875,
          26911.720703125,
          26967.916015625,
          27983.75,
          27530.78515625,
          27429.978515625,
          27799.39453125,
          27415.912109375,
          27946.59765625,
          27968.83984375,
          27935.08984375,
          27583.677734375,
          27391.01953125,
          26873.3203125,
          26756.798828125,
          26862.375,
          26861.70703125,
          27159.65234375,
          28519.466796875,
          28415.748046875,
          28328.341796875,
          28719.806640625,
          29682.94921875,
          29918.412109375,
          29993.896484375,
          33086.234375,
          33901.52734375,
          34502.8203125,
          34156.6484375,
          33909.80078125,
          34089.57421875,
          34538.48046875,
          34502.36328125,
          34667.78125,
          35437.25390625,
          34938.2421875,
          34732.32421875,
          35082.1953125,
          35049.35546875,
          35037.37109375,
          35443.5625,
          35655.27734375,
          36693.125,
          37313.96875,
          37138.05078125,
          37054.51953125,
          36502.35546875,
          35537.640625,
          37880.58203125,
          36154.76953125,
          36596.68359375,
          36585.703125,
          37386.546875,
          37476.95703125,
          35813.8125,
          37432.33984375,
          37289.62109375,
          37720.28125,
          37796.79296875,
          37479.12109375,
          37254.16796875,
          37831.0859375,
          37858.4921875,
          37712.74609375,
          38688.75,
          39476.33203125,
          39978.390625,
          41980.09765625,
          44080.6484375,
          43746.4453125,
          43292.6640625,
          44166.6015625,
          43725.984375,
          43779.69921875,
          41243.83203125,
          41450.22265625,
          42890.7421875,
          43023.97265625,
          41929.7578125,
          42240.1171875,
          41364.6640625,
          42623.5390625,
          42270.52734375,
          43652.25,
          43869.15234375,
          43997.90234375,
          43739.54296875,
          43016.1171875,
          43613.140625,
          42653.109375
         ],
         "high": [
          30195.53125,
          30045.998046875,
          29991.615234375,
          30330.640625,
          30093.39453125,
          29353.16015625,
          29675.552734375,
          29560.966796875,
          29521.513671875,
          29396.84375,
          29443.169921875,
          29489.873046875,
          29675.732421875,
          29987.998046875,
          29375.70703125,
          29302.078125,
          29102.46484375,
          29160.822265625,
          29244.28125,
          30176.796875,
          30093.435546875,
          29688.564453125,
          29517.7734375,
          29465.11328125,
          29441.43359375,
          29660.25390625,
          29439.12109375,
          29221.9765625,
          28745.947265625,
          26808.1953125,
          26249.44921875,
          26260.681640625,
          26220.201171875,
          26135.5078125,
          26786.8984375,
          26554.91015625,
          26248.103515625,
          26107.384765625,
          26165.373046875,
          26198.578125,
          28089.337890625,
          27760.16015625,
          27456.078125,
          26125.869140625,
          25970.28515625,
          26087.1484375,
          26081.525390625,
          25858.375,
          25953.015625,
          26409.302734375,
          26414.005859375,
          25921.9765625,
          25978.130859375,
          25883.947265625,
          26451.939453125,
          26376.11328125,
          26774.623046875,
          26840.498046875,
          26754.76953125,
          26617.998046875,
          27414.734375,
          27488.763671875,
          27379.505859375,
          27152.939453125,
          26726.078125,
          26634.185546875,
          26716.05859375,
          26421.5078125,
          26389.884765625,
          26817.841796875,
          27259.5,
          27225.9375,
          27091.794921875,
          28047.23828125,
          28494.458984375,
          27667.19140625,
          27826.658203125,
          28091.861328125,
          28252.537109375,
          28028.091796875,
          28102.169921875,
          27989.470703125,
          27715.84765625,
          27474.115234375,
          26921.439453125,
          27092.697265625,
          26969,
          27289.169921875,
          29448.138671875,
          28618.751953125,
          28889.009765625,
          28892.474609375,
          30104.0859375,
          30287.482421875,
          30199.43359375,
          34370.4375,
          35150.43359375,
          35133.7578125,
          34832.91015625,
          34238.2109375,
          34399.390625,
          34743.26171875,
          34843.93359375,
          34719.25390625,
          35527.9296875,
          35919.84375,
          34942.47265625,
          35256.03125,
          35340.33984375,
          35286.02734375,
          35892.41796875,
          35994.41796875,
          37926.2578125,
          37493.80078125,
          37407.09375,
          37227.69140625,
          37405.1171875,
          36753.3515625,
          37964.89453125,
          37934.625,
          36704.484375,
          36839.28125,
          37509.35546875,
          37756.8203125,
          37631.140625,
          37856.98046875,
          37643.91796875,
          38415.33984375,
          37892.4296875,
          37820.30078125,
          37559.35546875,
          38368.48046875,
          38366.11328125,
          38141.75390625,
          38954.109375,
          39678.9375,
          40135.60546875,
          42371.75,
          44408.6640625,
          44265.76953125,
          44042.58984375,
          44705.515625,
          44361.2578125,
          44034.625,
          43808.375,
          42048.3046875,
          43429.78125,
          43390.859375,
          43087.82421875,
          42664.9453125,
          42359.49609375,
          42720.296875,
          43354.296875,
          44275.5859375,
          44240.66796875,
          44367.95703125,
          44015.69921875,
          43945.5234375,
          43765.09375,
          43599.84765625
         ],
         "low": [
          29638.095703125,
          29733.8515625,
          29664.12109375,
          29741.52734375,
          28934.294921875,
          29062.43359375,
          29113.912109375,
          29099.3515625,
          29125.845703125,
          29264.166015625,
          29059.501953125,
          29131.578125,
          28657.0234375,
          28946.509765625,
          28959.48828125,
          28885.3359375,
          28957.796875,
          28963.833984375,
          28724.140625,
          29113.814453125,
          29376.80078125,
          29354.447265625,
          29253.517578125,
          29357.587890625,
          29265.806640625,
          29124.10546875,
          29088.853515625,
          28701.779296875,
          25409.111328125,
          25668.921875,
          25802.408203125,
          26004.314453125,
          25846.087890625,
          25520.728515625,
          25804.998046875,
          25914.92578125,
          25786.8125,
          25983.87890625,
          25965.09765625,
          25880.599609375,
          25912.62890625,
          27069.20703125,
          25752.9296875,
          25362.609375,
          25753.09375,
          25817.03125,
          25657.025390625,
          25589.98828125,
          25404.359375,
          25608.201171875,
          25677.48046875,
          25810.494140625,
          25640.26171875,
          24930.296875,
          25133.078125,
          25781.123046875,
          26171.451171875,
          26240.701171875,
          26473.890625,
          26445.07421875,
          26415.515625,
          26681.60546875,
          26864.08203125,
          26389.30078125,
          26495.533203125,
          26520.51953125,
          26221.05078125,
          26011.46875,
          26090.712890625,
          26111.46484375,
          26327.322265625,
          26721.763671875,
          26888.96875,
          26965.09375,
          27347.787109375,
          27216.001953125,
          27248.10546875,
          27375.6015625,
          27215.552734375,
          27870.423828125,
          27740.662109375,
          27302.5625,
          27301.654296875,
          26561.099609375,
          26558.3203125,
          26686.322265625,
          26814.5859375,
          26817.89453125,
          27130.47265625,
          28110.185546875,
          28174.251953125,
          28177.98828125,
          28601.669921875,
          29481.751953125,
          29720.3125,
          30097.828125,
          32880.76171875,
          33709.109375,
          33762.32421875,
          33416.88671875,
          33874.8046875,
          33947.56640625,
          34110.97265625,
          34083.30859375,
          34170.69140625,
          34401.57421875,
          34133.44140625,
          34616.69140625,
          34594.2421875,
          34765.36328125,
          34545.81640625,
          35147.80078125,
          35592.1015625,
          36362.75390625,
          36773.66796875,
          36779.1171875,
          36399.60546875,
          34948.5,
          35383.78125,
          35545.47265625,
          35901.234375,
          36233.3125,
          36414.59765625,
          36882.53125,
          35813.8125,
          35670.97265625,
          36923.86328125,
          37261.60546875,
          37617.41796875,
          37162.75,
          36750.12890625,
          36891.08984375,
          37612.6328125,
          37531.140625,
          37629.359375,
          38652.59375,
          39298.1640625,
          39978.62890625,
          41421.1484375,
          43478.08203125,
          42880.6484375,
          43125.296875,
          43627.59765625,
          43593.28515625,
          40234.578125,
          40667.5625,
          40676.8671875,
          41767.08984375,
          41692.96875,
          41723.11328125,
          41274.54296875,
          40530.2578125,
          41826.3359375,
          42223.81640625,
          43330.05078125,
          43441.96875,
          43351.35546875,
          42786.91796875,
          42765.76953125,
          42379.10546875
         ],
         "name": "Actual Price",
         "open": [
          29915.25,
          29805.111328125,
          29908.697265625,
          29790.111328125,
          30081.662109375,
          29178.970703125,
          29225.759765625,
          29353.798828125,
          29212.1640625,
          29319.4453125,
          29357.09375,
          29278.314453125,
          29230.873046875,
          29704.146484375,
          29161.8125,
          29174.3828125,
          29075.388671875,
          29043.701171875,
          29038.513671875,
          29180.01953125,
          29766.6953125,
          29563.97265625,
          29424.90234375,
          29399.787109375,
          29416.59375,
          29283.263671875,
          29408.048828125,
          29169.07421875,
          28699.802734375,
          26636.078125,
          26047.83203125,
          26096.861328125,
          26188.69140625,
          26130.748046875,
          26040.474609375,
          26431.51953125,
          26163.6796875,
          26047.234375,
          26008.2421875,
          26089.615234375,
          26102.486328125,
          27726.083984375,
          27301.9296875,
          25934.021484375,
          25800.91015625,
          25869.47265625,
          25968.169921875,
          25814.95703125,
          25783.931640625,
          25748.3125,
          26245.208984375,
          25905.42578125,
          25895.2109375,
          25831.71484375,
          25160.658203125,
          25837.5546875,
          26228.27734375,
          26533.818359375,
          26606.19921875,
          26567.927734375,
          26532.994140625,
          26760.8515625,
          27210.228515625,
          27129.83984375,
          26564.056640625,
          26578.556640625,
          26579.373046875,
          26253.775390625,
          26294.7578125,
          26209.498046875,
          26355.8125,
          27024.841796875,
          26911.689453125,
          26967.396484375,
          27976.798828125,
          27508.251953125,
          27429.07421875,
          27798.646484375,
          27412.123046875,
          27946.78125,
          27971.677734375,
          27934.47265625,
          27589.201171875,
          27392.076171875,
          26873.29296875,
          26752.87890625,
          26866.203125,
          26858.01171875,
          27162.62890625,
          28522.09765625,
          28413.53125,
          28332.416015625,
          28732.8125,
          29683.380859375,
          29918.654296875,
          30140.685546875,
          33077.3046875,
          33916.04296875,
          34504.2890625,
          34156.5,
          33907.72265625,
          34089.37109375,
          34531.7421875,
          34500.078125,
          34657.2734375,
          35441.578125,
          34942.47265625,
          34736.32421875,
          35090.01171875,
          35044.7890625,
          35047.79296875,
          35419.4765625,
          35633.6328125,
          36702.25,
          37310.0703125,
          37133.9921875,
          37070.3046875,
          36491.7890625,
          35548.11328125,
          37879.98046875,
          36164.82421875,
          36625.37109375,
          36585.765625,
          37374.07421875,
          37469.16015625,
          35756.5546875,
          37420.43359375,
          37296.31640625,
          37721.4140625,
          37796.828125,
          37454.19140625,
          37247.9921875,
          37826.10546875,
          37861.1171875,
          37718.0078125,
          38689.27734375,
          39472.20703125,
          39978.62890625,
          41986.265625,
          44080.0234375,
          43769.1328125,
          43293.13671875,
          44180.01953125,
          43728.3828125,
          43792.01953125,
          41238.734375,
          41468.46484375,
          42884.26171875,
          43028.25,
          41937.7421875,
          42236.109375,
          41348.203125,
          42641.51171875,
          42261.30078125,
          43648.125,
          43868.98828125,
          44012.19921875,
          43728.3671875,
          43010.57421875,
          43599.84765625
         ],
         "type": "candlestick",
         "x": [
          "2023-07-20T00:00:00+00:00",
          "2023-07-21T00:00:00+00:00",
          "2023-07-22T00:00:00+00:00",
          "2023-07-23T00:00:00+00:00",
          "2023-07-24T00:00:00+00:00",
          "2023-07-25T00:00:00+00:00",
          "2023-07-26T00:00:00+00:00",
          "2023-07-27T00:00:00+00:00",
          "2023-07-28T00:00:00+00:00",
          "2023-07-29T00:00:00+00:00",
          "2023-07-30T00:00:00+00:00",
          "2023-07-31T00:00:00+00:00",
          "2023-08-01T00:00:00+00:00",
          "2023-08-02T00:00:00+00:00",
          "2023-08-03T00:00:00+00:00",
          "2023-08-04T00:00:00+00:00",
          "2023-08-05T00:00:00+00:00",
          "2023-08-06T00:00:00+00:00",
          "2023-08-07T00:00:00+00:00",
          "2023-08-08T00:00:00+00:00",
          "2023-08-09T00:00:00+00:00",
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "name": "Pred. High",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          29610.193981423974,
          26001.287631657673,
          34439.37247759476,
          31387.467179702595,
          31709.558070920175,
          30756.56621988956,
          30805.895440266235,
          30940.836136235157,
          30791.543770362623,
          30904.628764646128,
          30944.312700413167,
          30861.274126582313,
          30811.267755977344,
          31310.132709071273,
          30738.47686188668,
          30751.726801681332,
          30647.38043771428,
          30613.979719375493,
          30608.51521333959,
          30757.671754145995,
          31376.066854480654,
          31162.38375836797,
          31015.797952876892,
          30989.324830695754,
          31007.040144201368,
          30866.501402021153,
          30998.036735726055,
          30746.141624846496,
          30251.498311490286,
          28076.195501443,
          27456.145047559403,
          27507.82829396031,
          27604.623306568246,
          27543.547150304774,
          27448.392940492602,
          27860.580307812896,
          27578.262409709394,
          27455.521288320422,
          27414.420923553407,
          27500.193539125845,
          27513.76052989997,
          29225.140685392544,
          28778.057037844323,
          27336.19044663082,
          27195.882217981387,
          27268.15167139517,
          27372.18518009386,
          27210.691639625467,
          27177.98880241858,
          27140.443845406175,
          27664.20598054258,
          27306.051754162647,
          27295.28462551348,
          27228.358574418817,
          26521.019903904526,
          27234.51416109223,
          27646.362025311682,
          27968.42273180955,
          28044.717008240987,
          28004.376298921416,
          27967.55711299088,
          28207.734132822603,
          28681.40761027392,
          28596.672553235665,
          28000.29907336738,
          28015.58304682281,
          28016.443593819626,
          27673.24493487389,
          27716.443163764663,
          27626.57363672997,
          27780.798910565674,
          28486.000784412725,
          28366.730604144977,
          28425.44956160686,
          29489.42751833168,
          28995.550879638176,
          28912.092213143595,
          29301.64628040651,
          28894.224535931367,
          29457.790303699672,
          29484.03287558863,
          29444.81621661689,
          29080.876810014714,
          28873.09377905773,
          28326.261330840178,
          28199.33679627534,
          28318.791360562667,
          28310.157065516803,
          28631.244140512776,
          30064.215956937056,
          29949.779637333006,
          29864.278705633013,
          30286.323623917997,
          31288.286830920028,
          31536.28091323399,
          31770.31683617481,
          34865.71555323899,
          35749.7983670854,
          36369.85356532037,
          36003.260378837585,
          35741.03223826736,
          35932.501972950995,
          36398.79101522267,
          36365.41495248675,
          36531.10944010317,
          37357.81960889697,
          36831.728699468076,
          36614.43430829793,
          36987.24484670907,
          36939.57710944116,
          36942.74342972785,
          37334.52729002852,
          37560.26248252485,
          38686.65737655759,
          39327.341154151596,
          39141.74288447108,
          39074.61194585916,
          38464.817293723114,
          37470.11910427222,
          39928.065059368964,
          38120.17420278629,
          38605.6218022923,
          38563.87495571561,
          39394.8056063815,
          39495.03262476297,
          37689.83046438452,
          39443.675988457166,
          39312.8436053996,
          39760.929824078456,
          39840.42136280611,
          39479.259013303556,
          39261.91104610078,
          39871.28166958969,
          39908.18639670126,
          39757.33940546401,
          40781.123392148875,
          41606.384404641576,
          42140.18742167298,
          44256.37274161354
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "name": "Pred. Low",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          28348.452489629388,
          25505.078089465736,
          32175.935226906324,
          29620.04513342469,
          29919.981308615883,
          29020.765915689524,
          29067.29606767895,
          29194.619899774552,
          29053.75318704592,
          29160.454422972165,
          29197.89870386198,
          29119.548349499586,
          29072.365943770856,
          29543.07301243767,
          29003.681622255594,
          29016.183771826793,
          28917.72816318064,
          28886.212487792363,
          28881.05311687081,
          29021.793403815245,
          29605.28798647737,
          29403.666338414885,
          29265.35011434462,
          29240.371063654777,
          29257.088326094672,
          29124.481202870957,
          29248.591490155784,
          29010.912657160778,
          28544.18567283475,
          26491.652649941854,
          25906.596129414393,
          25955.359580865013,
          26046.69174024253,
          25989.064117412083,
          25899.280152901076,
          26288.20478411205,
          26021.818758360576,
          25906.004818833433,
          25867.224049146753,
          25948.155886827386,
          25960.958748619305,
          27575.753264682135,
          27153.898730757646,
          25793.407320597908,
          25661.017721577082,
          25729.210017909063,
          25827.372153225937,
          25674.989973166725,
          25644.132800239953,
          25608.708319053054,
          26102.91069185408,
          25764.969751431607,
          25754.810291307047,
          25691.660005251644,
          25024.24171113188,
          25697.468186435755,
          26086.07241582754,
          26389.958425426157,
          26461.94685331965,
          26423.882867659675,
          26389.13867539051,
          26615.7607095344,
          27062.70287156047,
          26982.750042794505,
          26420.034345383174,
          26434.45573076082,
          26435.26771070261,
          26111.436911491677,
          26152.197141490877,
          26067.399623492733,
          26212.920812547207,
          26878.322874436155,
          26765.784001788124,
          26821.190617035492,
          27825.12040635303,
          27359.113795196754,
          27280.365329154534,
          27647.933929588064,
          27263.506059395033,
          27795.265570895746,
          27820.02707701677,
          27783.025374182034,
          27439.625785816228,
          27243.569502804196,
          26727.598892816808,
          26607.837657478172,
          26720.547486791387,
          26712.400490408298,
          27015.367808972253,
          28367.46625396493,
          28259.488436108455,
          28178.812964705634,
          28577.03871568665,
          29522.453607033123,
          29756.451519079157,
          29977.280832135584,
          32897.97938554548,
          33732.17053101864,
          34317.22750735469,
          33971.32394850254,
          33723.89532626141,
          33904.558974909596,
          34344.53179798089,
          34313.04145539645,
          34469.3845585878,
          35249.43726334814,
          34753.03761527245,
          34548.00677863858,
          34899.77681828267,
          34854.79932944896,
          34857.78695050045,
          35227.45552094327,
          35440.4507569992,
          36503.276794523,
          37107.80194239598,
          36932.67838636506,
          36869.33615418989,
          36293.95682487544,
          35355.39697507722,
          37674.62245562347,
          35968.764549375046,
          36426.81467041606,
          36387.42391403578,
          37171.458865494,
          37266.03154875687,
          35562.71049317764,
          37217.56914373324,
          37094.124822968384,
          37516.91793076834,
          37591.92315694969,
          37251.14394766255,
          37046.062580500264,
          37621.04178174143,
          37655.86369421752,
          37513.53014680324,
          38479.534211024875,
          39258.219463460846,
          39761.89591342746,
          41758.648789322935
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "name": "Pred. Close",
         "type": "scatter",
         "x": [
          "2023-08-10T00:00:00+00:00",
          "2023-08-11T00:00:00+00:00",
          "2023-08-12T00:00:00+00:00",
          "2023-08-13T00:00:00+00:00",
          "2023-08-14T00:00:00+00:00",
          "2023-08-15T00:00:00+00:00",
          "2023-08-16T00:00:00+00:00",
          "2023-08-17T00:00:00+00:00",
          "2023-08-18T00:00:00+00:00",
          "2023-08-19T00:00:00+00:00",
          "2023-08-20T00:00:00+00:00",
          "2023-08-21T00:00:00+00:00",
          "2023-08-22T00:00:00+00:00",
          "2023-08-23T00:00:00+00:00",
          "2023-08-24T00:00:00+00:00",
          "2023-08-25T00:00:00+00:00",
          "2023-08-26T00:00:00+00:00",
          "2023-08-27T00:00:00+00:00",
          "2023-08-28T00:00:00+00:00",
          "2023-08-29T00:00:00+00:00",
          "2023-08-30T00:00:00+00:00",
          "2023-08-31T00:00:00+00:00",
          "2023-09-01T00:00:00+00:00",
          "2023-09-02T00:00:00+00:00",
          "2023-09-03T00:00:00+00:00",
          "2023-09-04T00:00:00+00:00",
          "2023-09-05T00:00:00+00:00",
          "2023-09-06T00:00:00+00:00",
          "2023-09-07T00:00:00+00:00",
          "2023-09-08T00:00:00+00:00",
          "2023-09-09T00:00:00+00:00",
          "2023-09-10T00:00:00+00:00",
          "2023-09-11T00:00:00+00:00",
          "2023-09-12T00:00:00+00:00",
          "2023-09-13T00:00:00+00:00",
          "2023-09-14T00:00:00+00:00",
          "2023-09-15T00:00:00+00:00",
          "2023-09-16T00:00:00+00:00",
          "2023-09-17T00:00:00+00:00",
          "2023-09-18T00:00:00+00:00",
          "2023-09-19T00:00:00+00:00",
          "2023-09-20T00:00:00+00:00",
          "2023-09-21T00:00:00+00:00",
          "2023-09-22T00:00:00+00:00",
          "2023-09-23T00:00:00+00:00",
          "2023-09-24T00:00:00+00:00",
          "2023-09-25T00:00:00+00:00",
          "2023-09-26T00:00:00+00:00",
          "2023-09-27T00:00:00+00:00",
          "2023-09-28T00:00:00+00:00",
          "2023-09-29T00:00:00+00:00",
          "2023-09-30T00:00:00+00:00",
          "2023-10-01T00:00:00+00:00",
          "2023-10-02T00:00:00+00:00",
          "2023-10-03T00:00:00+00:00",
          "2023-10-04T00:00:00+00:00",
          "2023-10-05T00:00:00+00:00",
          "2023-10-06T00:00:00+00:00",
          "2023-10-07T00:00:00+00:00",
          "2023-10-08T00:00:00+00:00",
          "2023-10-09T00:00:00+00:00",
          "2023-10-10T00:00:00+00:00",
          "2023-10-11T00:00:00+00:00",
          "2023-10-12T00:00:00+00:00",
          "2023-10-13T00:00:00+00:00",
          "2023-10-14T00:00:00+00:00",
          "2023-10-15T00:00:00+00:00",
          "2023-10-16T00:00:00+00:00",
          "2023-10-17T00:00:00+00:00",
          "2023-10-18T00:00:00+00:00",
          "2023-10-19T00:00:00+00:00",
          "2023-10-20T00:00:00+00:00",
          "2023-10-21T00:00:00+00:00",
          "2023-10-22T00:00:00+00:00",
          "2023-10-23T00:00:00+00:00",
          "2023-10-24T00:00:00+00:00",
          "2023-10-25T00:00:00+00:00",
          "2023-10-26T00:00:00+00:00",
          "2023-10-27T00:00:00+00:00",
          "2023-10-28T00:00:00+00:00",
          "2023-10-29T00:00:00+00:00",
          "2023-10-30T00:00:00+00:00",
          "2023-10-31T00:00:00+00:00",
          "2023-11-01T00:00:00+00:00",
          "2023-11-02T00:00:00+00:00",
          "2023-11-03T00:00:00+00:00",
          "2023-11-04T00:00:00+00:00",
          "2023-11-05T00:00:00+00:00",
          "2023-11-06T00:00:00+00:00",
          "2023-11-07T00:00:00+00:00",
          "2023-11-08T00:00:00+00:00",
          "2023-11-09T00:00:00+00:00",
          "2023-11-10T00:00:00+00:00",
          "2023-11-11T00:00:00+00:00",
          "2023-11-12T00:00:00+00:00",
          "2023-11-13T00:00:00+00:00",
          "2023-11-14T00:00:00+00:00",
          "2023-11-15T00:00:00+00:00",
          "2023-11-16T00:00:00+00:00",
          "2023-11-17T00:00:00+00:00",
          "2023-11-18T00:00:00+00:00",
          "2023-11-19T00:00:00+00:00",
          "2023-11-20T00:00:00+00:00",
          "2023-11-21T00:00:00+00:00",
          "2023-11-22T00:00:00+00:00",
          "2023-11-23T00:00:00+00:00",
          "2023-11-24T00:00:00+00:00",
          "2023-11-25T00:00:00+00:00",
          "2023-11-26T00:00:00+00:00",
          "2023-11-27T00:00:00+00:00",
          "2023-11-28T00:00:00+00:00",
          "2023-11-29T00:00:00+00:00",
          "2023-11-30T00:00:00+00:00",
          "2023-12-01T00:00:00+00:00",
          "2023-12-02T00:00:00+00:00",
          "2023-12-03T00:00:00+00:00",
          "2023-12-04T00:00:00+00:00",
          "2023-12-05T00:00:00+00:00",
          "2023-12-06T00:00:00+00:00",
          "2023-12-07T00:00:00+00:00",
          "2023-12-08T00:00:00+00:00",
          "2023-12-09T00:00:00+00:00",
          "2023-12-10T00:00:00+00:00",
          "2023-12-11T00:00:00+00:00",
          "2023-12-12T00:00:00+00:00",
          "2023-12-13T00:00:00+00:00",
          "2023-12-14T00:00:00+00:00",
          "2023-12-15T00:00:00+00:00",
          "2023-12-16T00:00:00+00:00",
          "2023-12-17T00:00:00+00:00",
          "2023-12-18T00:00:00+00:00",
          "2023-12-19T00:00:00+00:00",
          "2023-12-20T00:00:00+00:00",
          "2023-12-21T00:00:00+00:00",
          "2023-12-22T00:00:00+00:00",
          "2023-12-23T00:00:00+00:00",
          "2023-12-24T00:00:00+00:00",
          "2023-12-25T00:00:00+00:00",
          "2023-12-26T00:00:00+00:00"
         ],
         "y": [
          29631.200539380312,
          26646.90098778298,
          34075.916156920604,
          31368.27263017441,
          31687.7551084233,
          30735.667966450565,
          30784.96019207826,
          30919.81266924762,
          30770.62174352724,
          30883.62640681956,
          30923.283373933285,
          30840.301231967518,
          30790.32884499966,
          31288.854778784793,
          30717.5874209553,
          30730.828356275335,
          30626.552904644515,
          30593.178347247886,
          30587.71409488772,
          30736.76927099703,
          31354.744118678384,
          31141.20623860648,
          30994.71654543886,
          30968.26492174063,
          30985.968196168542,
          30845.524962139316,
          30976.967402952723,
          30725.243504913524,
          30230.93640009407,
          28057.1121410653,
          27437.486239962745,
          27489.131254286738,
          27585.860475434456,
          27524.825832565082,
          27429.736298961332,
          27841.643502995837,
          27559.514379213564,
          27436.856698697433,
          27395.78737487085,
          27481.50169090042,
          27495.059460207354,
          29205.27639268199,
          28758.493374915794,
          27317.606981633697,
          27177.39721188275,
          27249.617543842178,
          27353.580341099063,
          27192.19349229103,
          27159.51288685645,
          27121.993453346193,
          27645.39952883986,
          27287.488779903855,
          27276.728970875032,
          27209.848418988287,
          26502.9906055443,
          27215.999821022153,
          27627.567706264555,
          27949.409472141415,
          28025.651882834733,
          27985.338597599417,
          27948.54128095135,
          28188.558188644238,
          28661.90965738264,
          28577.232204216998,
          27981.26414594357,
          27996.537729176925,
          27997.397691163933,
          27654.429215072887,
          27697.598072425462,
          27607.792774308007,
          27761.913204029202,
          28466.63567353366,
          28347.446574505884,
          28406.125614145305,
          29469.38026642101,
          28975.836093735415,
          28892.43417283427,
          29281.72337273648,
          28874.578644293826,
          29437.761229880154,
          29463.985958801117,
          29424.799294245895,
          29061.107297809096,
          28853.465520129772,
          28307.00481485622,
          28180.16656503966,
          28299.53672222607,
          28290.908297848422,
          28611.77705789497,
          30043.77456176048,
          29929.41605020687,
          29843.97325267992,
          30265.731212310493,
          31267.01669857558,
          31514.84219169896,
          31748.719014364295,
          34842.009503822774,
          35725.49524858408,
          36345.12481582537,
          35978.780884981155,
          35716.731039712206,
          35908.07058945857,
          36374.042590450495,
          36340.689220912755,
          36506.27104881033,
          37332.419117219746,
          36806.685909816995,
          36589.539262397215,
          36962.09631784074,
          36914.4651686931,
          36917.625158457085,
          37309.142638237216,
          37534.72434789408,
          38660.35337868333,
          39300.60153981764,
          39115.12946290802,
          39048.04416825157,
          38438.664130308665,
          37444.6422603135,
          39900.91699826298,
          38094.255370579194,
          38579.37290247297,
          38537.65444059111,
          39368.02012132807,
          39468.17899292568,
          37664.20423333626,
          39416.857278277166,
          39286.1182943685,
          39733.89540533535,
          39813.32839010842,
          39452.411644476,
          39235.21592230536,
          39844.17221995536,
          39881.05185464956,
          39730.30742793344,
          40753.3953191014,
          41578.09521665331,
          42111.53528809082,
          44226.281762402505
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "BTC-USD price predictions",
         "x": 0.3
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = prices_df_val.iloc[200:]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "        go.Candlestick(\n",
    "            x=df['Date'],\n",
    "            open=df['Open'],\n",
    "            high=df['High'],\n",
    "            low=df['Low'],\n",
    "            close=df['Close'],\n",
    "            name='Actual Price'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[0],\n",
    "            line=dict(color='red'),\n",
    "            name='Pred. High'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[1],\n",
    "            line=dict(color='blue'),\n",
    "            name='Pred. Low'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=date,\n",
    "            y=predict_out[2],\n",
    "            line=dict(color='green'),\n",
    "            name='Pred. Close'\n",
    "        ),\n",
    "    ]).update_layout(title_text=tickers+' price predictions', title_x=0.3)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
